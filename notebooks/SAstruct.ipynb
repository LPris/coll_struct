{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import matplotlib.pyplot as plt \n",
    "import scipy.io as spio\n",
    "import numpy as np\n",
    "# import gym_strucSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class StructSA(gym.Env):\n",
    "\n",
    "    def __init__(self, config=None):\n",
    "        empty_config = {\"config\": {\"components\": 2} }\n",
    "        config = config or empty_config\n",
    "        # Number of components #\n",
    "        self.ncomp = config['config'].get(\"components\", 2)\n",
    "        self.time = 0\n",
    "        self.ep_length = 30\n",
    "        self.nstcomp = 30\n",
    "        self.nobs = 2\n",
    "        self.actions_total = int(3**self.ncomp)\n",
    "        self.obs_total = int(self.ncomp*30 + 1)\n",
    "\n",
    "        # configure spaces\n",
    "        self.action_space = gym.spaces.Discrete(self.actions_total)\n",
    "        self.observation_space = gym.spaces.Box(low=0.0, high=1.0, shape=(self.obs_total,), dtype=np.float64)\n",
    "        ### Loading the underlying POMDP model ###\n",
    "        drmodel = np.load('Dr3031C10.npz')\n",
    "        self.belief0 = drmodel['belief0'][0,0:self.ncomp,:,0] # (10 components, 30 crack states)\n",
    "        self.P = drmodel['P'][:,0:self.ncomp,:,:,:] # (3 actions, 10 components, 31 det rates, 30 cracks, 30 cracks)\n",
    "        self.O = drmodel['O'][:,0:self.ncomp,:,:] # (3 actions, 10 components, 30 cracks, 2 observations)\n",
    "            \n",
    "    def reset(self, seed=None, return_info=False, options=None):\n",
    "        # We need the following line to seed self.np_random\n",
    "        # super().reset(seed=seed)\n",
    "\n",
    "        # Choose the agent's belief\n",
    "        self.time_step = 0\n",
    "        self.agent_belief = self.belief0\n",
    "        self.drate = np.zeros((self.ncomp, 1), dtype=int)\n",
    "        \n",
    "        observation = np.concatenate( ((self.agent_belief).reshape(self.obs_total - 1), [self.time_step/30]) )\n",
    "        info = {\"belief\": self.agent_belief}\n",
    "        return (observation, info) if return_info else observation\n",
    "    \n",
    "    def step(self, action, return_info=False):\n",
    "        action_ = np.zeros(1, dtype=int)\n",
    "        action_ = action\n",
    "        action_ = self.convert_base_action(action_, 3, self.ncomp)\n",
    "        observation_, belief_prime, drate_prime = self.belief_update(self.agent_belief, action_, self.drate)\n",
    "        observation = np.concatenate( (belief_prime.reshape(self.obs_total - 1), [self.time_step/30]) )\n",
    "        reward_ = self.immediate_cost(self.agent_belief, action_, belief_prime, self.drate)\n",
    "        reward = reward_.item() #Convert float64 to float\n",
    "        self.time_step += 1 \n",
    "        self.agent_belief = belief_prime\n",
    "        self.drate = drate_prime\n",
    "        # An episode is done if the agent has reached the target\n",
    "        done = np.array_equal(self.time_step, self.ep_length)\n",
    "        info = {\"belief\": self.agent_belief}\n",
    "        return (observation, reward, done, info) \n",
    "    \n",
    "    \n",
    "    def pf_sys(self, pf, k): # compute pf_sys for k-out-of-n components \n",
    "        n = pf.size\n",
    "        # k = ncomp-1\n",
    "        PF_sys = np.zeros(1)\n",
    "        nk = n-k\n",
    "        m = k+1\n",
    "        A = np.zeros(m+1)\n",
    "        A [1] = 1\n",
    "        L = 1\n",
    "        for j in range(1,n+1):\n",
    "            h = j + 1\n",
    "            Rel = 1-pf[j-1]\n",
    "            if nk < j:\n",
    "                L = h - nk\n",
    "            if k < j:\n",
    "                A[m] = A[m] + A[k]*Rel\n",
    "                h = k\n",
    "            for i in range(h, L-1, -1):\n",
    "                A[i] = A[i] + (A[i-1]-A[i])*Rel\n",
    "        PF_sys = 1-A[m]\n",
    "        return PF_sys  \n",
    "    \n",
    "    def immediate_cost(self, B, a, B_, drate): # immediate reward (-cost), based on current damage state and action#\n",
    "        cost_system = 0\n",
    "        PF = np.zeros((1,1))\n",
    "        PF = B[:,-1]\n",
    "        PF_ = np.zeros((1,1))\n",
    "        PF_ = B_[:,-1].copy()\n",
    "        for i in range(self.ncomp):\n",
    "            if a[i]==1:\n",
    "                cost_system += -1\n",
    "                Bplus = self.P[a[i],i,drate[i,0]].T.dot(B[i,:])\n",
    "                PF_[i] = Bplus[-1]         \n",
    "            elif a[i]==2:\n",
    "                cost_system +=  - 20\n",
    "        if self.ncomp < 2: # single component setting\n",
    "            PfSyS_ = PF_\n",
    "            PfSyS = PF\n",
    "        else:\n",
    "            PfSyS_ = self.pf_sys(PF_, self.ncomp-1) \n",
    "            PfSyS = self.pf_sys(PF, self.ncomp-1) \n",
    "        if PfSyS_ < PfSyS:\n",
    "            cost_system += PfSyS_*(-10000)\n",
    "        else:\n",
    "            cost_system += (PfSyS_-PfSyS)*(-10000) \n",
    "        return cost_system\n",
    "    \n",
    "    def belief_update(self, b, a, drate):  # Bayesian belief update based on previous belief, current observation, and action taken\n",
    "        b_prime = np.zeros((self.ncomp, self.nstcomp))\n",
    "        b_prime[:] = b\n",
    "        ob = np.zeros(self.ncomp)\n",
    "        drate_prime = np.zeros((self.ncomp, 1), dtype=int)\n",
    "        for i in range(self.ncomp):\n",
    "            p1 = self.P[a[i],i,drate[i,0]].T.dot(b_prime[i,:])  # environment transition\n",
    "            b_prime[i,:] = p1\n",
    "            drate_prime[i, 0] = drate[i, 0] + 1\n",
    "            ob[i] = 2\n",
    "            if a[i]==1:\n",
    "                Obs0 = np.sum(p1* self.O[a[i],i,:,0])\n",
    "                Obs1 = 1 - Obs0\n",
    "                if Obs1 < 1e-5:\n",
    "                    ob[i] = 0\n",
    "                else:\n",
    "                    ob_dist = np.array([Obs0, Obs1])\n",
    "                    ob[i] = np.random.choice(range(0,self.nobs), size=None, replace=True, p=ob_dist)           \n",
    "                b_prime[i,:] = p1* self.O[a[i],i,:,int(ob[i])]/(p1.dot(self.O[a[i],i,:,int(ob[i])])) # belief update\n",
    "            if a[i] == 2:\n",
    "                drate_prime[i, 0] = 0\n",
    "        return ob, b_prime, drate_prime\n",
    "    \n",
    "    def convert_base_action(self, action_, base, comp):\n",
    "        action_multi = np.zeros((comp,), dtype=int)\n",
    "        if action_ == 0:\n",
    "                return action_multi\n",
    "        digits = []\n",
    "        index_comp = int(comp) - 1 \n",
    "        while action_:\n",
    "            digits = (int(action_ % base))\n",
    "            action_multi[index_comp] = digits\n",
    "            action_ //= base\n",
    "            index_comp -= 1\n",
    "        return action_multi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Initialization of the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "struc_heur = StructSA()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "config = {\"config\": {\"components\": 5} }\n",
    "struc_heur = StructSA(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.052000e-04, 5.500000e-05, 8.660000e-05, 1.261000e-04,\n",
       "       2.006000e-04, 3.173000e-04, 4.853000e-04, 7.444000e-04,\n",
       "       1.138400e-03, 1.783100e-03, 2.713600e-03, 4.235700e-03,\n",
       "       6.473200e-03, 1.002420e-02, 1.530330e-02, 2.316180e-02,\n",
       "       3.453640e-02, 5.087030e-02, 7.324320e-02, 1.008326e-01,\n",
       "       1.309823e-01, 1.539425e-01, 1.567708e-01, 1.275575e-01,\n",
       "       7.401660e-02, 2.583390e-02, 4.230100e-03, 2.268000e-04,\n",
       "       3.200000e-06, 0.000000e+00, 1.052000e-04, 5.500000e-05,\n",
       "       8.660000e-05, 1.261000e-04, 2.006000e-04, 3.173000e-04,\n",
       "       4.853000e-04, 7.444000e-04, 1.138400e-03, 1.783100e-03,\n",
       "       2.713600e-03, 4.235700e-03, 6.473200e-03, 1.002420e-02,\n",
       "       1.530330e-02, 2.316180e-02, 3.453640e-02, 5.087030e-02,\n",
       "       7.324320e-02, 1.008326e-01, 1.309823e-01, 1.539425e-01,\n",
       "       1.567708e-01, 1.275575e-01, 7.401660e-02, 2.583390e-02,\n",
       "       4.230100e-03, 2.268000e-04, 3.200000e-06, 0.000000e+00,\n",
       "       0.000000e+00])"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "struc_heur.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "+ DN => -12.22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "act = 0\n",
    "observation, reward, done, info = struc_heur.step(act)\n",
    "reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Evaluation of the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 -4.000133557724439e-09 -4.000133557724439e-09 False\n",
      "1 -3.476958720938228e-06 -3.307110918449041e-06 False\n",
      "2 -0.0001687297479513461 -0.0001555857084445389 False\n",
      "3 -0.001843616697083661 -0.0017362565741066425 False\n",
      "4 -0.01013486400913699 -0.009991166652448777 False\n",
      "5 -0.03780590645985349 -0.039244656395991506 False\n",
      "6 -0.09920204552260614 -0.11216727559307135 False\n",
      "7 -0.2238554200961751 -0.2684938643789647 False\n",
      "8 -0.4413550902615526 -0.5612978487119069 False\n",
      "9 -0.7593812539308242 -1.0398974357577424 False\n",
      "10 -1.2462156899584187 -1.7860528035942904 False\n",
      "11 -1.8981349784419344 -2.865712154485238 False\n",
      "12 -2.744313284887001 -4.348629521680516 False\n",
      "13 -3.7775442486709387 -6.287801955973768 False\n",
      "14 -5.008365820033944 -8.730256652661753 False\n",
      "15 -6.437940208906534 -11.712897891741 False\n",
      "16 -8.10441202881651 -15.279865759365304 False\n",
      "17 -9.941630547547486 -19.43666365653107 False\n",
      "18 -11.958771886060315 -24.18685908084982 False\n",
      "19 -14.14826851551143 -29.525759174814922 False\n",
      "20 -16.552677114706206 -35.45966089861114 False\n",
      "21 -18.96946862093296 -41.91993398197743 False\n",
      "22 -21.807509677033153 -48.97539489483643 False\n",
      "23 -24.2698778561401 -56.43490853276856 False\n",
      "24 -26.944594454607618 -64.30243437877334 False\n",
      "25 -29.83617889510848 -72.57867930607415 False\n",
      "26 -32.88035212542173 -81.24331280423215 False\n",
      "27 -35.49525126732122 -90.1293391729294 False\n",
      "28 -38.706032097162165 -99.33467422719039 False\n",
      "29 -41.44437563022074 -108.69843165630341 True\n",
      "-108.69843165630341\n"
     ]
    }
   ],
   "source": [
    "total_rew = 0\n",
    "for episodes in range(1,2):\n",
    "    cum_reward = 0\n",
    "    struc_heur.reset()\n",
    "    for t in range(30):\n",
    "        if t%4 == 0:\n",
    "            action_ = 0\n",
    "        else:\n",
    "            action_ = 0\n",
    "        observation, reward, done, info = struc_heur.step(action_)\n",
    "        cum_reward += reward*0.95**t\n",
    "        print(t, reward, cum_reward, done)\n",
    "    total_rew += cum_reward\n",
    "    #print(episodes, total_rew)\n",
    "exp_reward = total_rew/episodes\n",
    "print(exp_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Evaluation of the environment with the trained policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "total_rew = 0\n",
    "for episodes in range(1,4):\n",
    "    cum_reward = 0\n",
    "    obs = struc_heur.reset()\n",
    "    action_ = trainer.compute_single_action(obs)\n",
    "    for t in range(30):\n",
    "        obs, reward, done, info = struc_heur.step(action_)\n",
    "        action_ = trainer.compute_single_action(obs)\n",
    "        cum_reward += reward*0.95**t\n",
    "        # print(t, reward, cum_reward, done)\n",
    "        print(t, action_)\n",
    "    total_rew += cum_reward\n",
    "    #print(episodes, total_rew)\n",
    "exp_reward = total_rew/episodes\n",
    "print(exp_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Configuration of the trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'node_ip_address': '127.0.0.1',\n",
       " 'raylet_ip_address': '127.0.0.1',\n",
       " 'redis_address': None,\n",
       " 'object_store_address': 'tcp://127.0.0.1:65094',\n",
       " 'raylet_socket_name': 'tcp://127.0.0.1:62789',\n",
       " 'webui_url': None,\n",
       " 'session_dir': 'C:\\\\Users\\\\user\\\\AppData\\\\Local\\\\Temp\\\\ray\\\\session_2022-07-03_18-29-37_429622_11440',\n",
       " 'metrics_export_port': 63003,\n",
       " 'gcs_address': '127.0.0.1:63263',\n",
       " 'address': '127.0.0.1:63263',\n",
       " 'node_id': '9f515986cdb5851653b526151abfd8fc3e7036af6568838d06aef1b3'}"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pprint\n",
    "import ray\n",
    "\n",
    "# Start a new instance of Ray (when running this tutorial locally) or\n",
    "# connect to an already running one (when running this tutorial through Anyscale).\n",
    "\n",
    "ray.init()  # Hear the engine humming? ;)\n",
    "\n",
    "# In case you encounter the following error during our tutorial: `RuntimeError: Maybe you called ray.init twice by accident?`\n",
    "# Try: `ray.shutdown() + ray.init()` or `ray.init(ignore_reinit_error=True)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "### Shutdown Ray's session\n",
    "ray.shutdown() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-03 18:30:00,031\tWARNING trainer.py:2348 -- You have specified 1 evaluation workers, but your `evaluation_interval` is None! Therefore, evaluation will not occur automatically with each call to `Trainer.train()`. Instead, you will have to call `Trainer.evaluate()` manually in order to trigger an evaluation run.\n",
      "2022-07-03 18:30:01,109\tWARNING deprecation.py:46 -- DeprecationWarning: `simple_optimizer` has been deprecated. This will raise an error in the future!\n",
      "2022-07-03 18:30:01,110\tWARNING trainer.py:2348 -- You have specified 1 evaluation workers, but your `evaluation_interval` is None! Therefore, evaluation will not occur automatically with each call to `Trainer.train()`. Instead, you will have to call `Trainer.evaluate()` manually in order to trigger an evaluation run.\n",
      "2022-07-03 18:30:01,187\tWARNING util.py:55 -- Install gputil for GPU system monitoring.\n",
      " pid=25916)\u001B[0m WARNING:tensorflow:From C:\\Users\\user\\Anaconda3\\envs\\gym\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      " pid=25916)\u001B[0m Instructions for updating:\n",
      " pid=25916)\u001B[0m If using Keras pass *_constraint arguments to layers.\n",
      " pid=7992)\u001B[0m WARNING:tensorflow:From C:\\Users\\user\\Anaconda3\\envs\\gym\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      " pid=7992)\u001B[0m Instructions for updating:\n",
      " pid=7992)\u001B[0m If using Keras pass *_constraint arguments to layers.\n"
     ]
    }
   ],
   "source": [
    "from ray.rllib.agents.dqn import DQNTrainer\n",
    "# Create an RLlib Trainer instance.\n",
    "\n",
    "config={\n",
    "        # Env class to use (here: our gym.Env sub-class from above).\n",
    "        \"env\": StructSA,\n",
    "        \n",
    "        \"env_config\": {\n",
    "            \"config\": {\"components\": 3},\n",
    "        },\n",
    "        # Number of steps after which the episode is forced to terminate. Defaults\n",
    "        # to `env.spec.max_episode_steps` (if present) for Gym envs.\n",
    "        \"horizon\": 30,\n",
    "        # Parallelize environment rollouts.\n",
    "        \"num_workers\": 1,\n",
    "        # Discount factor of the MDP.\n",
    "        \"gamma\": 0.95,\n",
    "        \n",
    "        # https://github.com/ray-project/ray/blob/releases/1.11.1/rllib/models/catalog.py\n",
    "        # FullyConnectedNetwork (tf and torch): rllib.models.tf|torch.fcnet.py\n",
    "        # These are used if no custom model is specified and the input space is 1D.\n",
    "        # Number of hidden layers to be used.\n",
    "        # Activation function descriptor.\n",
    "        # Supported values are: \"tanh\", \"relu\", \"swish\" (or \"silu\"),\n",
    "        # \"linear\" (or None).\n",
    "        \"model\": {\n",
    "            \"fcnet_hiddens\": [50],\n",
    "            \"fcnet_activation\": \"relu\"\n",
    "        },\n",
    "        \n",
    "        \"create_env_on_driver\": True,\n",
    "        \n",
    "        #\"evaluation_interval\": 2,\n",
    "        \"evaluation_num_workers\": 1,\n",
    "        \"evaluation_duration\": 50,\n",
    "        # === Deep Learning Framework Settings ===\n",
    "        # tf: TensorFlow (static-graph)\n",
    "        # tf2: TensorFlow 2.x (eager or traced, if eager_tracing=True)\n",
    "        # tfe: TensorFlow eager (or traced, if eager_tracing=True)\n",
    "        # torch: PyTorch\n",
    "#         \"framework\": \"torch\",\n",
    "    }\n",
    "\n",
    "trainer = DQNTrainer(config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Train policy and conduct evaluations periodically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 0; avg. reward=-610.0629631788396\n",
      "-213.54618459333696\n",
      "Iter: 1; avg. reward=-574.8093000168545\n",
      "Iter: 2; avg. reward=-542.9833661008147\n",
      "Iter: 3; avg. reward=-470.7264597790566\n",
      "Iter: 4; avg. reward=-401.3604766520824\n",
      "Iter: 5; avg. reward=-336.88161575437755\n",
      "-115.61829361948274\n"
     ]
    }
   ],
   "source": [
    "for i in range(6):\n",
    "    results = trainer.train()\n",
    "    #if i%100==0:\n",
    "    #trainer.export_policy_model(\"D:/14_DecomposedQ_DRL/single_agent_environment/struc_SA_jupyter/savedModel\")\n",
    "    print(f\"Iter: {i}; avg. reward={results['episode_reward_mean']}\")\n",
    "    #print(f\"Iter: {i}; evaluation={results['evaluation']['episode_reward_mean']}\")\n",
    "    \n",
    "    if i%5==0:\n",
    "        evaluat = trainer.evaluate()\n",
    "        print(evaluat['evaluation']['episode_reward_mean'])\n",
    "#         print(f\"Iter: {i}; evaluation={results['evaluation']['episode_reward_mean']}\")\n",
    "        \n",
    "''' export policy checkpoint\n",
    "def export_policy_checkpoint(\n",
    "            self,\n",
    "            export_dir: str,\n",
    "            filename_prefix: str = \"model\",\n",
    "            policy_id: PolicyID = DEFAULT_POLICY_ID,\n",
    "    )   \n",
    "'''\n",
    "PATH_model = \"D:/14_DecomposedQ_DRL/single_agent_environment/struc_SA_jupyter/savedModel\"\n",
    "\n",
    "# trainer.export_policy_checkpoint(PATH_model, filename_prefix='modelx')\n",
    "\n",
    "# trainer.save_checkpoint(PATH_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'agent_timesteps_total': 6000,\n",
      " 'custom_metrics': {},\n",
      " 'date': '2022-07-03_18-30-31',\n",
      " 'done': False,\n",
      " 'episode_len_mean': 30.0,\n",
      " 'episode_media': {},\n",
      " 'episode_reward_max': -178.17161322120927,\n",
      " 'episode_reward_mean': -336.88161575437755,\n",
      " 'episode_reward_min': -607.0112646335048,\n",
      " 'episodes_this_iter': 34,\n",
      " 'episodes_total': 200,\n",
      " 'experiment_id': '47066dc8bb4c46d8ab8e03c9d9023d69',\n",
      " 'hist_stats': {'episode_lengths': [30,\n",
      "                                    30,\n",
      "                                    30,\n",
      "                                    30,\n",
      "                                    30,\n",
      "                                    30,\n",
      "                                    30,\n",
      "                                    30,\n",
      "                                    30,\n",
      "                                    30,\n",
      "                                    30,\n",
      "                                    30,\n",
      "                                    30,\n",
      "                                    30,\n",
      "                                    30,\n",
      "                                    30,\n",
      "                                    30,\n",
      "                                    30,\n",
      "                                    30,\n",
      "                                    30,\n",
      "                                    30,\n",
      "                                    30,\n",
      "                                    30,\n",
      "                                    30,\n",
      "                                    30,\n",
      "                                    30,\n",
      "                                    30,\n",
      "                                    30,\n",
      "                                    30,\n",
      "                                    30,\n",
      "                                    30,\n",
      "                                    30,\n",
      "                                    30,\n",
      "                                    30,\n",
      "                                    30,\n",
      "                                    30,\n",
      "                                    30,\n",
      "                                    30,\n",
      "                                    30,\n",
      "                                    30,\n",
      "                                    30,\n",
      "                                    30,\n",
      "                                    30,\n",
      "                                    30,\n",
      "                                    30,\n",
      "                                    30,\n",
      "                                    30,\n",
      "                                    30,\n",
      "                                    30,\n",
      "                                    30,\n",
      "                                    30,\n",
      "                                    30,\n",
      "                                    30,\n",
      "                                    30,\n",
      "                                    30,\n",
      "                                    30,\n",
      "                                    30,\n",
      "                                    30,\n",
      "                                    30,\n",
      "                                    30,\n",
      "                                    30,\n",
      "                                    30,\n",
      "                                    30,\n",
      "                                    30,\n",
      "                                    30,\n",
      "                                    30,\n",
      "                                    30,\n",
      "                                    30,\n",
      "                                    30,\n",
      "                                    30,\n",
      "                                    30,\n",
      "                                    30,\n",
      "                                    30,\n",
      "                                    30,\n",
      "                                    30,\n",
      "                                    30,\n",
      "                                    30,\n",
      "                                    30,\n",
      "                                    30,\n",
      "                                    30,\n",
      "                                    30,\n",
      "                                    30,\n",
      "                                    30,\n",
      "                                    30,\n",
      "                                    30,\n",
      "                                    30,\n",
      "                                    30,\n",
      "                                    30,\n",
      "                                    30,\n",
      "                                    30,\n",
      "                                    30,\n",
      "                                    30,\n",
      "                                    30,\n",
      "                                    30,\n",
      "                                    30,\n",
      "                                    30,\n",
      "                                    30,\n",
      "                                    30,\n",
      "                                    30,\n",
      "                                    30],\n",
      "                'episode_reward': [-362.00531348442246,\n",
      "                                   -450.02637956197265,\n",
      "                                   -436.03288240727153,\n",
      "                                   -427.0089322087848,\n",
      "                                   -561.0003044004682,\n",
      "                                   -500.07030898122264,\n",
      "                                   -519.0078012495486,\n",
      "                                   -415.00868025004434,\n",
      "                                   -468.0026882826365,\n",
      "                                   -321.49053240403714,\n",
      "                                   -422.02370632385276,\n",
      "                                   -514.0006738434432,\n",
      "                                   -607.0112646335048,\n",
      "                                   -329.13179296617125,\n",
      "                                   -270.34480349617957,\n",
      "                                   -334.0935230003462,\n",
      "                                   -258.1526551126118,\n",
      "                                   -368.00140667017814,\n",
      "                                   -456.0028700853371,\n",
      "                                   -400.4578081732406,\n",
      "                                   -354.0530849173442,\n",
      "                                   -282.21634011890256,\n",
      "                                   -265.64930362922615,\n",
      "                                   -275.04389474363654,\n",
      "                                   -399.0592124655795,\n",
      "                                   -456.0106766225539,\n",
      "                                   -438.077789681533,\n",
      "                                   -323.34744730181376,\n",
      "                                   -342.0049315822404,\n",
      "                                   -259.4958710496717,\n",
      "                                   -461.26400723471147,\n",
      "                                   -278.0970699071312,\n",
      "                                   -353.19319593626335,\n",
      "                                   -318.1531575214708,\n",
      "                                   -343.02108746529944,\n",
      "                                   -275.0974644523211,\n",
      "                                   -394.0049216829223,\n",
      "                                   -337.2221710039973,\n",
      "                                   -376.0243844446168,\n",
      "                                   -399.0046755930306,\n",
      "                                   -289.7406595857973,\n",
      "                                   -376.0188111466226,\n",
      "                                   -520.0097960511481,\n",
      "                                   -398.109731162803,\n",
      "                                   -384.0224561222162,\n",
      "                                   -458.0007804749148,\n",
      "                                   -272.1529835718672,\n",
      "                                   -272.3745169681303,\n",
      "                                   -417.1204302809686,\n",
      "                                   -289.3989034812102,\n",
      "                                   -237.02736282632475,\n",
      "                                   -253.16592524711973,\n",
      "                                   -343.58047845031166,\n",
      "                                   -272.05098765653133,\n",
      "                                   -437.2012067738,\n",
      "                                   -255.20270541089656,\n",
      "                                   -295.52178147903373,\n",
      "                                   -361.1606811472165,\n",
      "                                   -414.0016650406665,\n",
      "                                   -196.79638244520063,\n",
      "                                   -233.22170249165288,\n",
      "                                   -402.7337022019027,\n",
      "                                   -394.01595708265404,\n",
      "                                   -178.17161322120927,\n",
      "                                   -218.3602521454027,\n",
      "                                   -257.0483688840247,\n",
      "                                   -294.02045291665917,\n",
      "                                   -317.04280162538726,\n",
      "                                   -203.77010364284064,\n",
      "                                   -277.6250594790919,\n",
      "                                   -369.12744576339037,\n",
      "                                   -230.05223629090722,\n",
      "                                   -337.04433429822143,\n",
      "                                   -332.0373348292092,\n",
      "                                   -233.6689354038802,\n",
      "                                   -322.01277322998374,\n",
      "                                   -356.0062939128476,\n",
      "                                   -352.39185332634895,\n",
      "                                   -390.0262816475271,\n",
      "                                   -241.2926642518475,\n",
      "                                   -192.871490305685,\n",
      "                                   -293.0574713161617,\n",
      "                                   -233.12676209563574,\n",
      "                                   -255.77294034675714,\n",
      "                                   -356.11950712381895,\n",
      "                                   -254.04886007422428,\n",
      "                                   -293.0181482192907,\n",
      "                                   -430.0038017662673,\n",
      "                                   -315.7560517672199,\n",
      "                                   -471.0037055835988,\n",
      "                                   -212.8742267270665,\n",
      "                                   -340.00301097754846,\n",
      "                                   -373.0050299643672,\n",
      "                                   -259.05535550634426,\n",
      "                                   -237.07150903728265,\n",
      "                                   -274.03794826166416,\n",
      "                                   -227.94064581853127,\n",
      "                                   -228.7546192368135,\n",
      "                                   -190.36703387035539,\n",
      "                                   -219.03003058182244]},\n",
      " 'hostname': 'DESKTOP-U95K42C',\n",
      " 'info': {'last_target_update_ts': 5536,\n",
      "          'learner': {'default_policy': {'custom_metrics': {},\n",
      "                                         'learner_stats': {'cur_lr': 0.0005000000237487257,\n",
      "                                                           'max_q': -4.2242985,\n",
      "                                                           'mean_q': -20.598766,\n",
      "                                                           'mean_td_error': -0.89078027,\n",
      "                                                           'min_q': -48.31362,\n",
      "                                                           'model': {}},\n",
      "                                         'td_error': array([ 0.24160147, -0.28441238,  0.08440781,  0.42092133,  0.63646317,\n",
      "       13.753422  , -5.1761084 ,  0.1998539 , -4.5266724 , -0.32470703,\n",
      "       -4.9044113 , -4.224298  , -0.4396801 , -4.7646465 ,  0.06508636,\n",
      "       -0.24923134, -4.736093  ,  0.20438385,  0.06793404, -4.9948583 ,\n",
      "        0.15790176,  0.0431385 ,  0.01760292,  0.17352247,  1.1908722 ,\n",
      "       -0.29379082, -0.21663952, -5.993967  , -0.2389803 ,  0.20900726,\n",
      "       -4.6872797 ,  0.08468866], dtype=float32)}},\n",
      "          'num_agent_steps_sampled': 6000,\n",
      "          'num_agent_steps_trained': 40032,\n",
      "          'num_steps_sampled': 6000,\n",
      "          'num_steps_trained': 40032,\n",
      "          'num_steps_trained_this_iter': 32,\n",
      "          'num_target_updates': 10},\n",
      " 'iterations_since_restore': 6,\n",
      " 'node_ip': '127.0.0.1',\n",
      " 'num_healthy_workers': 1,\n",
      " 'off_policy_estimator': {},\n",
      " 'perf': {'cpu_util_percent': 9.4, 'ram_util_percent': 26.8},\n",
      " 'pid': 11440,\n",
      " 'policy_reward_max': {},\n",
      " 'policy_reward_mean': {},\n",
      " 'policy_reward_min': {},\n",
      " 'sampler_perf': {'mean_action_processing_ms': 0.05457354530147082,\n",
      "                  'mean_env_render_ms': 0.0,\n",
      "                  'mean_env_wait_ms': 0.19869437036896037,\n",
      "                  'mean_inference_ms': 0.6349448700394975,\n",
      "                  'mean_raw_obs_processing_ms': 0.140596453975663},\n",
      " 'time_since_restore': 22.055148124694824,\n",
      " 'time_this_iter_s': 3.989499807357788,\n",
      " 'time_total_s': 22.055148124694824,\n",
      " 'timers': {'learn_throughput': 14529.659,\n",
      "            'learn_time_ms': 2.202,\n",
      "            'load_throughput': 160662.83,\n",
      "            'load_time_ms': 0.199,\n",
      "            'update_time_ms': 1.948},\n",
      " 'timestamp': 1656865831,\n",
      " 'timesteps_since_restore': 192,\n",
      " 'timesteps_this_iter': 32,\n",
      " 'timesteps_total': 6000,\n",
      " 'training_iteration': 6,\n",
      " 'trial_id': 'default'}\n"
     ]
    }
   ],
   "source": [
    "del results[\"config\"]\n",
    "pprint.pprint(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Relevant methods => Check policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "policy = trainer.get_policy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ray.rllib.models.catalog.FullyConnectedNetwork_as_DistributionalQTFModel at 0x1e51c93fac8>"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = trainer.get_policy().model\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our Policy's observation space is: Box([0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0.], [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1.], (241,), float64)\n",
      "Our Policy's action space is: Discrete(24)\n"
     ]
    }
   ],
   "source": [
    "# Print out the policy's action and observation spaces.\n",
    "print(f\"Our Policy's observation space is: {policy.observation_space}\")\n",
    "print(f\"Our Policy's action space is: {policy.action_space}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.49647475e-01, 1.98659094e-01, 9.97855666e-01, 9.67534702e-01,\n",
       "        5.73481555e-01, 7.04307302e-01, 5.62468851e-01, 3.08130601e-01,\n",
       "        8.91871071e-01, 1.51253779e-01, 1.50967371e-02, 2.00555386e-01,\n",
       "        3.89876109e-01, 7.89557344e-01, 9.75530714e-01, 2.18778612e-03,\n",
       "        2.34528466e-01, 3.85462266e-01, 1.82480210e-01, 2.87438213e-01,\n",
       "        3.66434182e-01, 1.46003751e-01, 3.78857642e-01, 9.95665012e-01,\n",
       "        7.89338130e-01, 9.12188021e-01, 3.20778832e-02, 4.80002429e-01,\n",
       "        9.09792072e-01, 4.44335792e-01, 6.79965002e-01, 8.73256638e-01,\n",
       "        1.41052361e-01, 4.41963088e-02, 3.08225769e-02, 9.84142132e-01,\n",
       "        2.55747697e-01, 9.19058891e-01, 8.91983720e-01, 8.95172290e-01,\n",
       "        1.43027340e-02, 2.80865647e-02, 9.87832092e-01, 2.34981230e-01,\n",
       "        6.23564863e-01, 6.41698085e-01, 9.37441152e-01, 2.82349040e-01,\n",
       "        6.71407780e-01, 5.00220609e-04, 8.60605001e-01, 7.10283511e-01,\n",
       "        6.04492286e-01, 6.44309111e-01, 6.30302662e-01, 5.55497421e-01,\n",
       "        4.18754610e-01, 2.67436395e-01, 9.52155310e-01, 3.77517295e-01,\n",
       "        3.71468125e-01, 9.72964171e-01, 2.13107340e-01, 6.31413126e-01,\n",
       "        8.05541088e-01, 5.38146533e-01, 3.36729809e-01, 3.98757651e-01,\n",
       "        9.86246276e-02, 1.73014262e-01, 3.18231439e-01, 1.70783213e-01,\n",
       "        6.33005484e-01, 1.12021328e-01, 1.98629533e-01, 9.82213049e-01,\n",
       "        2.61416573e-01, 1.74995134e-01, 5.59884648e-01, 5.44250324e-02,\n",
       "        2.79015107e-01, 1.26805360e-01, 1.60730102e-02, 5.79533378e-01,\n",
       "        9.04669152e-01, 1.57166941e-01, 2.14380784e-01, 2.39705777e-01,\n",
       "        2.98344101e-01, 6.43035007e-01, 1.89544148e-01, 2.01072091e-01,\n",
       "        9.98815035e-01, 8.00136440e-01, 5.56151186e-01, 4.66703293e-01,\n",
       "        8.51441046e-01, 5.70747855e-03, 7.90788081e-01, 3.06420101e-01,\n",
       "        3.51507260e-01, 9.04294681e-01, 6.08859115e-01, 5.30922923e-01,\n",
       "        4.49299678e-01, 1.51587972e-01, 6.42010338e-01, 8.44378624e-01,\n",
       "        1.14698353e-01, 3.41533843e-02, 1.41310359e-01, 7.91300130e-01,\n",
       "        3.91252994e-01, 5.28059679e-01, 1.46569333e-01, 3.33047862e-01,\n",
       "        1.55666654e-01, 5.87687601e-01, 6.55453853e-01, 1.50056330e-01,\n",
       "        9.65662991e-01, 2.97055246e-01, 9.41267432e-01, 9.87252422e-01,\n",
       "        3.52868032e-01, 4.94659620e-01, 5.54698710e-01, 5.31334540e-01,\n",
       "        7.78187503e-01, 4.08033432e-01, 3.20195106e-01, 2.41754153e-01,\n",
       "        8.93851994e-02, 2.47187820e-01, 6.20508461e-01, 9.46255898e-01,\n",
       "        5.32249762e-01, 1.74451037e-02, 1.86386789e-01, 5.44641545e-01,\n",
       "        5.81600912e-01, 1.49912556e-01, 4.19695096e-01, 5.34900399e-01,\n",
       "        4.71839147e-01, 1.53633983e-01, 6.35585098e-01, 7.31983149e-01,\n",
       "        9.08519582e-01, 3.67107150e-01, 6.19125213e-01, 1.53571141e-01,\n",
       "        6.98132768e-01, 8.77776420e-01, 8.78092768e-01, 9.32243610e-01,\n",
       "        3.86149704e-01, 7.27775207e-01, 1.90007706e-01, 8.16482227e-01,\n",
       "        6.86634099e-01, 4.61925134e-01, 9.39848917e-01, 6.20776160e-01,\n",
       "        7.66131148e-01, 1.92736051e-01, 9.11483070e-01, 1.01497710e-01,\n",
       "        9.17470285e-02, 8.18371777e-02, 7.73596316e-01, 3.55858526e-01,\n",
       "        3.03177076e-01, 3.33973149e-01, 4.29887215e-01, 4.02578584e-01,\n",
       "        3.20111195e-01, 2.74998804e-01, 7.57981266e-01, 7.55271090e-01,\n",
       "        3.61342706e-02, 1.51861580e-01, 5.17770588e-01, 4.43349763e-01,\n",
       "        9.55956026e-01, 8.06635434e-01, 7.54329751e-01, 5.16730205e-01,\n",
       "        1.21412151e-01, 4.17811896e-01, 2.10162541e-01, 7.05630747e-01,\n",
       "        2.71453238e-01, 5.56719786e-01, 2.34998426e-01, 9.46517098e-01,\n",
       "        8.11354700e-01, 6.73507316e-01, 5.64482100e-01, 8.67249559e-01,\n",
       "        6.21783568e-01, 8.50057290e-01, 6.51107608e-01, 8.16460251e-01,\n",
       "        5.89830654e-01, 6.74586008e-01, 9.61892315e-01, 8.17230241e-01,\n",
       "        5.55307086e-01, 5.68087079e-01, 9.99334609e-01, 3.00990563e-01,\n",
       "        7.98147289e-01, 6.51644242e-01, 7.41792867e-02, 1.60493499e-01,\n",
       "        4.42113790e-01, 9.99725150e-01, 5.20409036e-02, 5.58466990e-01,\n",
       "        8.90915874e-01, 5.66239561e-01, 8.79855632e-01, 8.77631731e-01,\n",
       "        7.62979279e-01, 9.99884091e-01, 2.86033699e-01, 9.70153758e-01,\n",
       "        3.86221262e-01, 1.62017499e-01, 4.52069934e-01, 2.33207296e-02,\n",
       "        1.81310058e-01, 3.28709020e-01, 8.60696917e-01, 2.96358321e-01,\n",
       "        6.89990442e-01, 6.03591036e-02, 5.23467153e-01, 2.12054651e-01,\n",
       "        9.26452997e-01]])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Produce a random obervation (B=1; batch of size 1).\n",
    "obs = np.array([policy.observation_space.sample()])\n",
    "# Alternatively for PyTorch:\n",
    "#import torch\n",
    "#obs = torch.from_numpy(obs)\n",
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'model_1/fc_out/Relu:0' shape=(1, 50) dtype=float32>"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits, _ = model({\"obs\": obs})\n",
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 50)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits_np = policy.get_session().run(logits)\n",
    "logits_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9.99999997e-07, 2.52637593e-03, 9.99999997e-07, 3.36667709e-02,\n",
       "       9.99999997e-07, 7.53862550e-03, 1.16389645e-02, 1.10806234e-01,\n",
       "       9.99999997e-07, 9.99999997e-07, 5.46629541e-03, 9.99999997e-07,\n",
       "       5.45822131e-03, 9.99999997e-07, 9.99999997e-07, 5.21160720e-04,\n",
       "       9.67060104e-02, 9.99999997e-07, 6.54321313e-02, 9.99999997e-07,\n",
       "       9.99999997e-07, 9.99999997e-07, 2.38046553e-02, 4.99389060e-02,\n",
       "       4.55672555e-02, 3.84213141e-04, 1.47604469e-05, 5.44570908e-02,\n",
       "       9.99999997e-07, 2.57992037e-02, 5.26364446e-02, 9.99999997e-07,\n",
       "       1.95286050e-02, 9.09144001e-06, 9.13316663e-03, 6.31047934e-02,\n",
       "       1.87054388e-02, 7.38888932e-03, 9.99999997e-07, 1.95658319e-02,\n",
       "       9.99999997e-07, 1.11107498e-01, 9.99999997e-07, 9.99999997e-07,\n",
       "       1.08681709e-01, 9.99999997e-07, 5.03034182e-02, 9.99999997e-07,\n",
       "       9.99999997e-07, 1.07398111e-04], dtype=float32)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ray.rllib.utils.numpy import softmax\n",
    "action_probs = np.squeeze(softmax(logits_np))\n",
    "action_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Action scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "(action_scores, logits, dist) = model.get_q_value_distributions(model_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "action_scores.graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Definition of action transformation (change of base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def convert_base_action(action_, base, comp):\n",
    "    action_multi = np.zeros((comp,), dtype=int)\n",
    "    if action_ == 0:\n",
    "            return action_multi\n",
    "    digits = []\n",
    "    index_comp = int(comp) - 1 \n",
    "    while action_:\n",
    "        digits = (int(action_ % base))\n",
    "        #print(digits)\n",
    "        action_multi[index_comp] = digits\n",
    "        action_ //= base\n",
    "        index_comp -= 1\n",
    "        #print(index_comp, action_multi)\n",
    "    return action_multi\n",
    "\n",
    "# Testing\n",
    "# action_test = convert_base_action(5, 3, 2)\n",
    "# action_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## If Ray does not start..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### To be investigated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "https://docs.ray.io/en/latest/rllib/rllib-training.html#accessing-policy-state\n",
    "\n",
    "help(trainer.get_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "trainer.get_policy().export_model(PATH_model+'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "rr = trainer.get_policy().q_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dir(trainer.get_policy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model.save('my_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ee = model.value_function()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(action_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "action_scores.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "help(model.base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model.base_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model.base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model_out = model.__call__({\"obs\": np.array([struc_heur.reset()])})\n",
    "model_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "with tf.Session():\n",
    "    model_out[0].eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dir(model_out[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(model_out[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Evaluate the trained policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "evaluation = trainer.evaluate()\n",
    "evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Storing and restoring checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "PATH_model = \"D:/14_DecomposedQ_DRL/single_agent_environment/struc_SA_jupyter/savedModel\"\n",
    "trainer.save(PATH_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "trainer.restore(PATH_model+'/checkpoint_000000/checkpoint-0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Get action from the trained policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-03 17:39:01,663\tERROR tf_run_builder.py:47 -- Error fetching: [<tf.Tensor 'default_policy/cond/Merge:0' shape=(?,) dtype=int64>, {'action_prob': <tf.Tensor 'default_policy/Exp:0' shape=(?,) dtype=float32>, 'action_logp': <tf.Tensor 'default_policy/zeros_like_1:0' shape=(?,) dtype=float32>, 'action_dist_inputs': <tf.Tensor 'default_policy/add_2:0' shape=(?, 6) dtype=float32>, 'q_values': <tf.Tensor 'default_policy/add_2:0' shape=(?, 6) dtype=float32>}], feed_dict={<tf.Tensor 'default_policy/obs:0' shape=(?, 61) dtype=float32>: array([[1.052000e-04, 5.500000e-05, 8.660000e-05, 1.261000e-04,\n",
      "        2.006000e-04, 3.173000e-04, 4.853000e-04, 7.444000e-04,\n",
      "        1.138400e-03, 1.783100e-03, 2.713600e-03, 4.235700e-03,\n",
      "        6.473200e-03, 1.002420e-02, 1.530330e-02, 2.316180e-02,\n",
      "        3.453640e-02, 5.087030e-02, 7.324320e-02, 1.008326e-01,\n",
      "        1.309823e-01, 1.539425e-01, 1.567708e-01, 1.275575e-01,\n",
      "        7.401660e-02, 2.583390e-02, 4.230100e-03, 2.268000e-04,\n",
      "        3.200000e-06, 0.000000e+00, 1.052000e-04, 5.500000e-05,\n",
      "        8.660000e-05, 1.261000e-04, 2.006000e-04, 3.173000e-04,\n",
      "        4.853000e-04, 7.444000e-04, 1.138400e-03, 1.783100e-03,\n",
      "        2.713600e-03, 4.235700e-03, 6.473200e-03, 1.002420e-02,\n",
      "        1.530330e-02, 2.316180e-02, 3.453640e-02, 5.087030e-02,\n",
      "        7.324320e-02, 1.008326e-01, 1.309823e-01, 1.539425e-01,\n",
      "        1.567708e-01, 1.275575e-01, 7.401660e-02, 2.583390e-02,\n",
      "        4.230100e-03, 2.268000e-04, 3.200000e-06, 0.000000e+00,\n",
      "        1.052000e-04, 5.500000e-05, 8.660000e-05, 1.261000e-04,\n",
      "        2.006000e-04, 3.173000e-04, 4.853000e-04, 7.444000e-04,\n",
      "        1.138400e-03, 1.783100e-03, 2.713600e-03, 4.235700e-03,\n",
      "        6.473200e-03, 1.002420e-02, 1.530330e-02, 2.316180e-02,\n",
      "        3.453640e-02, 5.087030e-02, 7.324320e-02, 1.008326e-01,\n",
      "        1.309823e-01, 1.539425e-01, 1.567708e-01, 1.275575e-01,\n",
      "        7.401660e-02, 2.583390e-02, 4.230100e-03, 2.268000e-04,\n",
      "        3.200000e-06, 0.000000e+00, 1.052000e-04, 5.500000e-05,\n",
      "        8.660000e-05, 1.261000e-04, 2.006000e-04, 3.173000e-04,\n",
      "        4.853000e-04, 7.444000e-04, 1.138400e-03, 1.783100e-03,\n",
      "        2.713600e-03, 4.235700e-03, 6.473200e-03, 1.002420e-02,\n",
      "        1.530330e-02, 2.316180e-02, 3.453640e-02, 5.087030e-02,\n",
      "        7.324320e-02, 1.008326e-01, 1.309823e-01, 1.539425e-01,\n",
      "        1.567708e-01, 1.275575e-01, 7.401660e-02, 2.583390e-02,\n",
      "        4.230100e-03, 2.268000e-04, 3.200000e-06, 0.000000e+00,\n",
      "        1.052000e-04, 5.500000e-05, 8.660000e-05, 1.261000e-04,\n",
      "        2.006000e-04, 3.173000e-04, 4.853000e-04, 7.444000e-04,\n",
      "        1.138400e-03, 1.783100e-03, 2.713600e-03, 4.235700e-03,\n",
      "        6.473200e-03, 1.002420e-02, 1.530330e-02, 2.316180e-02,\n",
      "        3.453640e-02, 5.087030e-02, 7.324320e-02, 1.008326e-01,\n",
      "        1.309823e-01, 1.539425e-01, 1.567708e-01, 1.275575e-01,\n",
      "        7.401660e-02, 2.583390e-02, 4.230100e-03, 2.268000e-04,\n",
      "        3.200000e-06, 0.000000e+00, 0.000000e+00]]), <tf.Tensor 'default_policy/is_training:0' shape=() dtype=bool>: False, <tf.Tensor 'default_policy/is_exploring:0' shape=() dtype=bool>: True, <tf.Tensor 'default_policy/timestep:0' shape=() dtype=int64>: 6000}\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\user\\Anaconda3\\envs\\gym\\lib\\site-packages\\ray\\rllib\\utils\\tf_run_builder.py\", line 44, in get\n",
      "    self.feed_dict, os.environ.get(\"TF_TIMELINE_DIR\"))\n",
      "  File \"C:\\Users\\user\\Anaconda3\\envs\\gym\\lib\\site-packages\\ray\\rllib\\utils\\tf_run_builder.py\", line 92, in run_timeline\n",
      "    fetches = sess.run(ops, feed_dict=feed_dict)\n",
      "  File \"C:\\Users\\user\\Anaconda3\\envs\\gym\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\", line 956, in run\n",
      "    run_metadata_ptr)\n",
      "  File \"C:\\Users\\user\\Anaconda3\\envs\\gym\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\", line 1156, in _run\n",
      "    (np_val.shape, subfeed_t.name, str(subfeed_t.get_shape())))\n",
      "ValueError: Cannot feed value of shape (1, 151) for Tensor 'default_policy/obs:0', which has shape '(?, 61)'\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot feed value of shape (1, 151) for Tensor 'default_policy/obs:0', which has shape '(?, 61)'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-9-310678cacf54>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[0mtrainer\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcompute_single_action\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mstruc_heur\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mreset\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32m~\\Anaconda3\\envs\\gym\\lib\\site-packages\\ray\\rllib\\agents\\trainer.py\u001B[0m in \u001B[0;36mcompute_single_action\u001B[1;34m(self, observation, state, prev_action, prev_reward, info, input_dict, policy_id, full_fetch, explore, timestep, episode, unsquash_action, clip_action, unsquash_actions, clip_actions, **kwargs)\u001B[0m\n\u001B[0;32m   1476\u001B[0m                 \u001B[0mexplore\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mexplore\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1477\u001B[0m                 \u001B[0mtimestep\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mtimestep\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1478\u001B[1;33m                 \u001B[0mepisode\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mepisode\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1479\u001B[0m             )\n\u001B[0;32m   1480\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Anaconda3\\envs\\gym\\lib\\site-packages\\ray\\rllib\\policy\\policy.py\u001B[0m in \u001B[0;36mcompute_single_action\u001B[1;34m(self, obs, state, prev_action, prev_reward, info, input_dict, episode, explore, timestep, **kwargs)\u001B[0m\n\u001B[0;32m    220\u001B[0m             \u001B[0mepisodes\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mepisodes\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    221\u001B[0m             \u001B[0mexplore\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mexplore\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 222\u001B[1;33m             \u001B[0mtimestep\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mtimestep\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    223\u001B[0m         )\n\u001B[0;32m    224\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Anaconda3\\envs\\gym\\lib\\site-packages\\ray\\rllib\\policy\\tf_policy.py\u001B[0m in \u001B[0;36mcompute_actions_from_input_dict\u001B[1;34m(self, input_dict, explore, timestep, episodes, **kwargs)\u001B[0m\n\u001B[0;32m    299\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    300\u001B[0m         \u001B[1;31m# Execute session run to get action (and other fetches).\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 301\u001B[1;33m         \u001B[0mfetched\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mbuilder\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mget\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mto_fetch\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    302\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    303\u001B[0m         \u001B[1;31m# Update our global timestep by the batch size.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Anaconda3\\envs\\gym\\lib\\site-packages\\ray\\rllib\\utils\\tf_run_builder.py\u001B[0m in \u001B[0;36mget\u001B[1;34m(self, to_fetch)\u001B[0m\n\u001B[0;32m     46\u001B[0m                 logger.exception(\"Error fetching: {}, feed_dict={}\".format(\n\u001B[0;32m     47\u001B[0m                     self.fetches, self.feed_dict))\n\u001B[1;32m---> 48\u001B[1;33m                 \u001B[1;32mraise\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     49\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mto_fetch\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mint\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     50\u001B[0m             \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_executed\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mto_fetch\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Anaconda3\\envs\\gym\\lib\\site-packages\\ray\\rllib\\utils\\tf_run_builder.py\u001B[0m in \u001B[0;36mget\u001B[1;34m(self, to_fetch)\u001B[0m\n\u001B[0;32m     42\u001B[0m                 self._executed = run_timeline(\n\u001B[0;32m     43\u001B[0m                     \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msession\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfetches\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdebug_name\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 44\u001B[1;33m                     self.feed_dict, os.environ.get(\"TF_TIMELINE_DIR\"))\n\u001B[0m\u001B[0;32m     45\u001B[0m             \u001B[1;32mexcept\u001B[0m \u001B[0mException\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     46\u001B[0m                 logger.exception(\"Error fetching: {}, feed_dict={}\".format(\n",
      "\u001B[1;32m~\\Anaconda3\\envs\\gym\\lib\\site-packages\\ray\\rllib\\utils\\tf_run_builder.py\u001B[0m in \u001B[0;36mrun_timeline\u001B[1;34m(sess, ops, debug_name, feed_dict, timeline_dir)\u001B[0m\n\u001B[0;32m     90\u001B[0m                 \u001B[1;34m\"Executing TF run without tracing. To dump TF timeline traces \"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     91\u001B[0m                 \"to disk, set the TF_TIMELINE_DIR environment variable.\")\n\u001B[1;32m---> 92\u001B[1;33m         \u001B[0mfetches\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0msess\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mrun\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mops\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfeed_dict\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mfeed_dict\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     93\u001B[0m     \u001B[1;32mreturn\u001B[0m \u001B[0mfetches\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Anaconda3\\envs\\gym\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001B[0m in \u001B[0;36mrun\u001B[1;34m(self, fetches, feed_dict, options, run_metadata)\u001B[0m\n\u001B[0;32m    954\u001B[0m     \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    955\u001B[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001B[1;32m--> 956\u001B[1;33m                          run_metadata_ptr)\n\u001B[0m\u001B[0;32m    957\u001B[0m       \u001B[1;32mif\u001B[0m \u001B[0mrun_metadata\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    958\u001B[0m         \u001B[0mproto_data\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtf_session\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mTF_GetBuffer\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mrun_metadata_ptr\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Anaconda3\\envs\\gym\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001B[0m in \u001B[0;36m_run\u001B[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001B[0m\n\u001B[0;32m   1154\u001B[0m                 \u001B[1;34m'Cannot feed value of shape %r for Tensor %r, '\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1155\u001B[0m                 \u001B[1;34m'which has shape %r'\u001B[0m \u001B[1;33m%\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1156\u001B[1;33m                 (np_val.shape, subfeed_t.name, str(subfeed_t.get_shape())))\n\u001B[0m\u001B[0;32m   1157\u001B[0m           \u001B[1;32mif\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mgraph\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mis_feedable\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0msubfeed_t\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1158\u001B[0m             \u001B[1;32mraise\u001B[0m \u001B[0mValueError\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'Tensor %s may not be fed.'\u001B[0m \u001B[1;33m%\u001B[0m \u001B[0msubfeed_t\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mValueError\u001B[0m: Cannot feed value of shape (1, 151) for Tensor 'default_policy/obs:0', which has shape '(?, 61)'"
     ]
    }
   ],
   "source": [
    "trainer.compute_single_action(struc_heur.reset())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Load checkpoint (it required setting up the same configuration as during training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "trainer.load_checkpoint(\"D:/14_DecomposedQ_DRL/single_agent_environment/struc_SA_jupyter/savedModel/checkpoint-52\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Running from the console"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!rllib train --run DQN --env CartPole-v0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Additional test code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for i in range(1):\n",
    "    results = trainer.train()\n",
    "    #if i%100==0:\n",
    "    print(f\"Iter: {i}; avg. reward={results['episode_reward_mean']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from ray.rllib.algorithms.ppo import PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dir(agents.qmix.QMixTrainer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python gym",
   "language": "python",
   "name": "gym"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}