{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import matplotlib.pyplot as plt \n",
    "import scipy.io as spio\n",
    "import numpy as np\n",
    "import os\n",
    "# import gym_strucSA\n",
    "from ray.rllib.env.multi_agent_env import MultiAgentEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class StructMA(MultiAgentEnv):\n",
    "\n",
    "    def __init__(self, config=None):\n",
    "        empty_config = {\"config\": {\"components\": 2} }\n",
    "        config_ = config or empty_config\n",
    "        # Number of components #\n",
    "        self.ncomp = config_['config'][\"components\"]\n",
    "        self.time = 0\n",
    "        self.ep_length = 30\n",
    "        self.nstcomp = 30\n",
    "        self.nobs = 2\n",
    "        self.actions_total = int(3)\n",
    "        self.obs_total = int(30 + 31 + 31)\n",
    "\n",
    "        # configure spaces\n",
    "        self.action_space = gym.spaces.Discrete(self.actions_total)\n",
    "        self.observation_space = gym.spaces.Box(low=0.0, high=1.0, shape=(self.obs_total,), dtype=np.float64)\n",
    "        ### Loading the underlying POMDP model ###\n",
    "        drmodel = np.load('Dr3031C10.npz')\n",
    "        self.belief0 = drmodel['belief0'][0,0:self.ncomp,:,0] # (10 components, 30 crack states)\n",
    "        self.P = drmodel['P'][:,0:self.ncomp,:,:,:] # (3 actions, 10 components, 31 det rates, 30 cracks, 30 cracks)\n",
    "        self.O = drmodel['O'][:,0:self.ncomp,:,:] # (3 actions, 10 components, 30 cracks, 2 observations)\n",
    "        \n",
    "        self.agent_list = []\n",
    "        for i in range(self.ncomp):\n",
    "            item = \"agent_\"+ str(i)\n",
    "            self.agent_list.append(item)\n",
    "        self._agent_ids = self.agent_list\n",
    "        # Reset env.\n",
    "        self.reset()\n",
    "            \n",
    "    def reset(self):\n",
    "        # We need the following line to seed self.np_random\n",
    "        # super().reset(seed=seed)\n",
    "\n",
    "        # Choose the agent's belief\n",
    "        self.time_step = 0\n",
    "        self.agent_belief = self.belief0\n",
    "        self.drate = np.zeros((self.ncomp, 1), dtype=int)\n",
    "        observations = {}\n",
    "        for i in range(self.ncomp):\n",
    "            observations[self.agent_list[i]] = np.concatenate( (self.agent_belief[i] , \\\n",
    "                self.one_hot_drate(self.drate, self.ep_length+1)[i] , self.one_hot_time(self.time_step ,self.ep_length+1) ) ) \n",
    " \n",
    "        return observations\n",
    "    \n",
    "    def step(self, action: dict):\n",
    "        action_ = np.zeros(self.ncomp, dtype=int)\n",
    "        for i in range(self.ncomp):\n",
    "            action_[i] = action[self.agent_list[i]]\n",
    "\n",
    "        observation_, belief_prime, drate_prime = self.belief_update(self.agent_belief, action_, self.drate)\n",
    "        \n",
    "        observations = {}\n",
    "        for i in range(self.ncomp):\n",
    "            observations[self.agent_list[i]] = np.concatenate( (belief_prime[i] , \\\n",
    "                self.one_hot_drate(drate_prime, self.ep_length+1)[i] , self.one_hot_time(self.time_step+1 ,self.ep_length+1) ) )\n",
    "        reward_ = self.immediate_cost(self.agent_belief, action_, belief_prime, self.drate)\n",
    "        reward = reward_.item() #Convert float64 to float\n",
    "        \n",
    "        rewards = {}\n",
    "        for i in range(self.ncomp):\n",
    "            rewards[self.agent_list[i]] = reward\n",
    "            \n",
    "        self.time_step += 1 \n",
    "        self.agent_belief = belief_prime\n",
    "        self.drate = drate_prime\n",
    "        # An episode is done if the agent has reached the target\n",
    "        done = np.array_equal(self.time_step, self.ep_length)\n",
    "        dones = {\"__all__\": done}\n",
    "        # info = {\"belief\": self.agent_belief}\n",
    "        return observations, rewards, dones, {} \n",
    "    \n",
    "    \n",
    "    def pf_sys(self, pf, k): # compute pf_sys for k-out-of-n components \n",
    "        n = pf.size\n",
    "        # k = ncomp-1\n",
    "        PF_sys = np.zeros(1)\n",
    "        nk = n-k\n",
    "        m = k+1\n",
    "        A = np.zeros(m+1)\n",
    "        A [1] = 1\n",
    "        L = 1\n",
    "        for j in range(1,n+1):\n",
    "            h = j + 1\n",
    "            Rel = 1-pf[j-1]\n",
    "            if nk < j:\n",
    "                L = h - nk\n",
    "            if k < j:\n",
    "                A[m] = A[m] + A[k]*Rel\n",
    "                h = k\n",
    "            for i in range(h, L-1, -1):\n",
    "                A[i] = A[i] + (A[i-1]-A[i])*Rel\n",
    "        PF_sys = 1-A[m]\n",
    "        return PF_sys  \n",
    "    \n",
    "    def immediate_cost(self, B, a, B_, drate): # immediate reward (-cost), based on current damage state and action#\n",
    "        cost_system = 0\n",
    "        PF = np.zeros((1,1))\n",
    "        PF = B[:,-1]\n",
    "        PF_ = np.zeros((1,1))\n",
    "        PF_ = B_[:,-1].copy()\n",
    "        for i in range(self.ncomp):\n",
    "            if a[i]==1:\n",
    "                cost_system += -1\n",
    "                Bplus = self.P[a[i],i,drate[i,0]].T.dot(B[i,:])\n",
    "                PF_[i] = Bplus[-1]         \n",
    "            elif a[i]==2:\n",
    "                cost_system +=  - 20\n",
    "        if self.ncomp < 2: # single component setting\n",
    "            PfSyS_ = PF_\n",
    "            PfSyS = PF\n",
    "        else:\n",
    "            PfSyS_ = self.pf_sys(PF_, self.ncomp-1) \n",
    "            PfSyS = self.pf_sys(PF, self.ncomp-1) \n",
    "        if PfSyS_ < PfSyS:\n",
    "            cost_system += PfSyS_*(-10000)\n",
    "        else:\n",
    "            cost_system += (PfSyS_-PfSyS)*(-10000) \n",
    "        return cost_system\n",
    "    \n",
    "    def belief_update(self, b, a, drate):  # Bayesian belief update based on previous belief, current observation, and action taken\n",
    "        b_prime = np.zeros((self.ncomp, self.nstcomp))\n",
    "        b_prime[:] = b\n",
    "        ob = np.zeros(self.ncomp)\n",
    "        drate_prime = np.zeros((self.ncomp, 1), dtype=int)\n",
    "        for i in range(self.ncomp):\n",
    "            p1 = self.P[a[i],i,drate[i,0]].T.dot(b_prime[i,:])  # environment transition\n",
    "            b_prime[i,:] = p1\n",
    "            drate_prime[i, 0] = drate[i, 0] + 1\n",
    "            ob[i] = 2\n",
    "            if a[i]==1:\n",
    "                Obs0 = np.sum(p1* self.O[a[i],i,:,0])\n",
    "                Obs1 = 1 - Obs0\n",
    "                if Obs1 < 1e-5:\n",
    "                    ob[i] = 0\n",
    "                else:\n",
    "                    ob_dist = np.array([Obs0, Obs1])\n",
    "                    ob[i] = np.random.choice(range(0,self.nobs), size=None, replace=True, p=ob_dist)           \n",
    "                b_prime[i,:] = p1* self.O[a[i],i,:,int(ob[i])]/(p1.dot(self.O[a[i],i,:,int(ob[i])])) # belief update\n",
    "            if a[i] == 2:\n",
    "                drate_prime[i, 0] = 0\n",
    "        return ob, b_prime, drate_prime\n",
    "    \n",
    "    def one_hot_drate(self, drate, ep_length):\n",
    "        ohDrate = np.zeros((self.ncomp, ep_length), dtype=int)\n",
    "        for i in range(self.ncomp):\n",
    "            ohDrate[i, drate[i][0]] = 1\n",
    "        return ohDrate\n",
    "\n",
    "    def one_hot_time(self, time, ep_length):\n",
    "        ohTime = np.zeros((ep_length), dtype=int)\n",
    "        ohTime[time] = 1\n",
    "        return ohTime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'agent_0': array([1.052000e-04, 5.500000e-05, 8.660000e-05, 1.261000e-04,\n",
       "        2.006000e-04, 3.173000e-04, 4.853000e-04, 7.444000e-04,\n",
       "        1.138400e-03, 1.783100e-03, 2.713600e-03, 4.235700e-03,\n",
       "        6.473200e-03, 1.002420e-02, 1.530330e-02, 2.316180e-02,\n",
       "        3.453640e-02, 5.087030e-02, 7.324320e-02, 1.008326e-01,\n",
       "        1.309823e-01, 1.539425e-01, 1.567708e-01, 1.275575e-01,\n",
       "        7.401660e-02, 2.583390e-02, 4.230100e-03, 2.268000e-04,\n",
       "        3.200000e-06, 0.000000e+00, 1.000000e+00, 0.000000e+00,\n",
       "        0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00,\n",
       "        0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00,\n",
       "        0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00,\n",
       "        0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00,\n",
       "        0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00,\n",
       "        0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00,\n",
       "        0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00,\n",
       "        0.000000e+00, 1.000000e+00, 0.000000e+00, 0.000000e+00,\n",
       "        0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00,\n",
       "        0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00,\n",
       "        0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00,\n",
       "        0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00,\n",
       "        0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00,\n",
       "        0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00,\n",
       "        0.000000e+00, 0.000000e+00, 0.000000e+00]),\n",
       " 'agent_1': array([1.052000e-04, 5.500000e-05, 8.660000e-05, 1.261000e-04,\n",
       "        2.006000e-04, 3.173000e-04, 4.853000e-04, 7.444000e-04,\n",
       "        1.138400e-03, 1.783100e-03, 2.713600e-03, 4.235700e-03,\n",
       "        6.473200e-03, 1.002420e-02, 1.530330e-02, 2.316180e-02,\n",
       "        3.453640e-02, 5.087030e-02, 7.324320e-02, 1.008326e-01,\n",
       "        1.309823e-01, 1.539425e-01, 1.567708e-01, 1.275575e-01,\n",
       "        7.401660e-02, 2.583390e-02, 4.230100e-03, 2.268000e-04,\n",
       "        3.200000e-06, 0.000000e+00, 1.000000e+00, 0.000000e+00,\n",
       "        0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00,\n",
       "        0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00,\n",
       "        0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00,\n",
       "        0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00,\n",
       "        0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00,\n",
       "        0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00,\n",
       "        0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00,\n",
       "        0.000000e+00, 1.000000e+00, 0.000000e+00, 0.000000e+00,\n",
       "        0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00,\n",
       "        0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00,\n",
       "        0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00,\n",
       "        0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00,\n",
       "        0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00,\n",
       "        0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00,\n",
       "        0.000000e+00, 0.000000e+00, 0.000000e+00])}"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strucMA_heur = StructMA()\n",
    "strucMA_heur.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Initialization of the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "strucMA_heur = StructMA()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "config = {\"config\": {\"components\": 5} }\n",
    "strucMA_heur = StructMA(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'agent_0': array([1.052000e-04, 5.500000e-05, 8.660000e-05, 1.261000e-04,\n",
       "        2.006000e-04, 3.173000e-04, 4.853000e-04, 7.444000e-04,\n",
       "        1.138400e-03, 1.783100e-03, 2.713600e-03, 4.235700e-03,\n",
       "        6.473200e-03, 1.002420e-02, 1.530330e-02, 2.316180e-02,\n",
       "        3.453640e-02, 5.087030e-02, 7.324320e-02, 1.008326e-01,\n",
       "        1.309823e-01, 1.539425e-01, 1.567708e-01, 1.275575e-01,\n",
       "        7.401660e-02, 2.583390e-02, 4.230100e-03, 2.268000e-04,\n",
       "        3.200000e-06, 0.000000e+00, 1.000000e+00, 0.000000e+00,\n",
       "        0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00,\n",
       "        0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00,\n",
       "        0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00,\n",
       "        0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00,\n",
       "        0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00,\n",
       "        0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00,\n",
       "        0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00,\n",
       "        0.000000e+00, 1.000000e+00, 0.000000e+00, 0.000000e+00,\n",
       "        0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00,\n",
       "        0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00,\n",
       "        0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00,\n",
       "        0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00,\n",
       "        0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00,\n",
       "        0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00,\n",
       "        0.000000e+00, 0.000000e+00, 0.000000e+00]),\n",
       " 'agent_1': array([1.052000e-04, 5.500000e-05, 8.660000e-05, 1.261000e-04,\n",
       "        2.006000e-04, 3.173000e-04, 4.853000e-04, 7.444000e-04,\n",
       "        1.138400e-03, 1.783100e-03, 2.713600e-03, 4.235700e-03,\n",
       "        6.473200e-03, 1.002420e-02, 1.530330e-02, 2.316180e-02,\n",
       "        3.453640e-02, 5.087030e-02, 7.324320e-02, 1.008326e-01,\n",
       "        1.309823e-01, 1.539425e-01, 1.567708e-01, 1.275575e-01,\n",
       "        7.401660e-02, 2.583390e-02, 4.230100e-03, 2.268000e-04,\n",
       "        3.200000e-06, 0.000000e+00, 1.000000e+00, 0.000000e+00,\n",
       "        0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00,\n",
       "        0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00,\n",
       "        0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00,\n",
       "        0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00,\n",
       "        0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00,\n",
       "        0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00,\n",
       "        0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00,\n",
       "        0.000000e+00, 1.000000e+00, 0.000000e+00, 0.000000e+00,\n",
       "        0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00,\n",
       "        0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00,\n",
       "        0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00,\n",
       "        0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00,\n",
       "        0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00,\n",
       "        0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00,\n",
       "        0.000000e+00, 0.000000e+00, 0.000000e+00])}"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strucMA_heur.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "+ DN => -12.22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'agent_0': 2, 'agent_1': 2}\n"
     ]
    }
   ],
   "source": [
    "agent_list = []\n",
    "for i in range(strucMA_heur.ncomp):\n",
    "    item = \"agent_\"+ str(i)\n",
    "    agent_list.append(item)\n",
    "# print(agent_list)\n",
    "act = {}\n",
    "for i in range(strucMA_heur.ncomp):\n",
    "    act[agent_list[i]] = 2 # Assign action to all components\n",
    "print(act)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 12\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'agent_0': -40.0, 'agent_1': -40.0}"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observation, reward, done, info = strucMA_heur.step(act)\n",
    "reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Evaluation of the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 {'agent_0': -4.000133557724439e-09, 'agent_1': -4.000133557724439e-09, 'agent_2': -4.000133557724439e-09, 'agent_3': -4.000133557724439e-09, 'agent_4': -4.000133557724439e-09} -4.000133557724439e-09 {'__all__': False}\n",
      "1 {'agent_0': -3.476958720938228e-06, 'agent_1': -3.476958720938228e-06, 'agent_2': -3.476958720938228e-06, 'agent_3': -3.476958720938228e-06, 'agent_4': -3.476958720938228e-06} -3.307110918449041e-06 {'__all__': False}\n",
      "2 {'agent_0': -0.0001687297479513461, 'agent_1': -0.0001687297479513461, 'agent_2': -0.0001687297479513461, 'agent_3': -0.0001687297479513461, 'agent_4': -0.0001687297479513461} -0.0001555857084445389 {'__all__': False}\n",
      "3 {'agent_0': -0.001843616697083661, 'agent_1': -0.001843616697083661, 'agent_2': -0.001843616697083661, 'agent_3': -0.001843616697083661, 'agent_4': -0.001843616697083661} -0.0017362565741066425 {'__all__': False}\n",
      "4 {'agent_0': -0.01013486400913699, 'agent_1': -0.01013486400913699, 'agent_2': -0.01013486400913699, 'agent_3': -0.01013486400913699, 'agent_4': -0.01013486400913699} -0.009991166652448777 {'__all__': False}\n",
      "5 {'agent_0': -0.03780590645985349, 'agent_1': -0.03780590645985349, 'agent_2': -0.03780590645985349, 'agent_3': -0.03780590645985349, 'agent_4': -0.03780590645985349} -0.039244656395991506 {'__all__': False}\n",
      "6 {'agent_0': -0.09920204552260614, 'agent_1': -0.09920204552260614, 'agent_2': -0.09920204552260614, 'agent_3': -0.09920204552260614, 'agent_4': -0.09920204552260614} -0.11216727559307135 {'__all__': False}\n",
      "7 {'agent_0': -0.2238554200961751, 'agent_1': -0.2238554200961751, 'agent_2': -0.2238554200961751, 'agent_3': -0.2238554200961751, 'agent_4': -0.2238554200961751} -0.2684938643789647 {'__all__': False}\n",
      "8 {'agent_0': -0.4413550902615526, 'agent_1': -0.4413550902615526, 'agent_2': -0.4413550902615526, 'agent_3': -0.4413550902615526, 'agent_4': -0.4413550902615526} -0.5612978487119069 {'__all__': False}\n",
      "9 {'agent_0': -0.7593812539308242, 'agent_1': -0.7593812539308242, 'agent_2': -0.7593812539308242, 'agent_3': -0.7593812539308242, 'agent_4': -0.7593812539308242} -1.0398974357577424 {'__all__': False}\n",
      "10 {'agent_0': -1.2462156899584187, 'agent_1': -1.2462156899584187, 'agent_2': -1.2462156899584187, 'agent_3': -1.2462156899584187, 'agent_4': -1.2462156899584187} -1.7860528035942904 {'__all__': False}\n",
      "11 {'agent_0': -1.8981349784419344, 'agent_1': -1.8981349784419344, 'agent_2': -1.8981349784419344, 'agent_3': -1.8981349784419344, 'agent_4': -1.8981349784419344} -2.865712154485238 {'__all__': False}\n",
      "12 {'agent_0': -2.744313284887001, 'agent_1': -2.744313284887001, 'agent_2': -2.744313284887001, 'agent_3': -2.744313284887001, 'agent_4': -2.744313284887001} -4.348629521680516 {'__all__': False}\n",
      "13 {'agent_0': -3.7775442486709387, 'agent_1': -3.7775442486709387, 'agent_2': -3.7775442486709387, 'agent_3': -3.7775442486709387, 'agent_4': -3.7775442486709387} -6.287801955973768 {'__all__': False}\n",
      "14 {'agent_0': -5.008365820033944, 'agent_1': -5.008365820033944, 'agent_2': -5.008365820033944, 'agent_3': -5.008365820033944, 'agent_4': -5.008365820033944} -8.730256652661753 {'__all__': False}\n",
      "15 {'agent_0': -6.437940208906534, 'agent_1': -6.437940208906534, 'agent_2': -6.437940208906534, 'agent_3': -6.437940208906534, 'agent_4': -6.437940208906534} -11.712897891741 {'__all__': False}\n",
      "16 {'agent_0': -8.10441202881651, 'agent_1': -8.10441202881651, 'agent_2': -8.10441202881651, 'agent_3': -8.10441202881651, 'agent_4': -8.10441202881651} -15.279865759365304 {'__all__': False}\n",
      "17 {'agent_0': -9.941630547547486, 'agent_1': -9.941630547547486, 'agent_2': -9.941630547547486, 'agent_3': -9.941630547547486, 'agent_4': -9.941630547547486} -19.43666365653107 {'__all__': False}\n",
      "18 {'agent_0': -11.958771886060315, 'agent_1': -11.958771886060315, 'agent_2': -11.958771886060315, 'agent_3': -11.958771886060315, 'agent_4': -11.958771886060315} -24.18685908084982 {'__all__': False}\n",
      "19 {'agent_0': -14.14826851551143, 'agent_1': -14.14826851551143, 'agent_2': -14.14826851551143, 'agent_3': -14.14826851551143, 'agent_4': -14.14826851551143} -29.525759174814922 {'__all__': False}\n",
      "20 {'agent_0': -16.552677114706206, 'agent_1': -16.552677114706206, 'agent_2': -16.552677114706206, 'agent_3': -16.552677114706206, 'agent_4': -16.552677114706206} -35.45966089861114 {'__all__': False}\n",
      "21 {'agent_0': -18.96946862093296, 'agent_1': -18.96946862093296, 'agent_2': -18.96946862093296, 'agent_3': -18.96946862093296, 'agent_4': -18.96946862093296} -41.91993398197743 {'__all__': False}\n",
      "22 {'agent_0': -21.807509677033153, 'agent_1': -21.807509677033153, 'agent_2': -21.807509677033153, 'agent_3': -21.807509677033153, 'agent_4': -21.807509677033153} -48.97539489483643 {'__all__': False}\n",
      "23 {'agent_0': -24.2698778561401, 'agent_1': -24.2698778561401, 'agent_2': -24.2698778561401, 'agent_3': -24.2698778561401, 'agent_4': -24.2698778561401} -56.43490853276856 {'__all__': False}\n",
      "24 {'agent_0': -26.944594454607618, 'agent_1': -26.944594454607618, 'agent_2': -26.944594454607618, 'agent_3': -26.944594454607618, 'agent_4': -26.944594454607618} -64.30243437877334 {'__all__': False}\n",
      "25 {'agent_0': -29.83617889510848, 'agent_1': -29.83617889510848, 'agent_2': -29.83617889510848, 'agent_3': -29.83617889510848, 'agent_4': -29.83617889510848} -72.57867930607415 {'__all__': False}\n",
      "26 {'agent_0': -32.88035212542173, 'agent_1': -32.88035212542173, 'agent_2': -32.88035212542173, 'agent_3': -32.88035212542173, 'agent_4': -32.88035212542173} -81.24331280423215 {'__all__': False}\n",
      "27 {'agent_0': -35.49525126732122, 'agent_1': -35.49525126732122, 'agent_2': -35.49525126732122, 'agent_3': -35.49525126732122, 'agent_4': -35.49525126732122} -90.1293391729294 {'__all__': False}\n",
      "28 {'agent_0': -38.706032097162165, 'agent_1': -38.706032097162165, 'agent_2': -38.706032097162165, 'agent_3': -38.706032097162165, 'agent_4': -38.706032097162165} -99.33467422719039 {'__all__': False}\n",
      "29 {'agent_0': -41.44437563022074, 'agent_1': -41.44437563022074, 'agent_2': -41.44437563022074, 'agent_3': -41.44437563022074, 'agent_4': -41.44437563022074} -108.69843165630341 {'__all__': True}\n",
      "-108.69843165630341\n"
     ]
    }
   ],
   "source": [
    "total_rew = 0\n",
    "for episodes in range(1,2):\n",
    "    cum_reward = 0\n",
    "    strucMA_heur.reset()\n",
    "    for t in range(30):\n",
    "#         if t%4 == 0:\n",
    "#             action_ = 0\n",
    "#         else:\n",
    "#             action_ = 0\n",
    "        observation, reward, done, info = strucMA_heur.step(act)\n",
    "        cum_reward += reward[\"agent_0\"]*0.95**t\n",
    "        print(t, reward, cum_reward, done)\n",
    "    total_rew += cum_reward\n",
    "    #print(episodes, total_rew)\n",
    "exp_reward = total_rew/episodes\n",
    "print(exp_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Evaluation of the environment with the trained policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-7-f7fc9a96062e>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      3\u001B[0m     \u001B[0mcum_reward\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;36m0\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      4\u001B[0m     \u001B[0mobs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mstruc_heur\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mreset\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 5\u001B[1;33m     \u001B[0maction_\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtrainer\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcompute_single_action\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mobs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      6\u001B[0m     \u001B[1;32mfor\u001B[0m \u001B[0mt\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m30\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      7\u001B[0m         \u001B[0mobs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mreward\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdone\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minfo\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mstruc_heur\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstep\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0maction_\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mNameError\u001B[0m: name 'trainer' is not defined"
     ]
    }
   ],
   "source": [
    "total_rew = 0\n",
    "for episodes in range(1,4):\n",
    "    cum_reward = 0\n",
    "    obs = struc_heur.reset()\n",
    "    action_ = trainer.compute_single_action(obs)\n",
    "    for t in range(30):\n",
    "        obs, reward, done, info = struc_heur.step(action_)\n",
    "        action_ = trainer.compute_single_action(obs)\n",
    "        cum_reward += reward*0.95**t\n",
    "        # print(t, reward, cum_reward, done)\n",
    "        print(t, action_)\n",
    "    total_rew += cum_reward\n",
    "    #print(episodes, total_rew)\n",
    "exp_reward = total_rew/episodes\n",
    "print(exp_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Configuration of the trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'node_ip_address': '127.0.0.1',\n",
       " 'raylet_ip_address': '127.0.0.1',\n",
       " 'redis_address': None,\n",
       " 'object_store_address': 'tcp://127.0.0.1:56200',\n",
       " 'raylet_socket_name': 'tcp://127.0.0.1:60375',\n",
       " 'webui_url': None,\n",
       " 'session_dir': 'C:\\\\Users\\\\user\\\\AppData\\\\Local\\\\Temp\\\\ray\\\\session_2022-07-06_10-39-16_480458_2200',\n",
       " 'metrics_export_port': 60243,\n",
       " 'gcs_address': '127.0.0.1:60291',\n",
       " 'address': '127.0.0.1:60291',\n",
       " 'node_id': 'e8732f9087481cf90d5cbf20fa6f5cbac25cd55ffd1d2379b56f3746'}"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pprint\n",
    "import ray\n",
    "from datetime import datetime\n",
    "import tempfile\n",
    "from ray.tune.logger import Logger, UnifiedLogger\n",
    "\n",
    "# Start a new instance of Ray (when running this tutorial locally) or\n",
    "# connect to an already running one (when running this tutorial through Anyscale).\n",
    "\n",
    "ray.init()  # Hear the engine humming? ;)\n",
    "\n",
    "# In case you encounter the following error during our tutorial: `RuntimeError: Maybe you called ray.init twice by accident?`\n",
    "# Try: `ray.shutdown() + ray.init()` or `ray.init(ignore_reinit_error=True)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "### Shutdown Ray's session\n",
    "ray.shutdown() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-06 10:39:22,426\tWARNING trainer.py:2348 -- You have specified 1 evaluation workers, but your `evaluation_interval` is None! Therefore, evaluation will not occur automatically with each call to `Trainer.train()`. Instead, you will have to call `Trainer.evaluate()` manually in order to trigger an evaluation run.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:/14_DecomposedQ_DRL/multiagent_environment/01_kOutOfN/log_files\\dqn_2022-07-06_10-39-22500qcail\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-06 10:39:26,715\tWARNING deprecation.py:46 -- DeprecationWarning: `simple_optimizer` has been deprecated. This will raise an error in the future!\n",
      "2022-07-06 10:39:26,716\tWARNING trainer.py:2348 -- You have specified 1 evaluation workers, but your `evaluation_interval` is None! Therefore, evaluation will not occur automatically with each call to `Trainer.train()`. Instead, you will have to call `Trainer.evaluate()` manually in order to trigger an evaluation run.\n",
      " pid=14536)\u001B[0m WARNING:tensorflow:From C:\\Users\\user\\Anaconda3\\envs\\gym\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      " pid=14536)\u001B[0m Instructions for updating:\n",
      " pid=14536)\u001B[0m If using Keras pass *_constraint arguments to layers.\n",
      "2022-07-06 10:39:26,804\tWARNING util.py:55 -- Install gputil for GPU system monitoring.\n",
      " pid=11752)\u001B[0m WARNING:tensorflow:From C:\\Users\\user\\Anaconda3\\envs\\gym\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      " pid=11752)\u001B[0m Instructions for updating:\n",
      " pid=11752)\u001B[0m If using Keras pass *_constraint arguments to layers.\n",
      " pid=972)\u001B[0m WARNING:tensorflow:From C:\\Users\\user\\Anaconda3\\envs\\gym\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      " pid=972)\u001B[0m Instructions for updating:\n",
      " pid=972)\u001B[0m If using Keras pass *_constraint arguments to layers.\n",
      " pid=32864)\u001B[0m WARNING:tensorflow:From C:\\Users\\user\\Anaconda3\\envs\\gym\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      " pid=32864)\u001B[0m Instructions for updating:\n",
      " pid=32864)\u001B[0m If using Keras pass *_constraint arguments to layers.\n"
     ]
    }
   ],
   "source": [
    "n_components = 3\n",
    "PATH_logger = \"D:/14_DecomposedQ_DRL/multiagent_environment/01_kOutOfN/log_files\"\n",
    "config_env = {\"config\": {\"components\": n_components} }\n",
    "\n",
    "env = StructMA(config_env)\n",
    "agent_list = []\n",
    "for i in range(env.ncomp):\n",
    "    item = \"agent_\"+ str(i)\n",
    "    agent_list.append(item)\n",
    "\n",
    "agent_list = []\n",
    "policy_list = []\n",
    "for i in range(env.ncomp):\n",
    "    item = \"agent_\"+ str(i)\n",
    "    item_ = \"policy\"+ str(i)\n",
    "    agent_list.append(item)\n",
    "    policy_list.append(item_)\n",
    "\n",
    "policies = {}\n",
    "mapping_agent2policy = {}\n",
    "# print(env.ncomp)\n",
    "for i in range(env.ncomp):\n",
    "    mapping_agent2policy[agent_list[i]] = policy_list[i]\n",
    "    policies[policy_list[i]] = (None, env.observation_space, env.action_space, {})\n",
    "\n",
    "# Define an agent->policy mapping function.\n",
    "# Which agents (defined by the environment) use which policies (defined by us)?\n",
    "# The mapping here is M (agents) -> N (policies), where M >= N.\n",
    "def policy_mapping_fn(agent_id: str):\n",
    "    return mapping_agent2policy[agent_id]\n",
    "\n",
    "from ray.rllib.agents.dqn import DQNTrainer\n",
    "# from ray.rllib.agents.ppo import PPOTrainer\n",
    "# Create an RLlib Trainer instance.\n",
    "\n",
    "config={\n",
    "        # Env class to use (here: our gym.Env sub-class from above).\n",
    "        \"env\": StructMA,\n",
    "        \n",
    "        \"env_config\": {\n",
    "            \"config\": {\"components\": n_components},\n",
    "        },\n",
    "        # Number of steps after which the episode is forced to terminate. Defaults\n",
    "        # to `env.spec.max_episode_steps` (if present) for Gym envs.\n",
    "        \"horizon\": 30,\n",
    "        # Parallelize environment rollouts.\n",
    "        \"num_workers\": 4,\n",
    "        # Discount factor of the MDP.\n",
    "        \"gamma\": 0.95,\n",
    "        \n",
    "        # https://github.com/ray-project/ray/blob/releases/1.11.1/rllib/models/catalog.py\n",
    "        # FullyConnectedNetwork (tf and torch): rllib.models.tf|torch.fcnet.py\n",
    "        # These are used if no custom model is specified and the input space is 1D.\n",
    "        # Number of hidden layers to be used.\n",
    "        # Activation function descriptor.\n",
    "        # Supported values are: \"tanh\", \"relu\", \"swish\" (or \"silu\"),\n",
    "        # \"linear\" (or None).\n",
    "        \"model\": {\n",
    "            \"fcnet_hiddens\": [100],\n",
    "            \"fcnet_activation\": \"relu\"\n",
    "        },\n",
    "        \n",
    "        \"create_env_on_driver\": True,\n",
    "        \n",
    "        #\"evaluation_interval\": 2,\n",
    "        \"evaluation_num_workers\": 1,\n",
    "        \"evaluation_duration\": 50,\n",
    "    \n",
    "        \"multiagent\": {\n",
    "            \"policies\": policies,\n",
    "            \"policy_mapping_fn\": policy_mapping_fn,\n",
    "            # We'll leave this empty: Means, we train both policy1 and policy2.\n",
    "            # \"policies_to_train\": policies_to_train,\n",
    "        },\n",
    "        # === Deep Learning Framework Settings ===\n",
    "        # tf: TensorFlow (static-graph)\n",
    "        # tf2: TensorFlow 2.x (eager or traced, if eager_tracing=True)\n",
    "        # tfe: TensorFlow eager (or traced, if eager_tracing=True)\n",
    "        # torch: PyTorch\n",
    "#         \"framework\": \"torch\",\n",
    "    }\n",
    "\n",
    "### Defaut logger creator ###\n",
    "def custom_log_creator(custom_path, custom_str):\n",
    "\n",
    "    timestr = datetime.today().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    logdir_prefix = \"{}_{}\".format(custom_str, timestr)\n",
    "    def logger_creator(config):\n",
    "\n",
    "        if not os.path.exists(custom_path):\n",
    "            os.makedirs(custom_path)\n",
    "        logdir = tempfile.mkdtemp(prefix=logdir_prefix, dir=custom_path)\n",
    "        print(logdir)\n",
    "        return UnifiedLogger(config, logdir, loggers=None)\n",
    "\n",
    "    return logger_creator\n",
    "\n",
    "trainer = DQNTrainer(config=config, logger_creator=custom_log_creator(PATH_logger, 'dqn') )\n",
    "# trainer = PPOTrainer(config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Train policy and conduct evaluations periodically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Training and evaluating specific policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[2m\u001B[36m(RolloutWorker\r",
      " pid=14536)\u001B[0m 2022-07-06 10:39:31,061\tWARNING deprecation.py:46 -- DeprecationWarning: `policy_mapping_fn(agent_id)` has been deprecated. Use `policy_mapping_fn(agent_id, episode, worker, **kwargs)` instead. This will raise an error in the future!\r\n",
      "\u001B[2m\u001B[36m(RolloutWorker\r",
      " pid=7840)\u001B[0m WARNING:tensorflow:From C:\\Users\\user\\Anaconda3\\envs\\gym\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\r\n",
      "\u001B[2m\u001B[36m(RolloutWorker\r",
      " pid=7840)\u001B[0m Instructions for updating:\r\n",
      "\u001B[2m\u001B[36m(RolloutWorker\r",
      " pid=7840)\u001B[0m If using Keras pass *_constraint arguments to layers.\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 0; avg. reward=-1850.5717400658423\n",
      "Iter (policy_1): 0; avg. reward=-616.857246688614\n",
      "Iter (policy_2): 0; avg. reward=-616.857246688614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[2m\u001B[36m(RolloutWorker\r",
      " pid=7840)\u001B[0m 2022-07-06 10:39:34,749\tWARNING deprecation.py:46 -- DeprecationWarning: `policy_mapping_fn(agent_id)` has been deprecated. Use `policy_mapping_fn(agent_id, episode, worker, **kwargs)` instead. This will raise an error in the future!\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-657.6189208802923\n",
      "-219.20630696009738\n",
      "-219.20630696009738\n",
      "Iter: 1; avg. reward=-1758.9365479876046\n",
      "Iter (policy_1): 1; avg. reward=-586.312182662535\n",
      "Iter (policy_2): 1; avg. reward=-586.312182662535\n",
      "Iter: 2; avg. reward=-1666.9854081368499\n",
      "Iter (policy_1): 2; avg. reward=-555.6618027122834\n",
      "Iter (policy_2): 2; avg. reward=-555.6618027122834\n",
      "Iter: 3; avg. reward=-1519.7199700226877\n",
      "Iter (policy_1): 3; avg. reward=-506.57332334089597\n",
      "Iter (policy_2): 3; avg. reward=-506.57332334089597\n",
      "Iter: 4; avg. reward=-1301.1130152443181\n",
      "Iter (policy_1): 4; avg. reward=-433.70433841477274\n",
      "Iter (policy_2): 4; avg. reward=-433.70433841477274\n",
      "Iter: 5; avg. reward=-1107.836623509402\n",
      "Iter (policy_1): 5; avg. reward=-369.278874503134\n",
      "Iter (policy_2): 5; avg. reward=-369.278874503134\n",
      "-379.9407269595361\n",
      "-126.64690898651205\n",
      "-126.64690898651205\n",
      "Iter: 6; avg. reward=-894.6071676977386\n",
      "Iter (policy_1): 6; avg. reward=-298.2023892325796\n",
      "Iter (policy_2): 6; avg. reward=-298.2023892325796\n",
      "Iter: 7; avg. reward=-706.0425798396401\n",
      "Iter (policy_1): 7; avg. reward=-235.34752661321338\n",
      "Iter (policy_2): 7; avg. reward=-235.34752661321338\n",
      "Iter: 8; avg. reward=-560.1017791804971\n",
      "Iter (policy_1): 8; avg. reward=-186.70059306016572\n",
      "Iter (policy_2): 8; avg. reward=-186.70059306016572\n",
      "Iter: 9; avg. reward=-406.43152634051205\n",
      "Iter (policy_1): 9; avg. reward=-135.47717544683735\n",
      "Iter (policy_2): 9; avg. reward=-135.47717544683735\n",
      "Iter: 10; avg. reward=-354.03334669894554\n",
      "Iter (policy_1): 10; avg. reward=-118.01111556631518\n",
      "Iter (policy_2): 10; avg. reward=-118.01111556631518\n",
      "-346.8548808584483\n",
      "-115.61829361948274\n",
      "-115.61829361948274\n",
      "Iter: 11; avg. reward=-330.3653274320606\n",
      "Iter (policy_1): 11; avg. reward=-110.12177581068686\n",
      "Iter (policy_2): 11; avg. reward=-110.12177581068686\n",
      "Iter: 12; avg. reward=-357.9798205877952\n",
      "Iter (policy_1): 12; avg. reward=-119.3266068625984\n",
      "Iter (policy_2): 12; avg. reward=-119.3266068625984\n",
      "Iter: 13; avg. reward=-344.0526835573614\n",
      "Iter (policy_1): 13; avg. reward=-114.68422785245379\n",
      "Iter (policy_2): 13; avg. reward=-114.68422785245379\n",
      "Iter: 14; avg. reward=-316.98833510712655\n",
      "Iter (policy_1): 14; avg. reward=-105.66277836904216\n",
      "Iter (policy_2): 14; avg. reward=-105.66277836904216\n",
      "Iter: 15; avg. reward=-309.5950056161414\n",
      "Iter (policy_1): 15; avg. reward=-103.19833520538045\n",
      "Iter (policy_2): 15; avg. reward=-103.19833520538045\n",
      "-264.9062680992713\n",
      "-88.30208936642379\n",
      "-88.30208936642379\n",
      "Iter: 16; avg. reward=-331.2445587432063\n",
      "Iter (policy_1): 16; avg. reward=-110.41485291440209\n",
      "Iter (policy_2): 16; avg. reward=-110.41485291440209\n",
      "Iter: 17; avg. reward=-335.7317170909712\n",
      "Iter (policy_1): 17; avg. reward=-111.91057236365707\n",
      "Iter (policy_2): 17; avg. reward=-111.91057236365707\n",
      "Iter: 18; avg. reward=-342.5504045019914\n",
      "Iter (policy_1): 18; avg. reward=-114.18346816733049\n",
      "Iter (policy_2): 18; avg. reward=-114.18346816733049\n",
      "Iter: 19; avg. reward=-321.57314938543874\n",
      "Iter (policy_1): 19; avg. reward=-107.19104979514627\n",
      "Iter (policy_2): 19; avg. reward=-107.19104979514627\n",
      "Iter: 20; avg. reward=-354.92057369784544\n",
      "Iter (policy_1): 20; avg. reward=-118.30685789928182\n",
      "Iter (policy_2): 20; avg. reward=-118.30685789928182\n",
      "-346.8548808584483\n",
      "-115.61829361948274\n",
      "-115.61829361948274\n",
      "Iter: 21; avg. reward=-347.3989075241593\n",
      "Iter (policy_1): 21; avg. reward=-115.7996358413864\n",
      "Iter (policy_2): 21; avg. reward=-115.7996358413864\n",
      "Iter: 22; avg. reward=-359.8549091982919\n",
      "Iter (policy_1): 22; avg. reward=-119.95163639943064\n",
      "Iter (policy_2): 22; avg. reward=-119.95163639943064\n",
      "Iter: 23; avg. reward=-328.04194750781346\n",
      "Iter (policy_1): 23; avg. reward=-109.34731583593786\n",
      "Iter (policy_2): 23; avg. reward=-109.34731583593786\n",
      "Iter: 24; avg. reward=-321.261297118004\n",
      "Iter (policy_1): 24; avg. reward=-107.08709903933466\n",
      "Iter (policy_2): 24; avg. reward=-107.08709903933466\n",
      "Iter: 25; avg. reward=-336.1224602143315\n",
      "Iter (policy_1): 25; avg. reward=-112.04082007144389\n",
      "Iter (policy_2): 25; avg. reward=-112.04082007144389\n",
      "-250.7746853022006\n",
      "-83.59156176740018\n",
      "-83.59156176740018\n",
      "Iter: 26; avg. reward=-361.14978425029955\n",
      "Iter (policy_1): 26; avg. reward=-120.38326141676652\n",
      "Iter (policy_2): 26; avg. reward=-120.38326141676652\n",
      "Iter: 27; avg. reward=-333.40759331311875\n",
      "Iter (policy_1): 27; avg. reward=-111.13586443770626\n",
      "Iter (policy_2): 27; avg. reward=-111.13586443770626\n",
      "Iter: 28; avg. reward=-285.94848264110266\n",
      "Iter (policy_1): 28; avg. reward=-95.31616088036753\n",
      "Iter (policy_2): 28; avg. reward=-95.31616088036753\n",
      "Iter: 29; avg. reward=-347.79605259521037\n",
      "Iter (policy_1): 29; avg. reward=-115.93201753173679\n",
      "Iter (policy_2): 29; avg. reward=-115.93201753173679\n",
      "Iter: 30; avg. reward=-379.2480905320928\n",
      "Iter (policy_1): 30; avg. reward=-126.41603017736423\n",
      "Iter (policy_2): 30; avg. reward=-126.41603017736423\n",
      "-571.67043625443\n",
      "-190.55681208481002\n",
      "-190.55681208481002\n",
      "Iter: 31; avg. reward=-405.32712161263424\n",
      "Iter (policy_1): 31; avg. reward=-135.10904053754473\n",
      "Iter (policy_2): 31; avg. reward=-135.10904053754473\n",
      "Iter: 32; avg. reward=-302.04851991657785\n",
      "Iter (policy_1): 32; avg. reward=-100.68283997219261\n",
      "Iter (policy_2): 32; avg. reward=-100.68283997219261\n",
      "Iter: 33; avg. reward=-282.1086002101385\n",
      "Iter (policy_1): 33; avg. reward=-94.03620007004616\n",
      "Iter (policy_2): 33; avg. reward=-94.03620007004616\n",
      "Iter: 34; avg. reward=-316.9112107833326\n",
      "Iter (policy_1): 34; avg. reward=-105.63707026111086\n",
      "Iter (policy_2): 34; avg. reward=-105.63707026111086\n",
      "Iter: 35; avg. reward=-349.6224079896511\n",
      "Iter (policy_1): 35; avg. reward=-116.540802663217\n",
      "Iter (policy_2): 35; avg. reward=-116.540802663217\n",
      "-399.91546116123254\n",
      "-133.30515372041083\n",
      "-133.30515372041083\n",
      "Iter: 36; avg. reward=-353.82059823162604\n",
      "Iter (policy_1): 36; avg. reward=-117.94019941054198\n",
      "Iter (policy_2): 36; avg. reward=-117.94019941054198\n",
      "Iter: 37; avg. reward=-323.20719045735063\n",
      "Iter (policy_1): 37; avg. reward=-107.7357301524502\n",
      "Iter (policy_2): 37; avg. reward=-107.7357301524502\n",
      "Iter: 38; avg. reward=-262.49764950430114\n",
      "Iter (policy_1): 38; avg. reward=-87.49921650143371\n",
      "Iter (policy_2): 38; avg. reward=-87.49921650143371\n",
      "Iter: 39; avg. reward=-238.94715433132893\n",
      "Iter (policy_1): 39; avg. reward=-79.6490514437763\n",
      "Iter (policy_2): 39; avg. reward=-79.6490514437763\n",
      "Iter: 40; avg. reward=-243.23173369851884\n",
      "Iter (policy_1): 40; avg. reward=-81.07724456617296\n",
      "Iter (policy_2): 40; avg. reward=-81.07724456617296\n",
      "-346.8548808584483\n",
      "-115.61829361948274\n",
      "-115.61829361948274\n",
      "Iter: 41; avg. reward=-290.77821833042356\n",
      "Iter (policy_1): 41; avg. reward=-96.92607277680783\n",
      "Iter (policy_2): 41; avg. reward=-96.92607277680783\n",
      "Iter: 42; avg. reward=-283.60684352515125\n",
      "Iter (policy_1): 42; avg. reward=-94.53561450838379\n",
      "Iter (policy_2): 42; avg. reward=-94.53561450838379\n",
      "Iter: 43; avg. reward=-257.2843706849569\n",
      "Iter (policy_1): 43; avg. reward=-85.76145689498564\n",
      "Iter (policy_2): 43; avg. reward=-85.76145689498564\n",
      "Iter: 44; avg. reward=-216.28138590816383\n",
      "Iter (policy_1): 44; avg. reward=-72.09379530272129\n",
      "Iter (policy_2): 44; avg. reward=-72.09379530272129\n",
      "Iter: 45; avg. reward=-222.43206495128052\n",
      "Iter (policy_1): 45; avg. reward=-74.14402165042684\n",
      "Iter (policy_2): 45; avg. reward=-74.14402165042684\n",
      "-154.9789759691507\n",
      "-51.65965865638356\n",
      "-51.65965865638356\n",
      "Iter: 46; avg. reward=-212.5834150732026\n",
      "Iter (policy_1): 46; avg. reward=-70.8611383577342\n",
      "Iter (policy_2): 46; avg. reward=-70.8611383577342\n",
      "Iter: 47; avg. reward=-190.0810797838597\n",
      "Iter (policy_1): 47; avg. reward=-63.36035992795324\n",
      "Iter (policy_2): 47; avg. reward=-63.36035992795324\n",
      "Iter: 48; avg. reward=-153.9221099968949\n",
      "Iter (policy_1): 48; avg. reward=-51.30736999896498\n",
      "Iter (policy_2): 48; avg. reward=-51.30736999896498\n",
      "Iter: 49; avg. reward=-146.8819689366795\n",
      "Iter (policy_1): 49; avg. reward=-48.960656312226504\n",
      "Iter (policy_2): 49; avg. reward=-48.960656312226504\n",
      "Iter: 50; avg. reward=-142.1855638272018\n",
      "Iter (policy_1): 50; avg. reward=-47.39518794240059\n",
      "Iter (policy_2): 50; avg. reward=-47.39518794240059\n",
      "-86.48101433197549\n",
      "-28.827004777325165\n",
      "-28.827004777325165\n",
      "Iter: 51; avg. reward=-130.80757484327015\n",
      "Iter (policy_1): 51; avg. reward=-43.60252494775672\n",
      "Iter (policy_2): 51; avg. reward=-43.60252494775672\n",
      "Iter: 52; avg. reward=-141.19283601598121\n",
      "Iter (policy_1): 52; avg. reward=-47.06427867199374\n",
      "Iter (policy_2): 52; avg. reward=-47.06427867199374\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 53; avg. reward=-150.49223309584778\n",
      "Iter (policy_1): 53; avg. reward=-50.16407769861591\n",
      "Iter (policy_2): 53; avg. reward=-50.16407769861591\n",
      "Iter: 54; avg. reward=-160.45241814363254\n",
      "Iter (policy_1): 54; avg. reward=-53.484139381210845\n",
      "Iter (policy_2): 54; avg. reward=-53.484139381210845\n",
      "Iter: 55; avg. reward=-173.8483921511862\n",
      "Iter (policy_1): 55; avg. reward=-57.94946405039539\n",
      "Iter (policy_2): 55; avg. reward=-57.94946405039539\n",
      "-155.7830156390995\n",
      "-51.92767187969984\n",
      "-51.92767187969984\n",
      "Iter: 56; avg. reward=-173.2117945779884\n",
      "Iter (policy_1): 56; avg. reward=-57.737264859329464\n",
      "Iter (policy_2): 56; avg. reward=-57.737264859329464\n",
      "Iter: 57; avg. reward=-177.8944160366257\n",
      "Iter (policy_1): 57; avg. reward=-59.29813867887522\n",
      "Iter (policy_2): 57; avg. reward=-59.29813867887522\n",
      "Iter: 58; avg. reward=-169.8781397675141\n",
      "Iter (policy_1): 58; avg. reward=-56.62604658917137\n",
      "Iter (policy_2): 58; avg. reward=-56.62604658917137\n",
      "Iter: 59; avg. reward=-170.24893401596557\n",
      "Iter (policy_1): 59; avg. reward=-56.74964467198852\n",
      "Iter (policy_2): 59; avg. reward=-56.74964467198852\n",
      "Iter: 60; avg. reward=-176.5542952043927\n",
      "Iter (policy_1): 60; avg. reward=-58.85143173479754\n",
      "Iter (policy_2): 60; avg. reward=-58.85143173479754\n",
      "-145.68193313391168\n",
      "-48.56064437797057\n",
      "-48.56064437797057\n",
      "Iter: 61; avg. reward=-186.27774539326273\n",
      "Iter (policy_1): 61; avg. reward=-62.09258179775425\n",
      "Iter (policy_2): 61; avg. reward=-62.09258179775425\n",
      "Iter: 62; avg. reward=-190.93808603844843\n",
      "Iter (policy_1): 62; avg. reward=-63.6460286794828\n",
      "Iter (policy_2): 62; avg. reward=-63.6460286794828\n",
      "Iter: 63; avg. reward=-181.34863766870177\n",
      "Iter (policy_1): 63; avg. reward=-60.44954588956725\n",
      "Iter (policy_2): 63; avg. reward=-60.44954588956725\n",
      "Iter: 64; avg. reward=-166.59901323746013\n",
      "Iter (policy_1): 64; avg. reward=-55.53300441248672\n",
      "Iter (policy_2): 64; avg. reward=-55.53300441248672\n",
      "Iter: 65; avg. reward=-168.3276447077681\n",
      "Iter (policy_1): 65; avg. reward=-56.109214902589365\n",
      "Iter (policy_2): 65; avg. reward=-56.109214902589365\n",
      "-147.27388192144784\n",
      "-49.09129397381594\n",
      "-49.09129397381594\n",
      "Iter: 66; avg. reward=-171.08987147693367\n",
      "Iter (policy_1): 66; avg. reward=-57.029957158977886\n",
      "Iter (policy_2): 66; avg. reward=-57.029957158977886\n",
      "Iter: 67; avg. reward=-173.94137825848492\n",
      "Iter (policy_1): 67; avg. reward=-57.98045941949495\n",
      "Iter (policy_2): 67; avg. reward=-57.98045941949495\n",
      "Iter: 68; avg. reward=-165.99742257453428\n",
      "Iter (policy_1): 68; avg. reward=-55.33247419151143\n",
      "Iter (policy_2): 68; avg. reward=-55.33247419151143\n",
      "Iter: 69; avg. reward=-159.62979883539126\n",
      "Iter (policy_1): 69; avg. reward=-53.20993294513042\n",
      "Iter (policy_2): 69; avg. reward=-53.20993294513042\n",
      "Iter: 70; avg. reward=-161.61034931747577\n",
      "Iter (policy_1): 70; avg. reward=-53.870116439158565\n",
      "Iter (policy_2): 70; avg. reward=-53.870116439158565\n",
      "-152.46675521403475\n",
      "-50.82225173801158\n",
      "-50.82225173801158\n",
      "Iter: 71; avg. reward=-156.23839694197744\n",
      "Iter (policy_1): 71; avg. reward=-52.079465647325826\n",
      "Iter (policy_2): 71; avg. reward=-52.079465647325826\n",
      "Iter: 72; avg. reward=-162.72511444921645\n",
      "Iter (policy_1): 72; avg. reward=-54.24170481640548\n",
      "Iter (policy_2): 72; avg. reward=-54.24170481640548\n",
      "Iter: 73; avg. reward=-153.52555402631359\n",
      "Iter (policy_1): 73; avg. reward=-51.17518467543785\n",
      "Iter (policy_2): 73; avg. reward=-51.17518467543785\n",
      "Iter: 74; avg. reward=-164.57367122748394\n",
      "Iter (policy_1): 74; avg. reward=-54.85789040916132\n",
      "Iter (policy_2): 74; avg. reward=-54.85789040916132\n",
      "Iter: 75; avg. reward=-169.49611420717287\n",
      "Iter (policy_1): 75; avg. reward=-56.49870473572429\n",
      "Iter (policy_2): 75; avg. reward=-56.49870473572429\n",
      "-125.75597835950542\n",
      "-41.91865945316847\n",
      "-41.91865945316847\n",
      "Iter: 76; avg. reward=-171.27457575176115\n",
      "Iter (policy_1): 76; avg. reward=-57.091525250587054\n",
      "Iter (policy_2): 76; avg. reward=-57.091525250587054\n",
      "Iter: 77; avg. reward=-164.42497062425227\n",
      "Iter (policy_1): 77; avg. reward=-54.80832354141743\n",
      "Iter (policy_2): 77; avg. reward=-54.80832354141743\n",
      "Iter: 78; avg. reward=-150.63563255737165\n",
      "Iter (policy_1): 78; avg. reward=-50.21187751912389\n",
      "Iter (policy_2): 78; avg. reward=-50.21187751912389\n",
      "Iter: 79; avg. reward=-139.60516254136542\n",
      "Iter (policy_1): 79; avg. reward=-46.53505418045513\n",
      "Iter (policy_2): 79; avg. reward=-46.53505418045513\n",
      "Iter: 80; avg. reward=-141.9057010745532\n",
      "Iter (policy_1): 80; avg. reward=-47.301900358184405\n",
      "Iter (policy_2): 80; avg. reward=-47.301900358184405\n",
      "-100.52985721937374\n",
      "-33.50995240645792\n",
      "-33.50995240645792\n",
      "Iter: 81; avg. reward=-146.04161201042615\n",
      "Iter (policy_1): 81; avg. reward=-48.68053733680872\n",
      "Iter (policy_2): 81; avg. reward=-48.68053733680872\n",
      "Iter: 82; avg. reward=-153.79281120362486\n",
      "Iter (policy_1): 82; avg. reward=-51.26427040120829\n",
      "Iter (policy_2): 82; avg. reward=-51.26427040120829\n",
      "Iter: 83; avg. reward=-152.41265016818932\n",
      "Iter (policy_1): 83; avg. reward=-50.80421672272978\n",
      "Iter (policy_2): 83; avg. reward=-50.80421672272978\n",
      "Iter: 84; avg. reward=-153.40282998319367\n",
      "Iter (policy_1): 84; avg. reward=-51.13427666106457\n",
      "Iter (policy_2): 84; avg. reward=-51.13427666106457\n",
      "Iter: 85; avg. reward=-158.86829560953038\n",
      "Iter (policy_1): 85; avg. reward=-52.956098536510126\n",
      "Iter (policy_2): 85; avg. reward=-52.956098536510126\n",
      "-111.95140389804814\n",
      "-37.31713463268271\n",
      "-37.31713463268271\n",
      "Iter: 86; avg. reward=-163.49960834071996\n",
      "Iter (policy_1): 86; avg. reward=-54.49986944690665\n",
      "Iter (policy_2): 86; avg. reward=-54.49986944690665\n",
      "Iter: 87; avg. reward=-158.25309012711963\n",
      "Iter (policy_1): 87; avg. reward=-52.7510300423732\n",
      "Iter (policy_2): 87; avg. reward=-52.7510300423732\n",
      "Iter: 88; avg. reward=-149.92924877823754\n",
      "Iter (policy_1): 88; avg. reward=-49.97641625941251\n",
      "Iter (policy_2): 88; avg. reward=-49.97641625941251\n",
      "Iter: 89; avg. reward=-145.71397589362175\n",
      "Iter (policy_1): 89; avg. reward=-48.57132529787391\n",
      "Iter (policy_2): 89; avg. reward=-48.57132529787391\n",
      "Iter: 90; avg. reward=-152.5866046837258\n",
      "Iter (policy_1): 90; avg. reward=-50.862201561241925\n",
      "Iter (policy_2): 90; avg. reward=-50.862201561241925\n",
      "-104.79726825474465\n",
      "-34.93242275158155\n",
      "-34.93242275158155\n",
      "Iter: 91; avg. reward=-164.9238585472684\n",
      "Iter (policy_1): 91; avg. reward=-54.97461951575611\n",
      "Iter (policy_2): 91; avg. reward=-54.97461951575611\n",
      "Iter: 92; avg. reward=-156.41641305429783\n",
      "Iter (policy_1): 92; avg. reward=-52.13880435143262\n",
      "Iter (policy_2): 92; avg. reward=-52.13880435143262\n",
      "Iter: 93; avg. reward=-153.010210013448\n",
      "Iter (policy_1): 93; avg. reward=-51.00340333781599\n",
      "Iter (policy_2): 93; avg. reward=-51.00340333781599\n",
      "Iter: 94; avg. reward=-150.90057872742395\n",
      "Iter (policy_1): 94; avg. reward=-50.30019290914132\n",
      "Iter (policy_2): 94; avg. reward=-50.30019290914132\n",
      "Iter: 95; avg. reward=-152.4537708833288\n",
      "Iter (policy_1): 95; avg. reward=-50.81792362777625\n",
      "Iter (policy_2): 95; avg. reward=-50.81792362777625\n",
      "-132.996841604396\n",
      "-44.33228053479866\n",
      "-44.33228053479866\n",
      "Iter: 96; avg. reward=-145.65497007163268\n",
      "Iter (policy_1): 96; avg. reward=-48.55165669054422\n",
      "Iter (policy_2): 96; avg. reward=-48.55165669054422\n",
      "Iter: 97; avg. reward=-138.11291006858738\n",
      "Iter (policy_1): 97; avg. reward=-46.03763668952913\n",
      "Iter (policy_2): 97; avg. reward=-46.03763668952913\n",
      "Iter: 98; avg. reward=-137.73426599832877\n",
      "Iter (policy_1): 98; avg. reward=-45.91142199944292\n",
      "Iter (policy_2): 98; avg. reward=-45.91142199944292\n",
      "Iter: 99; avg. reward=-149.27868141023845\n",
      "Iter (policy_1): 99; avg. reward=-49.75956047007948\n",
      "Iter (policy_2): 99; avg. reward=-49.75956047007948\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    results = trainer.train()\n",
    "    #if i%100==0:\n",
    "    #trainer.export_policy_model(\"D:/14_DecomposedQ_DRL/single_agent_environment/struc_SA_jupyter/savedModel\")\n",
    "    print(f\"Iter: {i}; avg. reward={results['episode_reward_mean']}\")\n",
    "    print(f\"Iter (policy_1): {i}; avg. reward={results['policy_reward_mean']['policy1']}\")\n",
    "    print(f\"Iter (policy_2): {i}; avg. reward={results['policy_reward_mean']['policy2']}\")\n",
    "    #print(f\"Iter: {i}; evaluation={results['evaluation']['episode_reward_mean']}\")\n",
    "    \n",
    "    if i%5==0:\n",
    "        evaluat = trainer.evaluate()\n",
    "        print(evaluat['evaluation']['episode_reward_mean'])\n",
    "        print(evaluat['evaluation']['policy_reward_mean']['policy1'])\n",
    "        print(evaluat['evaluation']['policy_reward_mean']['policy2'])\n",
    "#         print(f\"Iter: {i}; evaluation={results['evaluation']['episode_reward_mean']}\")\n",
    "        \n",
    "''' export policy checkpoint\n",
    "def export_policy_checkpoint(\n",
    "            self,\n",
    "            export_dir: str,\n",
    "            filename_prefix: str = \"model\",\n",
    "            policy_id: PolicyID = DEFAULT_POLICY_ID,\n",
    "    )   \n",
    "'''\n",
    "PATH_model = \"D:/14_DecomposedQ_DRL/single_agent_environment/struc_SA_jupyter/savedModel\"\n",
    "\n",
    "# trainer.export_policy_checkpoint(PATH_model, filename_prefix='modelx')\n",
    "\n",
    "# trainer.save_checkpoint(PATH_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'episode_reward_max': -513.226132756993,\n",
       " 'episode_reward_min': -1638.0075702959427,\n",
       " 'episode_reward_mean': -1086.9377084412256,\n",
       " 'episode_len_mean': 30.0,\n",
       " 'episode_media': {},\n",
       " 'episodes_this_iter': 34,\n",
       " 'policy_reward_min': {'policy0': -546.0025234319812,\n",
       "  'policy1': -546.0025234319812,\n",
       "  'policy2': -546.0025234319812},\n",
       " 'policy_reward_max': {'policy0': -171.07537758566437,\n",
       "  'policy1': -171.07537758566437,\n",
       "  'policy2': -171.07537758566437},\n",
       " 'policy_reward_mean': {'policy0': -362.3125694804086,\n",
       "  'policy1': -362.3125694804086,\n",
       "  'policy2': -362.3125694804086},\n",
       " 'custom_metrics': {},\n",
       " 'hist_stats': {'episode_reward': [-882.4683286363015,\n",
       "   -1566.0026840314174,\n",
       "   -1359.0015357448517,\n",
       "   -1635.0052196033155,\n",
       "   -1017.056338913421,\n",
       "   -1467.0024147505567,\n",
       "   -1638.0075702959427,\n",
       "   -1242.0067186706358,\n",
       "   -1617.0004678286161,\n",
       "   -1236.1355738702478,\n",
       "   -979.55606251021,\n",
       "   -1083.0196116272584,\n",
       "   -1548.006079969102,\n",
       "   -1263.0096808305107,\n",
       "   -1212.0094545178354,\n",
       "   -1146.1927264471146,\n",
       "   -1218.0769494232757,\n",
       "   -1290.0027538603429,\n",
       "   -813.0066617531685,\n",
       "   -1350.1603588769185,\n",
       "   -1521.358671707671,\n",
       "   -1347.0017789220137,\n",
       "   -1353.0057997640165,\n",
       "   -1119.0014819861128,\n",
       "   -1173.0013257943174,\n",
       "   -1302.0019114919346,\n",
       "   -1005.0133232042263,\n",
       "   -1053.0291503265155,\n",
       "   -1290.0037361007878,\n",
       "   -1347.0261956964084,\n",
       "   -1224.0029010775002,\n",
       "   -1161.060195355473,\n",
       "   -939.4049859811739,\n",
       "   -1197.8297962792522,\n",
       "   -762.1423601539285,\n",
       "   -1014.0187240691818,\n",
       "   -1059.3748624648188,\n",
       "   -1257.0024921619868,\n",
       "   -1611.014966775356,\n",
       "   -1129.3134248581598,\n",
       "   -1089.0197406293412,\n",
       "   -1380.0021990237528,\n",
       "   -987.3605266546194,\n",
       "   -1395.002428634553,\n",
       "   -1227.0441084032068,\n",
       "   -1020.0382784800491,\n",
       "   -1206.0124705401145,\n",
       "   -954.0206796620395,\n",
       "   -957.0444519891528,\n",
       "   -1476.0118073544065,\n",
       "   -1155.3578611339296,\n",
       "   -975.0080283879781,\n",
       "   -1263.0119558483225,\n",
       "   -1425.0017068025172,\n",
       "   -1386.002534150296,\n",
       "   -720.1432550428286,\n",
       "   -1273.3365025067233,\n",
       "   -1095.9038501851232,\n",
       "   -726.086539963432,\n",
       "   -1104.0047859260685,\n",
       "   -1266.0209482066352,\n",
       "   -966.1534992746973,\n",
       "   -720.1516970525497,\n",
       "   -1314.0365230067769,\n",
       "   -963.1503395064368,\n",
       "   -1053.0065248264627,\n",
       "   -1155.1187320189863,\n",
       "   -900.0334623884771,\n",
       "   -1119.242166839634,\n",
       "   -1083.0362449247082,\n",
       "   -909.0306696837125,\n",
       "   -804.3886832065704,\n",
       "   -927.0114706514715,\n",
       "   -1167.0039026496463,\n",
       "   -870.7422184315717,\n",
       "   -1029.1239090346357,\n",
       "   -685.4372882920231,\n",
       "   -789.05616379581,\n",
       "   -969.0660388491659,\n",
       "   -652.6222051997763,\n",
       "   -780.395908710862,\n",
       "   -726.1382471465439,\n",
       "   -885.9311669637291,\n",
       "   -711.1192371220692,\n",
       "   -871.0514399276003,\n",
       "   -757.2068963233667,\n",
       "   -907.0384432311746,\n",
       "   -696.5310383757961,\n",
       "   -1092.0542999905679,\n",
       "   -1146.0569088648094,\n",
       "   -740.833017449653,\n",
       "   -972.0180313600453,\n",
       "   -780.1552545798461,\n",
       "   -975.1848053965725,\n",
       "   -763.7332662765094,\n",
       "   -1098.9554071385694,\n",
       "   -822.0232186353945,\n",
       "   -966.8642411076315,\n",
       "   -513.226132756993,\n",
       "   -900.0722072747664],\n",
       "  'episode_lengths': [30,\n",
       "   30,\n",
       "   30,\n",
       "   30,\n",
       "   30,\n",
       "   30,\n",
       "   30,\n",
       "   30,\n",
       "   30,\n",
       "   30,\n",
       "   30,\n",
       "   30,\n",
       "   30,\n",
       "   30,\n",
       "   30,\n",
       "   30,\n",
       "   30,\n",
       "   30,\n",
       "   30,\n",
       "   30,\n",
       "   30,\n",
       "   30,\n",
       "   30,\n",
       "   30,\n",
       "   30,\n",
       "   30,\n",
       "   30,\n",
       "   30,\n",
       "   30,\n",
       "   30,\n",
       "   30,\n",
       "   30,\n",
       "   30,\n",
       "   30,\n",
       "   30,\n",
       "   30,\n",
       "   30,\n",
       "   30,\n",
       "   30,\n",
       "   30,\n",
       "   30,\n",
       "   30,\n",
       "   30,\n",
       "   30,\n",
       "   30,\n",
       "   30,\n",
       "   30,\n",
       "   30,\n",
       "   30,\n",
       "   30,\n",
       "   30,\n",
       "   30,\n",
       "   30,\n",
       "   30,\n",
       "   30,\n",
       "   30,\n",
       "   30,\n",
       "   30,\n",
       "   30,\n",
       "   30,\n",
       "   30,\n",
       "   30,\n",
       "   30,\n",
       "   30,\n",
       "   30,\n",
       "   30,\n",
       "   30,\n",
       "   30,\n",
       "   30,\n",
       "   30,\n",
       "   30,\n",
       "   30,\n",
       "   30,\n",
       "   30,\n",
       "   30,\n",
       "   30,\n",
       "   30,\n",
       "   30,\n",
       "   30,\n",
       "   30,\n",
       "   30,\n",
       "   30,\n",
       "   30,\n",
       "   30,\n",
       "   30,\n",
       "   30,\n",
       "   30,\n",
       "   30,\n",
       "   30,\n",
       "   30,\n",
       "   30,\n",
       "   30,\n",
       "   30,\n",
       "   30,\n",
       "   30,\n",
       "   30,\n",
       "   30,\n",
       "   30,\n",
       "   30,\n",
       "   30],\n",
       "  'policy_policy0_reward': [-294.1561095454338,\n",
       "   -522.0008946771391,\n",
       "   -453.0005119149505,\n",
       "   -545.0017398677721,\n",
       "   -339.01877963780703,\n",
       "   -489.00080491685213,\n",
       "   -546.0025234319812,\n",
       "   -414.00223955687886,\n",
       "   -539.000155942872,\n",
       "   -412.04519129008224,\n",
       "   -326.51868750340367,\n",
       "   -361.00653720908633,\n",
       "   -516.0020266563672,\n",
       "   -421.0032269435035,\n",
       "   -404.0031515059451,\n",
       "   -382.06424214903825,\n",
       "   -406.0256498077589,\n",
       "   -430.0009179534477,\n",
       "   -271.0022205843897,\n",
       "   -450.05345295897234,\n",
       "   -507.1195572358902,\n",
       "   -449.0005929740043,\n",
       "   -451.00193325467194,\n",
       "   -373.0004939953713,\n",
       "   -391.00044193143924,\n",
       "   -434.0006371639783,\n",
       "   -335.00444106807555,\n",
       "   -351.0097167755053,\n",
       "   -430.00124536692965,\n",
       "   -449.008731898803,\n",
       "   -408.0009670258339,\n",
       "   -387.0200651184912,\n",
       "   -313.134995327058,\n",
       "   -399.27659875975087,\n",
       "   -254.0474533846428,\n",
       "   -338.00624135639384,\n",
       "   -353.12495415493976,\n",
       "   -419.0008307206623,\n",
       "   -537.0049889251186,\n",
       "   -376.4378082860534,\n",
       "   -363.00658020978034,\n",
       "   -460.00073300791774,\n",
       "   -329.12017555154017,\n",
       "   -465.0008095448512,\n",
       "   -409.0147028010688,\n",
       "   -340.0127594933496,\n",
       "   -402.0041568467046,\n",
       "   -318.00689322067973,\n",
       "   -319.01481732971763,\n",
       "   -492.00393578480254,\n",
       "   -385.11928704464316,\n",
       "   -325.002676129326,\n",
       "   -421.00398528277384,\n",
       "   -475.00056893417195,\n",
       "   -462.0008447167654,\n",
       "   -240.04775168094275,\n",
       "   -424.44550083557414,\n",
       "   -365.3012833950408,\n",
       "   -242.02884665447732,\n",
       "   -368.0015953086897,\n",
       "   -422.0069827355453,\n",
       "   -322.0511664248993,\n",
       "   -240.05056568418314,\n",
       "   -438.0121743355918,\n",
       "   -321.0501131688124,\n",
       "   -351.00217494215417,\n",
       "   -385.03957733966223,\n",
       "   -300.0111541294923,\n",
       "   -373.0807222798783,\n",
       "   -361.01208164156935,\n",
       "   -303.0102232279043,\n",
       "   -268.1295610688567,\n",
       "   -309.0038235504906,\n",
       "   -389.0013008832156,\n",
       "   -290.24740614385735,\n",
       "   -343.0413030115453,\n",
       "   -228.47909609734108,\n",
       "   -263.01872126527024,\n",
       "   -323.02201294972195,\n",
       "   -217.54073506659205,\n",
       "   -260.1319695702873,\n",
       "   -242.04608238218128,\n",
       "   -295.3103889879097,\n",
       "   -237.03974570735645,\n",
       "   -290.3504799758669,\n",
       "   -252.40229877445566,\n",
       "   -302.346147743725,\n",
       "   -232.17701279193204,\n",
       "   -364.01809999685594,\n",
       "   -382.01896962160356,\n",
       "   -246.94433914988412,\n",
       "   -324.0060104533483,\n",
       "   -260.05175152661513,\n",
       "   -325.0616017988575,\n",
       "   -254.5777554255031,\n",
       "   -366.31846904619,\n",
       "   -274.00773954513147,\n",
       "   -322.28808036921066,\n",
       "   -171.07537758566437,\n",
       "   -300.02406909158873],\n",
       "  'policy_policy1_reward': [-294.1561095454338,\n",
       "   -522.0008946771391,\n",
       "   -453.0005119149505,\n",
       "   -545.0017398677721,\n",
       "   -339.01877963780703,\n",
       "   -489.00080491685213,\n",
       "   -546.0025234319812,\n",
       "   -414.00223955687886,\n",
       "   -539.000155942872,\n",
       "   -412.04519129008224,\n",
       "   -326.51868750340367,\n",
       "   -361.00653720908633,\n",
       "   -516.0020266563672,\n",
       "   -421.0032269435035,\n",
       "   -404.0031515059451,\n",
       "   -382.06424214903825,\n",
       "   -406.0256498077589,\n",
       "   -430.0009179534477,\n",
       "   -271.0022205843897,\n",
       "   -450.05345295897234,\n",
       "   -507.1195572358902,\n",
       "   -449.0005929740043,\n",
       "   -451.00193325467194,\n",
       "   -373.0004939953713,\n",
       "   -391.00044193143924,\n",
       "   -434.0006371639783,\n",
       "   -335.00444106807555,\n",
       "   -351.0097167755053,\n",
       "   -430.00124536692965,\n",
       "   -449.008731898803,\n",
       "   -408.0009670258339,\n",
       "   -387.0200651184912,\n",
       "   -313.134995327058,\n",
       "   -399.27659875975087,\n",
       "   -254.0474533846428,\n",
       "   -338.00624135639384,\n",
       "   -353.12495415493976,\n",
       "   -419.0008307206623,\n",
       "   -537.0049889251186,\n",
       "   -376.4378082860534,\n",
       "   -363.00658020978034,\n",
       "   -460.00073300791774,\n",
       "   -329.12017555154017,\n",
       "   -465.0008095448512,\n",
       "   -409.0147028010688,\n",
       "   -340.0127594933496,\n",
       "   -402.0041568467046,\n",
       "   -318.00689322067973,\n",
       "   -319.01481732971763,\n",
       "   -492.00393578480254,\n",
       "   -385.11928704464316,\n",
       "   -325.002676129326,\n",
       "   -421.00398528277384,\n",
       "   -475.00056893417195,\n",
       "   -462.0008447167654,\n",
       "   -240.04775168094275,\n",
       "   -424.44550083557414,\n",
       "   -365.3012833950408,\n",
       "   -242.02884665447732,\n",
       "   -368.0015953086897,\n",
       "   -422.0069827355453,\n",
       "   -322.0511664248993,\n",
       "   -240.05056568418314,\n",
       "   -438.0121743355918,\n",
       "   -321.0501131688124,\n",
       "   -351.00217494215417,\n",
       "   -385.03957733966223,\n",
       "   -300.0111541294923,\n",
       "   -373.0807222798783,\n",
       "   -361.01208164156935,\n",
       "   -303.0102232279043,\n",
       "   -268.1295610688567,\n",
       "   -309.0038235504906,\n",
       "   -389.0013008832156,\n",
       "   -290.24740614385735,\n",
       "   -343.0413030115453,\n",
       "   -228.47909609734108,\n",
       "   -263.01872126527024,\n",
       "   -323.02201294972195,\n",
       "   -217.54073506659205,\n",
       "   -260.1319695702873,\n",
       "   -242.04608238218128,\n",
       "   -295.3103889879097,\n",
       "   -237.03974570735645,\n",
       "   -290.3504799758669,\n",
       "   -252.40229877445566,\n",
       "   -302.346147743725,\n",
       "   -232.17701279193204,\n",
       "   -364.01809999685594,\n",
       "   -382.01896962160356,\n",
       "   -246.94433914988412,\n",
       "   -324.0060104533483,\n",
       "   -260.05175152661513,\n",
       "   -325.0616017988575,\n",
       "   -254.5777554255031,\n",
       "   -366.31846904619,\n",
       "   -274.00773954513147,\n",
       "   -322.28808036921066,\n",
       "   -171.07537758566437,\n",
       "   -300.02406909158873],\n",
       "  'policy_policy2_reward': [-294.1561095454338,\n",
       "   -522.0008946771391,\n",
       "   -453.0005119149505,\n",
       "   -545.0017398677721,\n",
       "   -339.01877963780703,\n",
       "   -489.00080491685213,\n",
       "   -546.0025234319812,\n",
       "   -414.00223955687886,\n",
       "   -539.000155942872,\n",
       "   -412.04519129008224,\n",
       "   -326.51868750340367,\n",
       "   -361.00653720908633,\n",
       "   -516.0020266563672,\n",
       "   -421.0032269435035,\n",
       "   -404.0031515059451,\n",
       "   -382.06424214903825,\n",
       "   -406.0256498077589,\n",
       "   -430.0009179534477,\n",
       "   -271.0022205843897,\n",
       "   -450.05345295897234,\n",
       "   -507.1195572358902,\n",
       "   -449.0005929740043,\n",
       "   -451.00193325467194,\n",
       "   -373.0004939953713,\n",
       "   -391.00044193143924,\n",
       "   -434.0006371639783,\n",
       "   -335.00444106807555,\n",
       "   -351.0097167755053,\n",
       "   -430.00124536692965,\n",
       "   -449.008731898803,\n",
       "   -408.0009670258339,\n",
       "   -387.0200651184912,\n",
       "   -313.134995327058,\n",
       "   -399.27659875975087,\n",
       "   -254.0474533846428,\n",
       "   -338.00624135639384,\n",
       "   -353.12495415493976,\n",
       "   -419.0008307206623,\n",
       "   -537.0049889251186,\n",
       "   -376.4378082860534,\n",
       "   -363.00658020978034,\n",
       "   -460.00073300791774,\n",
       "   -329.12017555154017,\n",
       "   -465.0008095448512,\n",
       "   -409.0147028010688,\n",
       "   -340.0127594933496,\n",
       "   -402.0041568467046,\n",
       "   -318.00689322067973,\n",
       "   -319.01481732971763,\n",
       "   -492.00393578480254,\n",
       "   -385.11928704464316,\n",
       "   -325.002676129326,\n",
       "   -421.00398528277384,\n",
       "   -475.00056893417195,\n",
       "   -462.0008447167654,\n",
       "   -240.04775168094275,\n",
       "   -424.44550083557414,\n",
       "   -365.3012833950408,\n",
       "   -242.02884665447732,\n",
       "   -368.0015953086897,\n",
       "   -422.0069827355453,\n",
       "   -322.0511664248993,\n",
       "   -240.05056568418314,\n",
       "   -438.0121743355918,\n",
       "   -321.0501131688124,\n",
       "   -351.00217494215417,\n",
       "   -385.03957733966223,\n",
       "   -300.0111541294923,\n",
       "   -373.0807222798783,\n",
       "   -361.01208164156935,\n",
       "   -303.0102232279043,\n",
       "   -268.1295610688567,\n",
       "   -309.0038235504906,\n",
       "   -389.0013008832156,\n",
       "   -290.24740614385735,\n",
       "   -343.0413030115453,\n",
       "   -228.47909609734108,\n",
       "   -263.01872126527024,\n",
       "   -323.02201294972195,\n",
       "   -217.54073506659205,\n",
       "   -260.1319695702873,\n",
       "   -242.04608238218128,\n",
       "   -295.3103889879097,\n",
       "   -237.03974570735645,\n",
       "   -290.3504799758669,\n",
       "   -252.40229877445566,\n",
       "   -302.346147743725,\n",
       "   -232.17701279193204,\n",
       "   -364.01809999685594,\n",
       "   -382.01896962160356,\n",
       "   -246.94433914988412,\n",
       "   -324.0060104533483,\n",
       "   -260.05175152661513,\n",
       "   -325.0616017988575,\n",
       "   -254.5777554255031,\n",
       "   -366.31846904619,\n",
       "   -274.00773954513147,\n",
       "   -322.28808036921066,\n",
       "   -171.07537758566437,\n",
       "   -300.02406909158873]},\n",
       " 'sampler_perf': {'mean_raw_obs_processing_ms': 0.3069099577265052,\n",
       "  'mean_inference_ms': 1.846793265045004,\n",
       "  'mean_action_processing_ms': 0.12532527962261242,\n",
       "  'mean_env_wait_ms': 0.2046682833018141,\n",
       "  'mean_env_render_ms': 0.0},\n",
       " 'off_policy_estimator': {},\n",
       " 'num_healthy_workers': 1,\n",
       " 'timesteps_total': 6000,\n",
       " 'timesteps_this_iter': 32,\n",
       " 'agent_timesteps_total': 18000,\n",
       " 'timers': {'load_time_ms': 0.4,\n",
       "  'load_throughput': 80058.293,\n",
       "  'learn_time_ms': 9.351,\n",
       "  'learn_throughput': 3422.245,\n",
       "  'update_time_ms': 4.8},\n",
       " 'info': {'learner': {'policy0': {'learner_stats': {'cur_lr': 0.0005000000237487257,\n",
       "     'mean_q': -67.51409,\n",
       "     'min_q': -104.991135,\n",
       "     'max_q': -39.046135,\n",
       "     'mean_td_error': 3.708776,\n",
       "     'model': {}},\n",
       "    'td_error': array([ 34.35708  ,  -3.5460052,  -5.7429047, -38.046135 ,  13.05481  ,\n",
       "            -5.785717 ,  -6.0919914,  -5.8850174,  15.114601 ,  -6.438671 ,\n",
       "            16.421188 ,  -5.15691  ,  15.275658 ,  14.350136 ,  13.296322 ,\n",
       "            -5.8850174,  -7.022644 ,  12.4183655,  -8.239044 ,  -5.1568527,\n",
       "            13.4183655,  -4.2707596,  -3.6700134,  -6.441883 ,  -6.015579 ,\n",
       "            14.940269 ,  14.188999 ,  33.35936  ,  12.10894  ,  33.057636 ,\n",
       "            -7.152527 ,  -6.1332245], dtype=float32),\n",
       "    'custom_metrics': {}},\n",
       "   'policy1': {'learner_stats': {'cur_lr': 0.0005000000237487257,\n",
       "     'mean_q': -89.56109,\n",
       "     'min_q': -116.11176,\n",
       "     'max_q': -58.119823,\n",
       "     'mean_td_error': 5.587375,\n",
       "     'model': {}},\n",
       "    'td_error': array([ 18.690567  ,  29.449844  ,  16.748543  ,  -7.348564  ,\n",
       "           -37.119823  ,  12.557129  ,  15.395645  ,  -1.9804764 ,\n",
       "            33.04738   ,  -4.668022  ,  -0.41278458,  34.100243  ,\n",
       "            17.440742  ,  16.681808  ,  15.605896  ,  -5.338196  ,\n",
       "            29.943161  ,  -4.932083  ,  -3.8808365 ,  17.798294  ,\n",
       "           -58.254486  ,  14.2028885 ,  -9.775848  ,  -9.940468  ,\n",
       "            30.890854  ,  -7.3946915 ,  13.66964   ,  -3.467186  ,\n",
       "            -3.7957687 ,  -8.775848  ,  14.2476425 ,  15.410812  ],\n",
       "          dtype=float32),\n",
       "    'custom_metrics': {}},\n",
       "   'policy2': {'learner_stats': {'cur_lr': 0.0005000000237487257,\n",
       "     'mean_q': -87.88694,\n",
       "     'min_q': -114.372116,\n",
       "     'max_q': -66.49666,\n",
       "     'mean_td_error': 10.5479355,\n",
       "     'model': {}},\n",
       "    'td_error': array([-2.8133469 ,  2.2313232 , 18.530563  , -5.094612  , -0.5174484 ,\n",
       "           17.825882  , 20.435623  , 18.259949  , 36.82467   , -1.5633545 ,\n",
       "           15.974998  , -1.4500732 , 17.068886  , 20.977562  , 35.934853  ,\n",
       "           17.666351  , -0.6732025 , -3.0149002 , -1.9149857 , 17.03109   ,\n",
       "           16.078896  , -1.6373291 , 19.678947  , 15.312027  , -3.2398682 ,\n",
       "           18.413498  , 18.711227  , -0.40244293, -3.1982498 , 22.479034  ,\n",
       "           16.55362   , -2.9352646 ], dtype=float32),\n",
       "    'custom_metrics': {}}},\n",
       "  'num_steps_sampled': 6000,\n",
       "  'num_agent_steps_sampled': 18000,\n",
       "  'num_steps_trained': 40032,\n",
       "  'num_steps_trained_this_iter': 32,\n",
       "  'num_agent_steps_trained': 120096,\n",
       "  'last_target_update_ts': 5536,\n",
       "  'num_target_updates': 10},\n",
       " 'done': False,\n",
       " 'episodes_total': 200,\n",
       " 'training_iteration': 6,\n",
       " 'trial_id': 'default',\n",
       " 'experiment_id': 'caad0196e9964c7c8f47dd3c0810425b',\n",
       " 'date': '2022-07-04_17-34-26',\n",
       " 'timestamp': 1656948866,\n",
       " 'time_this_iter_s': 10.716490030288696,\n",
       " 'time_total_s': 57.39332842826843,\n",
       " 'pid': 31032,\n",
       " 'hostname': 'DESKTOP-U95K42C',\n",
       " 'node_ip': '127.0.0.1',\n",
       " 'config': {'num_workers': 1,\n",
       "  'num_envs_per_worker': 1,\n",
       "  'create_env_on_driver': True,\n",
       "  'rollout_fragment_length': 4,\n",
       "  'batch_mode': 'truncate_episodes',\n",
       "  'gamma': 0.95,\n",
       "  'lr': 0.0005,\n",
       "  'train_batch_size': 32,\n",
       "  'model': {'_use_default_native_models': False,\n",
       "   '_disable_preprocessor_api': False,\n",
       "   '_disable_action_flattening': False,\n",
       "   'fcnet_hiddens': [200],\n",
       "   'fcnet_activation': 'relu',\n",
       "   'conv_filters': None,\n",
       "   'conv_activation': 'relu',\n",
       "   'post_fcnet_hiddens': [],\n",
       "   'post_fcnet_activation': 'relu',\n",
       "   'free_log_std': False,\n",
       "   'no_final_linear': False,\n",
       "   'vf_share_layers': True,\n",
       "   'use_lstm': False,\n",
       "   'max_seq_len': 20,\n",
       "   'lstm_cell_size': 256,\n",
       "   'lstm_use_prev_action': False,\n",
       "   'lstm_use_prev_reward': False,\n",
       "   '_time_major': False,\n",
       "   'use_attention': False,\n",
       "   'attention_num_transformer_units': 1,\n",
       "   'attention_dim': 64,\n",
       "   'attention_num_heads': 1,\n",
       "   'attention_head_dim': 32,\n",
       "   'attention_memory_inference': 50,\n",
       "   'attention_memory_training': 50,\n",
       "   'attention_position_wise_mlp_dim': 32,\n",
       "   'attention_init_gru_gate_bias': 2.0,\n",
       "   'attention_use_n_prev_actions': 0,\n",
       "   'attention_use_n_prev_rewards': 0,\n",
       "   'framestack': True,\n",
       "   'dim': 84,\n",
       "   'grayscale': False,\n",
       "   'zero_mean': True,\n",
       "   'custom_model': None,\n",
       "   'custom_model_config': {},\n",
       "   'custom_action_dist': None,\n",
       "   'custom_preprocessor': None,\n",
       "   'lstm_use_prev_action_reward': -1},\n",
       "  'optimizer': {},\n",
       "  'horizon': 30,\n",
       "  'soft_horizon': False,\n",
       "  'no_done_at_end': False,\n",
       "  'env': 'StructMA',\n",
       "  'observation_space': None,\n",
       "  'action_space': None,\n",
       "  'env_config': {'config': {'components': 3}},\n",
       "  'remote_worker_envs': False,\n",
       "  'remote_env_batch_wait_ms': 0,\n",
       "  'env_task_fn': None,\n",
       "  'render_env': False,\n",
       "  'record_env': False,\n",
       "  'clip_rewards': None,\n",
       "  'normalize_actions': True,\n",
       "  'clip_actions': False,\n",
       "  'preprocessor_pref': 'deepmind',\n",
       "  'log_level': 'WARN',\n",
       "  'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
       "  'ignore_worker_failures': False,\n",
       "  'log_sys_usage': True,\n",
       "  'fake_sampler': False,\n",
       "  'framework': 'tf',\n",
       "  'eager_tracing': False,\n",
       "  'eager_max_retraces': 20,\n",
       "  'explore': True,\n",
       "  'exploration_config': {'type': 'EpsilonGreedy',\n",
       "   'initial_epsilon': 1.0,\n",
       "   'final_epsilon': 0.02,\n",
       "   'epsilon_timesteps': 10000},\n",
       "  'evaluation_interval': None,\n",
       "  'evaluation_duration': 50,\n",
       "  'evaluation_duration_unit': 'episodes',\n",
       "  'evaluation_parallel_to_training': False,\n",
       "  'in_evaluation': False,\n",
       "  'evaluation_config': {'num_workers': 1,\n",
       "   'num_envs_per_worker': 1,\n",
       "   'create_env_on_driver': True,\n",
       "   'rollout_fragment_length': 1,\n",
       "   'batch_mode': 'complete_episodes',\n",
       "   'gamma': 0.95,\n",
       "   'lr': 0.0005,\n",
       "   'train_batch_size': 32,\n",
       "   'model': {'_use_default_native_models': False,\n",
       "    '_disable_preprocessor_api': False,\n",
       "    '_disable_action_flattening': False,\n",
       "    'fcnet_hiddens': [200],\n",
       "    'fcnet_activation': 'relu',\n",
       "    'conv_filters': None,\n",
       "    'conv_activation': 'relu',\n",
       "    'post_fcnet_hiddens': [],\n",
       "    'post_fcnet_activation': 'relu',\n",
       "    'free_log_std': False,\n",
       "    'no_final_linear': False,\n",
       "    'vf_share_layers': True,\n",
       "    'use_lstm': False,\n",
       "    'max_seq_len': 20,\n",
       "    'lstm_cell_size': 256,\n",
       "    'lstm_use_prev_action': False,\n",
       "    'lstm_use_prev_reward': False,\n",
       "    '_time_major': False,\n",
       "    'use_attention': False,\n",
       "    'attention_num_transformer_units': 1,\n",
       "    'attention_dim': 64,\n",
       "    'attention_num_heads': 1,\n",
       "    'attention_head_dim': 32,\n",
       "    'attention_memory_inference': 50,\n",
       "    'attention_memory_training': 50,\n",
       "    'attention_position_wise_mlp_dim': 32,\n",
       "    'attention_init_gru_gate_bias': 2.0,\n",
       "    'attention_use_n_prev_actions': 0,\n",
       "    'attention_use_n_prev_rewards': 0,\n",
       "    'framestack': True,\n",
       "    'dim': 84,\n",
       "    'grayscale': False,\n",
       "    'zero_mean': True,\n",
       "    'custom_model': None,\n",
       "    'custom_model_config': {},\n",
       "    'custom_action_dist': None,\n",
       "    'custom_preprocessor': None,\n",
       "    'lstm_use_prev_action_reward': -1},\n",
       "   'optimizer': {},\n",
       "   'horizon': 30,\n",
       "   'soft_horizon': False,\n",
       "   'no_done_at_end': False,\n",
       "   'env': 'StructMA',\n",
       "   'observation_space': None,\n",
       "   'action_space': None,\n",
       "   'env_config': {'config': {'components': 3}},\n",
       "   'remote_worker_envs': False,\n",
       "   'remote_env_batch_wait_ms': 0,\n",
       "   'env_task_fn': None,\n",
       "   'render_env': False,\n",
       "   'record_env': False,\n",
       "   'clip_rewards': None,\n",
       "   'normalize_actions': True,\n",
       "   'clip_actions': False,\n",
       "   'preprocessor_pref': 'deepmind',\n",
       "   'log_level': 'WARN',\n",
       "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
       "   'ignore_worker_failures': False,\n",
       "   'log_sys_usage': True,\n",
       "   'fake_sampler': False,\n",
       "   'framework': 'tf',\n",
       "   'eager_tracing': False,\n",
       "   'eager_max_retraces': 20,\n",
       "   'explore': False,\n",
       "   'exploration_config': {'type': 'EpsilonGreedy',\n",
       "    'initial_epsilon': 1.0,\n",
       "    'final_epsilon': 0.02,\n",
       "    'epsilon_timesteps': 10000},\n",
       "   'evaluation_interval': None,\n",
       "   'evaluation_duration': 50,\n",
       "   'evaluation_duration_unit': 'episodes',\n",
       "   'evaluation_parallel_to_training': False,\n",
       "   'in_evaluation': True,\n",
       "   'evaluation_config': {'explore': False},\n",
       "   'evaluation_num_workers': 1,\n",
       "   'custom_eval_function': None,\n",
       "   'always_attach_evaluation_results': False,\n",
       "   'sample_async': False,\n",
       "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
       "   'observation_filter': 'NoFilter',\n",
       "   'synchronize_filters': True,\n",
       "   'tf_session_args': {'intra_op_parallelism_threads': 2,\n",
       "    'inter_op_parallelism_threads': 2,\n",
       "    'gpu_options': {'allow_growth': True},\n",
       "    'log_device_placement': False,\n",
       "    'device_count': {'CPU': 1},\n",
       "    'allow_soft_placement': True},\n",
       "   'local_tf_session_args': {'intra_op_parallelism_threads': 8,\n",
       "    'inter_op_parallelism_threads': 8},\n",
       "   'compress_observations': False,\n",
       "   'metrics_episode_collection_timeout_s': 180,\n",
       "   'metrics_num_episodes_for_smoothing': 100,\n",
       "   'min_time_s_per_reporting': 1,\n",
       "   'min_train_timesteps_per_reporting': None,\n",
       "   'min_sample_timesteps_per_reporting': 1000,\n",
       "   'seed': None,\n",
       "   'extra_python_environs_for_driver': {},\n",
       "   'extra_python_environs_for_worker': {},\n",
       "   'num_gpus': 0,\n",
       "   '_fake_gpus': False,\n",
       "   'num_cpus_per_worker': 1,\n",
       "   'num_gpus_per_worker': 0,\n",
       "   'custom_resources_per_worker': {},\n",
       "   'num_cpus_for_driver': 1,\n",
       "   'placement_strategy': 'PACK',\n",
       "   'input': 'sampler',\n",
       "   'input_config': {},\n",
       "   'actions_in_input_normalized': False,\n",
       "   'input_evaluation': ['is', 'wis'],\n",
       "   'postprocess_inputs': False,\n",
       "   'shuffle_buffer_size': 0,\n",
       "   'output': None,\n",
       "   'output_compress_columns': ['obs', 'new_obs'],\n",
       "   'output_max_file_size': 67108864,\n",
       "   'multiagent': {'policies': {'policy0': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.DQNTFPolicy'>, observation_space=Box([0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
       "      0. 0. 0. 0. 0. 0. 0.], [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       "      1. 1. 1. 1. 1. 1. 1.], (31,), float64), action_space=Discrete(3), config={}),\n",
       "     'policy1': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.DQNTFPolicy'>, observation_space=Box([0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
       "      0. 0. 0. 0. 0. 0. 0.], [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       "      1. 1. 1. 1. 1. 1. 1.], (31,), float64), action_space=Discrete(3), config={}),\n",
       "     'policy2': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.DQNTFPolicy'>, observation_space=Box([0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
       "      0. 0. 0. 0. 0. 0. 0.], [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       "      1. 1. 1. 1. 1. 1. 1.], (31,), float64), action_space=Discrete(3), config={})},\n",
       "    'policy_map_capacity': 100,\n",
       "    'policy_map_cache': None,\n",
       "    'policy_mapping_fn': <function __main__.policy_mapping_fn(agent_id:str)>,\n",
       "    'policies_to_train': None,\n",
       "    'observation_fn': None,\n",
       "    'replay_mode': 'independent',\n",
       "    'count_steps_by': 'env_steps'},\n",
       "   'logger_config': None,\n",
       "   '_tf_policy_handles_more_than_one_loss': False,\n",
       "   '_disable_preprocessor_api': False,\n",
       "   '_disable_action_flattening': False,\n",
       "   '_disable_execution_plan_api': False,\n",
       "   'simple_optimizer': False,\n",
       "   'monitor': -1,\n",
       "   'evaluation_num_episodes': -1,\n",
       "   'metrics_smoothing_episodes': -1,\n",
       "   'timesteps_per_iteration': 1000,\n",
       "   'min_iter_time_s': -1,\n",
       "   'collect_metrics_timeout': -1,\n",
       "   'target_network_update_freq': 500,\n",
       "   'buffer_size': -1,\n",
       "   'replay_buffer_config': {'type': 'MultiAgentReplayBuffer',\n",
       "    'capacity': 50000},\n",
       "   'store_buffer_in_checkpoints': False,\n",
       "   'replay_sequence_length': 1,\n",
       "   'lr_schedule': None,\n",
       "   'adam_epsilon': 1e-08,\n",
       "   'grad_clip': 40,\n",
       "   'learning_starts': 1000,\n",
       "   'num_atoms': 1,\n",
       "   'v_min': -10.0,\n",
       "   'v_max': 10.0,\n",
       "   'noisy': False,\n",
       "   'sigma0': 0.5,\n",
       "   'dueling': True,\n",
       "   'hiddens': [256],\n",
       "   'double_q': True,\n",
       "   'n_step': 1,\n",
       "   'prioritized_replay': True,\n",
       "   'prioritized_replay_alpha': 0.6,\n",
       "   'prioritized_replay_beta': 0.4,\n",
       "   'final_prioritized_replay_beta': 0.4,\n",
       "   'prioritized_replay_beta_annealing_timesteps': 20000,\n",
       "   'prioritized_replay_eps': 1e-06,\n",
       "   'before_learn_on_batch': None,\n",
       "   'training_intensity': None,\n",
       "   'worker_side_prioritization': False},\n",
       "  'evaluation_num_workers': 1,\n",
       "  'custom_eval_function': None,\n",
       "  'always_attach_evaluation_results': False,\n",
       "  'sample_async': False,\n",
       "  'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
       "  'observation_filter': 'NoFilter',\n",
       "  'synchronize_filters': True,\n",
       "  'tf_session_args': {'intra_op_parallelism_threads': 2,\n",
       "   'inter_op_parallelism_threads': 2,\n",
       "   'gpu_options': {'allow_growth': True},\n",
       "   'log_device_placement': False,\n",
       "   'device_count': {'CPU': 1},\n",
       "   'allow_soft_placement': True},\n",
       "  'local_tf_session_args': {'intra_op_parallelism_threads': 8,\n",
       "   'inter_op_parallelism_threads': 8},\n",
       "  'compress_observations': False,\n",
       "  'metrics_episode_collection_timeout_s': 180,\n",
       "  'metrics_num_episodes_for_smoothing': 100,\n",
       "  'min_time_s_per_reporting': 1,\n",
       "  'min_train_timesteps_per_reporting': None,\n",
       "  'min_sample_timesteps_per_reporting': 1000,\n",
       "  'seed': None,\n",
       "  'extra_python_environs_for_driver': {},\n",
       "  'extra_python_environs_for_worker': {},\n",
       "  'num_gpus': 0,\n",
       "  '_fake_gpus': False,\n",
       "  'num_cpus_per_worker': 1,\n",
       "  'num_gpus_per_worker': 0,\n",
       "  'custom_resources_per_worker': {},\n",
       "  'num_cpus_for_driver': 1,\n",
       "  'placement_strategy': 'PACK',\n",
       "  'input': 'sampler',\n",
       "  'input_config': {},\n",
       "  'actions_in_input_normalized': False,\n",
       "  'input_evaluation': ['is', 'wis'],\n",
       "  'postprocess_inputs': False,\n",
       "  'shuffle_buffer_size': 0,\n",
       "  'output': None,\n",
       "  'output_compress_columns': ['obs', 'new_obs'],\n",
       "  'output_max_file_size': 67108864,\n",
       "  'multiagent': {'policies': {'policy0': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.DQNTFPolicy'>, observation_space=Box([0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
       "     0. 0. 0. 0. 0. 0. 0.], [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       "     1. 1. 1. 1. 1. 1. 1.], (31,), float64), action_space=Discrete(3), config={}),\n",
       "    'policy1': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.DQNTFPolicy'>, observation_space=Box([0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
       "     0. 0. 0. 0. 0. 0. 0.], [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       "     1. 1. 1. 1. 1. 1. 1.], (31,), float64), action_space=Discrete(3), config={}),\n",
       "    'policy2': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.DQNTFPolicy'>, observation_space=Box([0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
       "     0. 0. 0. 0. 0. 0. 0.], [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       "     1. 1. 1. 1. 1. 1. 1.], (31,), float64), action_space=Discrete(3), config={})},\n",
       "   'policy_map_capacity': 100,\n",
       "   'policy_map_cache': None,\n",
       "   'policy_mapping_fn': <function __main__.policy_mapping_fn(agent_id:str)>,\n",
       "   'policies_to_train': None,\n",
       "   'observation_fn': None,\n",
       "   'replay_mode': 'independent',\n",
       "   'count_steps_by': 'env_steps'},\n",
       "  'logger_config': None,\n",
       "  '_tf_policy_handles_more_than_one_loss': False,\n",
       "  '_disable_preprocessor_api': False,\n",
       "  '_disable_action_flattening': False,\n",
       "  '_disable_execution_plan_api': False,\n",
       "  'simple_optimizer': False,\n",
       "  'monitor': -1,\n",
       "  'evaluation_num_episodes': -1,\n",
       "  'metrics_smoothing_episodes': -1,\n",
       "  'timesteps_per_iteration': 1000,\n",
       "  'min_iter_time_s': -1,\n",
       "  'collect_metrics_timeout': -1,\n",
       "  'target_network_update_freq': 500,\n",
       "  'buffer_size': -1,\n",
       "  'replay_buffer_config': {'type': 'MultiAgentReplayBuffer',\n",
       "   'capacity': 50000},\n",
       "  'store_buffer_in_checkpoints': False,\n",
       "  'replay_sequence_length': 1,\n",
       "  'lr_schedule': None,\n",
       "  'adam_epsilon': 1e-08,\n",
       "  'grad_clip': 40,\n",
       "  'learning_starts': 1000,\n",
       "  'num_atoms': 1,\n",
       "  'v_min': -10.0,\n",
       "  'v_max': 10.0,\n",
       "  'noisy': False,\n",
       "  'sigma0': 0.5,\n",
       "  'dueling': True,\n",
       "  'hiddens': [256],\n",
       "  'double_q': True,\n",
       "  'n_step': 1,\n",
       "  'prioritized_replay': True,\n",
       "  'prioritized_replay_alpha': 0.6,\n",
       "  'prioritized_replay_beta': 0.4,\n",
       "  'final_prioritized_replay_beta': 0.4,\n",
       "  'prioritized_replay_beta_annealing_timesteps': 20000,\n",
       "  'prioritized_replay_eps': 1e-06,\n",
       "  'before_learn_on_batch': None,\n",
       "  'training_intensity': None,\n",
       "  'worker_side_prioritization': False},\n",
       " 'time_since_restore': 57.39332842826843,\n",
       " 'timesteps_since_restore': 192,\n",
       " 'iterations_since_restore': 6,\n",
       " 'perf': {'cpu_util_percent': 13.026666666666667,\n",
       "  'ram_util_percent': 27.093333333333344}}"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# del results[\"config\"]\n",
    "pprint.pprint(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Relevant methods => Check policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "policy = trainer.get_policy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model = trainer.get_policy().model\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Print out the policy's action and observation spaces.\n",
    "print(f\"Our Policy's observation space is: {policy.observation_space}\")\n",
    "print(f\"Our Policy's action space is: {policy.action_space}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Produce a random obervation (B=1; batch of size 1).\n",
    "obs = np.array([policy.observation_space.sample()])\n",
    "# Alternatively for PyTorch:\n",
    "#import torch\n",
    "#obs = torch.from_numpy(obs)\n",
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "logits, _ = model({\"obs\": obs})\n",
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "logits_np = policy.get_session().run(logits)\n",
    "logits_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from ray.rllib.utils.numpy import softmax\n",
    "action_probs = np.squeeze(softmax(logits_np))\n",
    "action_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Action scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "(action_scores, logits, dist) = model.get_q_value_distributions(model_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "action_scores.graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Mapping policies and agents ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "# Define the policies definition dict:\n",
    "# Each policy in there is defined by its ID (key) mapping to a 4-tuple (value):\n",
    "# - Policy class (None for using the \"default\" class, e.g. PPOTFPolicy for PPO+tf or PPOTorchPolicy for PPO+torch).\n",
    "# - obs-space (we get this directly from our already created env object).\n",
    "# - act-space (we get this directly from our already created env object).\n",
    "# - config-overrides dict (leave empty for using the Trainer's config as-is)\n",
    "\n",
    "config_env = {\"config\": {\"components\": 4} }\n",
    "\n",
    "env = StructMA(config_env)\n",
    "agent_list = []\n",
    "for i in range(env.ncomp):\n",
    "    item = \"agent_\"+ str(i)\n",
    "    agent_list.append(item)\n",
    "\n",
    "agent_list = []\n",
    "policy_list = []\n",
    "for i in range(env.ncomp):\n",
    "    item = \"agent_\"+ str(i)\n",
    "    item_ = \"policy\"+ str(i)\n",
    "    agent_list.append(item)\n",
    "    policy_list.append(item_)\n",
    "\n",
    "policies = {}\n",
    "mapping_agent2policy = {}\n",
    "print(env.ncomp)\n",
    "for i in range(env.ncomp):\n",
    "    mapping_agent2policy[agent_list[i]] = policy_list[i]\n",
    "    policies[policy_list[i]] = (None, env.observation_space, env.action_space, {})\n",
    "\n",
    "# Define an agent->policy mapping function.\n",
    "# Which agents (defined by the environment) use which policies (defined by us)?\n",
    "# The mapping here is M (agents) -> N (policies), where M >= N.\n",
    "def policy_mapping_fn(agent_id: str):\n",
    "    return mapping_agent2policy[agent_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'policy0': (None,\n",
       "  Box([0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
       "   0. 0. 0. 0. 0. 0. 0.], [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       "   1. 1. 1. 1. 1. 1. 1.], (31,), float64),\n",
       "  Discrete(3),\n",
       "  {}),\n",
       " 'policy1': (None,\n",
       "  Box([0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
       "   0. 0. 0. 0. 0. 0. 0.], [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       "   1. 1. 1. 1. 1. 1. 1.], (31,), float64),\n",
       "  Discrete(3),\n",
       "  {}),\n",
       " 'policy2': (None,\n",
       "  Box([0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
       "   0. 0. 0. 0. 0. 0. 0.], [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       "   1. 1. 1. 1. 1. 1. 1.], (31,), float64),\n",
       "  Discrete(3),\n",
       "  {}),\n",
       " 'policy3': (None,\n",
       "  Box([0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
       "   0. 0. 0. 0. 0. 0. 0.], [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       "   1. 1. 1. 1. 1. 1. 1.], (31,), float64),\n",
       "  Discrete(3),\n",
       "  {})}"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'policy0'"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_mapping_fn('agent_0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## If Ray does not start..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python gym",
   "language": "python",
   "name": "gym"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}