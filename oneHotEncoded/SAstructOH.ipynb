{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import matplotlib.pyplot as plt \n",
    "import scipy.io as spio\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StructSA(gym.Env):\n",
    "\n",
    "    def __init__(self, config=None):\n",
    "        empty_config = {\"config\": {\"components\": 2} }\n",
    "        config = config or empty_config\n",
    "        # Number of components #\n",
    "        self.ncomp = config['config'].get(\"components\", 2)\n",
    "        self.time = 0\n",
    "        self.ep_length = 30\n",
    "        self.nstcomp = 30\n",
    "        self.nobs = 2\n",
    "        self.actions_total = int(3**self.ncomp)\n",
    "        self.obs_total = int(self.ncomp*self.nstcomp + self.ncomp*31 + 31)\n",
    "\n",
    "        # configure spaces\n",
    "        self.action_space = gym.spaces.Discrete(self.actions_total)\n",
    "        self.observation_space = gym.spaces.Box(low=0.0, high=1.0, shape=(self.obs_total,), dtype=np.float64)\n",
    "        ### Loading the underlying POMDP model ###\n",
    "        drmodel = np.load('Dr3031C10.npz')\n",
    "        self.belief0 = drmodel['belief0'][0,0:self.ncomp,:,0] # (10 components, 30 crack states)\n",
    "        self.P = drmodel['P'][:,0:self.ncomp,:,:,:] # (3 actions, 10 components, 31 det rates, 30 cracks, 30 cracks)\n",
    "        self.O = drmodel['O'][:,0:self.ncomp,:,:] # (3 actions, 10 components, 30 cracks, 2 observations)\n",
    "            \n",
    "    def reset(self, seed=None, return_info=False, options=None):\n",
    "        # We need the following line to seed self.np_random\n",
    "        # super().reset(seed=seed)\n",
    "\n",
    "        # Choose the agent's belief\n",
    "        self.time_step = 0\n",
    "        self.agent_belief = self.belief0\n",
    "        self.drate = np.zeros((self.ncomp, 1), dtype=int)\n",
    "        \n",
    "        observation = np.concatenate( ((self.agent_belief).reshape(self.nstcomp*self.ncomp), \\\n",
    "            ( self.one_hot_drate(self.drate, self.ep_length+1) ).reshape(31*self.ncomp), \\\n",
    "             self.one_hot_time(self.time_step ,self.ep_length+1)  ) )\n",
    "        info = {\"belief\": self.agent_belief}\n",
    "        return (observation, info) if return_info else observation\n",
    "    \n",
    "    def step(self, action, return_info=False):\n",
    "        action_ = np.zeros(1, dtype=int)\n",
    "        action_ = action\n",
    "        action_ = self.convert_base_action(action_, 3, self.ncomp)\n",
    "        observation_, belief_prime, drate_prime = self.belief_update(self.agent_belief, action_, self.drate)\n",
    "        observation = np.concatenate( ((belief_prime).reshape(self.nstcomp*self.ncomp), \\\n",
    "            ( self.one_hot_drate(drate_prime, self.ep_length+1) ).reshape(31*self.ncomp), \\\n",
    "             self.one_hot_time(self.time_step+1 ,self.ep_length+1)  ) )\n",
    "        reward_ = self.immediate_cost(self.agent_belief, action_, belief_prime, self.drate)\n",
    "        reward = reward_.item() #Convert float64 to float\n",
    "        self.time_step += 1 \n",
    "        self.agent_belief = belief_prime\n",
    "        self.drate = drate_prime\n",
    "        # An episode is done if the agent has reached the target\n",
    "        done = np.array_equal(self.time_step, self.ep_length)\n",
    "        info = {\"belief\": self.agent_belief}\n",
    "        return (observation, reward, done, info) \n",
    "    \n",
    "    \n",
    "    def pf_sys(self, pf, k): # compute pf_sys for k-out-of-n components \n",
    "        n = pf.size\n",
    "        # k = ncomp-1\n",
    "        PF_sys = np.zeros(1)\n",
    "        nk = n-k\n",
    "        m = k+1\n",
    "        A = np.zeros(m+1)\n",
    "        A [1] = 1\n",
    "        L = 1\n",
    "        for j in range(1,n+1):\n",
    "            h = j + 1\n",
    "            Rel = 1-pf[j-1]\n",
    "            if nk < j:\n",
    "                L = h - nk\n",
    "            if k < j:\n",
    "                A[m] = A[m] + A[k]*Rel\n",
    "                h = k\n",
    "            for i in range(h, L-1, -1):\n",
    "                A[i] = A[i] + (A[i-1]-A[i])*Rel\n",
    "        PF_sys = 1-A[m]\n",
    "        return PF_sys  \n",
    "    \n",
    "    def immediate_cost(self, B, a, B_, drate): # immediate reward (-cost), based on current damage state and action#\n",
    "        cost_system = 0\n",
    "        PF = np.zeros((1,1))\n",
    "        PF = B[:,-1]\n",
    "        PF_ = np.zeros((1,1))\n",
    "        PF_ = B_[:,-1].copy()\n",
    "        for i in range(self.ncomp):\n",
    "            if a[i]==1:\n",
    "                cost_system += -1\n",
    "                Bplus = self.P[a[i],i,drate[i,0]].T.dot(B[i,:])\n",
    "                PF_[i] = Bplus[-1]         \n",
    "            elif a[i]==2:\n",
    "                cost_system +=  - 20\n",
    "        if self.ncomp < 2: # single component setting\n",
    "            PfSyS_ = PF_\n",
    "            PfSyS = PF\n",
    "        else:\n",
    "            PfSyS_ = self.pf_sys(PF_, self.ncomp-1) \n",
    "            PfSyS = self.pf_sys(PF, self.ncomp-1) \n",
    "        if PfSyS_ < PfSyS:\n",
    "            cost_system += PfSyS_*(-10000)\n",
    "        else:\n",
    "            cost_system += (PfSyS_-PfSyS)*(-10000) \n",
    "        return cost_system\n",
    "    \n",
    "    def belief_update(self, b, a, drate):  # Bayesian belief update based on previous belief, current observation, and action taken\n",
    "        b_prime = np.zeros((self.ncomp, self.nstcomp))\n",
    "        b_prime[:] = b\n",
    "        ob = np.zeros(self.ncomp)\n",
    "        drate_prime = np.zeros((self.ncomp, 1), dtype=int)\n",
    "        for i in range(self.ncomp):\n",
    "            p1 = self.P[a[i],i,drate[i,0]].T.dot(b_prime[i,:])  # environment transition\n",
    "            b_prime[i,:] = p1\n",
    "            drate_prime[i, 0] = drate[i, 0] + 1\n",
    "            ob[i] = 2\n",
    "            if a[i]==1:\n",
    "                Obs0 = np.sum(p1* self.O[a[i],i,:,0])\n",
    "                Obs1 = 1 - Obs0\n",
    "                if Obs1 < 1e-5:\n",
    "                    ob[i] = 0\n",
    "                else:\n",
    "                    ob_dist = np.array([Obs0, Obs1])\n",
    "                    ob[i] = np.random.choice(range(0,self.nobs), size=None, replace=True, p=ob_dist)           \n",
    "                b_prime[i,:] = p1* self.O[a[i],i,:,int(ob[i])]/(p1.dot(self.O[a[i],i,:,int(ob[i])])) # belief update\n",
    "            if a[i] == 2:\n",
    "                drate_prime[i, 0] = 0\n",
    "        return ob, b_prime, drate_prime\n",
    "    \n",
    "    def convert_base_action(self, action_, base, comp):\n",
    "        action_multi = np.zeros((comp,), dtype=int)\n",
    "        if action_ == 0:\n",
    "                return action_multi\n",
    "        digits = []\n",
    "        index_comp = int(comp) - 1 \n",
    "        while action_:\n",
    "            digits = (int(action_ % base))\n",
    "            action_multi[index_comp] = digits\n",
    "            action_ //= base\n",
    "            index_comp -= 1\n",
    "        return action_multi\n",
    "    \n",
    "    def one_hot_drate(self, drate, ep_length):\n",
    "        ohDrate = np.zeros((self.ncomp, ep_length), dtype=int)\n",
    "        for i in range(self.ncomp):\n",
    "            ohDrate[i, drate[i][0]] = 1\n",
    "        return ohDrate\n",
    "\n",
    "    def one_hot_time(self, time, ep_length):\n",
    "        ohTime = np.zeros((ep_length), dtype=int)\n",
    "        ohTime[time] = 1\n",
    "        return ohTime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization of the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "struc_heur = StructSA()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"config\": {\"components\": 5} }\n",
    "struc_heur = StructSA(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.052000e-04, 5.500000e-05, 8.660000e-05, 1.261000e-04,\n",
       "       2.006000e-04, 3.173000e-04, 4.853000e-04, 7.444000e-04,\n",
       "       1.138400e-03, 1.783100e-03, 2.713600e-03, 4.235700e-03,\n",
       "       6.473200e-03, 1.002420e-02, 1.530330e-02, 2.316180e-02,\n",
       "       3.453640e-02, 5.087030e-02, 7.324320e-02, 1.008326e-01,\n",
       "       1.309823e-01, 1.539425e-01, 1.567708e-01, 1.275575e-01,\n",
       "       7.401660e-02, 2.583390e-02, 4.230100e-03, 2.268000e-04,\n",
       "       3.200000e-06, 0.000000e+00, 1.052000e-04, 5.500000e-05,\n",
       "       8.660000e-05, 1.261000e-04, 2.006000e-04, 3.173000e-04,\n",
       "       4.853000e-04, 7.444000e-04, 1.138400e-03, 1.783100e-03,\n",
       "       2.713600e-03, 4.235700e-03, 6.473200e-03, 1.002420e-02,\n",
       "       1.530330e-02, 2.316180e-02, 3.453640e-02, 5.087030e-02,\n",
       "       7.324320e-02, 1.008326e-01, 1.309823e-01, 1.539425e-01,\n",
       "       1.567708e-01, 1.275575e-01, 7.401660e-02, 2.583390e-02,\n",
       "       4.230100e-03, 2.268000e-04, 3.200000e-06, 0.000000e+00,\n",
       "       1.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00,\n",
       "       0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00,\n",
       "       0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00,\n",
       "       0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00,\n",
       "       0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00,\n",
       "       0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00,\n",
       "       0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00,\n",
       "       0.000000e+00, 0.000000e+00, 0.000000e+00, 1.000000e+00,\n",
       "       0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00,\n",
       "       0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00,\n",
       "       0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00,\n",
       "       0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00,\n",
       "       0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00,\n",
       "       0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00,\n",
       "       0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00,\n",
       "       0.000000e+00, 0.000000e+00, 1.000000e+00, 0.000000e+00,\n",
       "       0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00,\n",
       "       0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00,\n",
       "       0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00,\n",
       "       0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00,\n",
       "       0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00,\n",
       "       0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00,\n",
       "       0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00,\n",
       "       0.000000e+00])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "struc_heur.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ DN => -12.22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.052000e-04, 5.500000e-05, 8.660000e-05, 1.258000e-04,\n",
       "       2.009000e-04, 3.164000e-04, 4.854000e-04, 7.417000e-04,\n",
       "       1.136900e-03, 1.778300e-03, 2.701200e-03, 4.202100e-03,\n",
       "       6.416000e-03, 9.892800e-03, 1.504950e-02, 2.263210e-02,\n",
       "       3.342290e-02, 4.873800e-02, 6.927450e-02, 9.389430e-02,\n",
       "       1.200616e-01, 1.401097e-01, 1.444372e-01, 1.247854e-01,\n",
       "       8.591560e-02, 4.474280e-02, 1.769880e-02, 5.987300e-03,\n",
       "       2.144100e-03, 2.861900e-03, 1.052000e-04, 5.500000e-05,\n",
       "       8.660000e-05, 1.259000e-04, 2.008000e-04, 3.170000e-04,\n",
       "       4.854000e-04, 7.430000e-04, 1.136800e-03, 1.781100e-03,\n",
       "       2.706700e-03, 4.218400e-03, 6.436100e-03, 9.955300e-03,\n",
       "       1.516110e-02, 2.286210e-02, 3.391110e-02, 4.966510e-02,\n",
       "       7.095350e-02, 9.689090e-02, 1.246830e-01, 1.459779e-01,\n",
       "       1.498704e-01, 1.269122e-01, 8.244760e-02, 3.756170e-02,\n",
       "       1.136830e-02, 2.490700e-03, 5.424000e-04, 3.487000e-04,\n",
       "       0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00,\n",
       "       0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00,\n",
       "       0.000000e+00, 1.000000e+00, 0.000000e+00, 0.000000e+00,\n",
       "       0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00,\n",
       "       0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00,\n",
       "       0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00,\n",
       "       0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00,\n",
       "       0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00,\n",
       "       0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00,\n",
       "       1.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00,\n",
       "       0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00,\n",
       "       0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00,\n",
       "       0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00,\n",
       "       0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00,\n",
       "       0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00,\n",
       "       0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00,\n",
       "       0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00,\n",
       "       0.000000e+00, 0.000000e+00, 0.000000e+00, 1.000000e+00,\n",
       "       0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00,\n",
       "       0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00,\n",
       "       0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00,\n",
       "       0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00,\n",
       "       0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00,\n",
       "       0.000000e+00])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "act = 0\n",
    "observation, reward, done, info = struc_heur.step(act)\n",
    "observation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 -3.9968028886505635e-10 -3.9968028886505635e-10 False\n",
      "1 -3.4770075707513115e-07 -3.3071539951023965e-07 False\n",
      "2 -1.6874399655364414e-05 -1.5559861088476623e-05 False\n",
      "3 -0.00018441750015973923 -0.00017367481528793302 False\n",
      "4 -0.001014276900246358 -0.000999809689769218 False\n",
      "5 -0.003786815999662352 -0.003929975724127952 False\n",
      "6 -0.009949665500386118 -0.011243894147893119 False\n",
      "7 -0.02249372169971231 -0.02695209893875552 False\n",
      "8 -0.04445859600021862 -0.05644683987172673 False\n",
      "9 -0.07673020799958685 -0.10480600817151282 False\n",
      "10 -0.12639293029970133 -0.18048212440052575 False\n",
      "11 -0.19336570119987684 -0.290468553086118 False\n",
      "12 -0.28099668930026134 -0.4423079487493179 False\n",
      "13 -0.3890220447000381 -0.6420093356172881 False\n",
      "14 -0.5190692265000241 -0.8951464098102014 False\n",
      "15 -0.6718887338996726 -1.2064265678690598 False\n",
      "16 -0.8522119999998523 -1.5815077964140531 False\n",
      "17 -1.0539131616005282 -2.022170320834369 False\n",
      "18 -1.2787437253003464 -2.530105638162269 False\n",
      "19 -1.526750699999413 -3.1062305149803495 False\n",
      "20 -1.8035030856000667 -3.7527609821883408 False\n",
      "21 -2.087775645600587 -4.463777251378795 False\n",
      "22 -2.4256029960001424 -5.24854118737357 False\n",
      "23 -2.729235330299895 -6.087390409779016 False\n",
      "24 -3.064348769999947 -6.982146617365018 False\n",
      "25 -3.432880558099649 -7.934391889954522 False\n",
      "26 -3.8288619000004243 -8.943373939538915 False\n",
      "27 -4.1845835823994815 -9.990959707425935 False\n",
      "28 -4.621072747199584 -11.089975045430712 False\n",
      "29 -5.012406507900469 -12.222455821467861 True\n",
      "-12.222455821467861\n"
     ]
    }
   ],
   "source": [
    "total_rew = 0\n",
    "for episodes in range(1,2):\n",
    "    cum_reward = 0\n",
    "    struc_heur.reset()\n",
    "    for t in range(30):\n",
    "        if t%4 == 0:\n",
    "            action_ = 0\n",
    "        else:\n",
    "            action_ = 0\n",
    "        observation, reward, done, info = struc_heur.step(action_)\n",
    "        cum_reward += reward*0.95**t\n",
    "        print(t, reward, cum_reward, done)\n",
    "    total_rew += cum_reward\n",
    "    #print(episodes, total_rew)\n",
    "exp_reward = total_rew/episodes\n",
    "print(exp_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of the environment with the trained policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-f7fc9a96062e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mcum_reward\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mobs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstruc_heur\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0maction_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_single_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0mobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstruc_heur\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'trainer' is not defined"
     ]
    }
   ],
   "source": [
    "total_rew = 0\n",
    "for episodes in range(1,4):\n",
    "    cum_reward = 0\n",
    "    obs = struc_heur.reset()\n",
    "    action_ = trainer.compute_single_action(obs)\n",
    "    for t in range(30):\n",
    "        obs, reward, done, info = struc_heur.step(action_)\n",
    "        action_ = trainer.compute_single_action(obs)\n",
    "        cum_reward += reward*0.95**t\n",
    "        # print(t, reward, cum_reward, done)\n",
    "        print(t, action_)\n",
    "    total_rew += cum_reward\n",
    "    #print(episodes, total_rew)\n",
    "exp_reward = total_rew/episodes\n",
    "print(exp_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration of the trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'node_ip_address': '127.0.0.1',\n",
       " 'raylet_ip_address': '127.0.0.1',\n",
       " 'redis_address': None,\n",
       " 'object_store_address': 'tcp://127.0.0.1:56811',\n",
       " 'raylet_socket_name': 'tcp://127.0.0.1:58687',\n",
       " 'webui_url': None,\n",
       " 'session_dir': 'C:\\\\Users\\\\user\\\\AppData\\\\Local\\\\Temp\\\\ray\\\\session_2022-07-07_15-25-45_242275_28996',\n",
       " 'metrics_export_port': 61219,\n",
       " 'gcs_address': '127.0.0.1:55030',\n",
       " 'address': '127.0.0.1:55030',\n",
       " 'node_id': '394336ab1d3b78a7f7dfad335b8485f5d848fb0b06d2cf479b4687ea'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pprint\n",
    "import ray\n",
    "from datetime import datetime\n",
    "import tempfile\n",
    "from ray.tune.logger import Logger, UnifiedLogger\n",
    "\n",
    "# Start a new instance of Ray (when running this tutorial locally) or\n",
    "# connect to an already running one (when running this tutorial through Anyscale).\n",
    "\n",
    "ray.init()  # Hear the engine humming? ;)\n",
    "\n",
    "# In case you encounter the following error during our tutorial: `RuntimeError: Maybe you called ray.init twice by accident?`\n",
    "# Try: `ray.shutdown() + ray.init()` or `ray.init(ignore_reinit_error=True)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Shutdown Ray's session\n",
    "ray.shutdown() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-07 15:26:34,143\tINFO trainer.py:2141 -- Your framework setting is 'tf', meaning you are using static-graph mode. Set framework='tf2' to enable eager execution with tf2.x. You may also then want to set eager_tracing=True in order to reach similar execution speed as with static-graph mode.\n",
      "2022-07-07 15:26:34,147\tWARNING trainer.py:2348 -- You have specified 1 evaluation workers, but your `evaluation_interval` is None! Therefore, evaluation will not occur automatically with each call to `Trainer.train()`. Instead, you will have to call `Trainer.evaluate()` manually in order to trigger an evaluation run.\n",
      "2022-07-07 15:26:34,147\tINFO simple_q.py:155 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting `simple_optimizer=True` if this doesn't work for you.\n",
      "2022-07-07 15:26:34,148\tINFO trainer.py:781 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:/14_DecomposedQ_DRL/single_agent_environment/01_single_agent_multiComponent/log_files\\dqn_2022-07-07_15-26-34oik05esn\n",
      "WARNING:tensorflow:From C:\\Users\\user\\Anaconda3\\envs\\gym\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-07 15:26:35,662\tWARNING deprecation.py:46 -- DeprecationWarning: `simple_optimizer` has been deprecated. This will raise an error in the future!\n",
      "2022-07-07 15:26:35,663\tWARNING trainer.py:2348 -- You have specified 1 evaluation workers, but your `evaluation_interval` is None! Therefore, evaluation will not occur automatically with each call to `Trainer.train()`. Instead, you will have to call `Trainer.evaluate()` manually in order to trigger an evaluation run.\n",
      "2022-07-07 15:26:35,748\tWARNING util.py:55 -- Install gputil for GPU system monitoring.\n",
      " pid=24256)\u001b[0m WARNING:tensorflow:From C:\\Users\\user\\Anaconda3\\envs\\gym\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      " pid=24256)\u001b[0m Instructions for updating:\n",
      " pid=24256)\u001b[0m If using Keras pass *_constraint arguments to layers.\n",
      " pid=10488)\u001b[0m WARNING:tensorflow:From C:\\Users\\user\\Anaconda3\\envs\\gym\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      " pid=10488)\u001b[0m Instructions for updating:\n",
      " pid=10488)\u001b[0m If using Keras pass *_constraint arguments to layers.\n",
      " pid=33260)\u001b[0m WARNING:tensorflow:From C:\\Users\\user\\Anaconda3\\envs\\gym\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      " pid=33260)\u001b[0m Instructions for updating:\n",
      " pid=33260)\u001b[0m If using Keras pass *_constraint arguments to layers.\n",
      " pid=28744)\u001b[0m WARNING:tensorflow:From C:\\Users\\user\\Anaconda3\\envs\\gym\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      " pid=28744)\u001b[0m Instructions for updating:\n",
      " pid=28744)\u001b[0m If using Keras pass *_constraint arguments to layers.\n"
     ]
    }
   ],
   "source": [
    "from ray.rllib.agents.dqn import DQNTrainer\n",
    "# Create an RLlib Trainer instance.\n",
    "\n",
    "config={\n",
    "        # Env class to use (here: our gym.Env sub-class from above).\n",
    "        \"env\": StructSA,\n",
    "        \n",
    "        \"env_config\": {\n",
    "            \"config\": {\"components\": 3},\n",
    "        },\n",
    "        # Number of steps after which the episode is forced to terminate. Defaults\n",
    "        # to `env.spec.max_episode_steps` (if present) for Gym envs.\n",
    "        \"horizon\": 30,\n",
    "        # Parallelize environment rollouts.\n",
    "        \"num_workers\": 3,\n",
    "        # Discount factor of the MDP.\n",
    "        \"gamma\": 0.95,\n",
    "        \n",
    "        # https://github.com/ray-project/ray/blob/releases/1.11.1/rllib/models/catalog.py\n",
    "        # FullyConnectedNetwork (tf and torch): rllib.models.tf|torch.fcnet.py\n",
    "        # These are used if no custom model is specified and the input space is 1D.\n",
    "        # Number of hidden layers to be used.\n",
    "        # Activation function descriptor.\n",
    "        # Supported values are: \"tanh\", \"relu\", \"swish\" (or \"silu\"),\n",
    "        # \"linear\" (or None).\n",
    "        \"model\": {\n",
    "            \"fcnet_hiddens\": [100],\n",
    "            \"fcnet_activation\": \"relu\"\n",
    "        },\n",
    "        \n",
    "        \"create_env_on_driver\": True,\n",
    "        \n",
    "        #\"evaluation_interval\": 2,\n",
    "        \"evaluation_num_workers\": 1,\n",
    "        \"evaluation_duration\": 50,\n",
    "        # === Deep Learning Framework Settings ===\n",
    "        # tf: TensorFlow (static-graph)\n",
    "        # tf2: TensorFlow 2.x (eager or traced, if eager_tracing=True)\n",
    "        # tfe: TensorFlow eager (or traced, if eager_tracing=True)\n",
    "        # torch: PyTorch\n",
    "#         \"framework\": \"torch\",\n",
    "    }\n",
    "\n",
    "PATH_logger = \"D:/14_DecomposedQ_DRL/single_agent_environment/01_single_agent_multiComponent/log_files\"\n",
    "### Defaut logger creator ###\n",
    "def custom_log_creator(custom_path, custom_str):\n",
    "\n",
    "    timestr = datetime.today().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    logdir_prefix = \"{}_{}\".format(custom_str, timestr)\n",
    "    def logger_creator(config):\n",
    "\n",
    "        if not os.path.exists(custom_path):\n",
    "            os.makedirs(custom_path)\n",
    "        logdir = tempfile.mkdtemp(prefix=logdir_prefix, dir=custom_path)\n",
    "        print(logdir)\n",
    "        return UnifiedLogger(config, logdir, loggers=None)\n",
    "\n",
    "    return logger_creator\n",
    "\n",
    "trainer = DQNTrainer(config=config, logger_creator=custom_log_creator(PATH_logger, 'dqn'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train policy and conduct evaluations periodically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 0; avg. reward=-622.3679274583674\n",
      "-797.5626443162677\n",
      "Iter: 1; avg. reward=-596.1110848079109\n",
      "Iter: 2; avg. reward=-562.8474449614669\n",
      "Iter: 3; avg. reward=-493.97519639753494\n",
      "Iter: 4; avg. reward=-416.8451801202591\n",
      "Iter: 5; avg. reward=-352.4606007736644\n",
      "-115.61829361948274\n",
      "Iter: 6; avg. reward=-302.0749573562642\n",
      "Iter: 7; avg. reward=-244.20190277362275\n",
      "Iter: 8; avg. reward=-184.5709092635305\n",
      "Iter: 9; avg. reward=-132.64731963854294\n",
      "Iter: 10; avg. reward=-117.42668895301873\n",
      "-123.07695051959621\n",
      "Iter: 11; avg. reward=-132.5139914022651\n",
      "Iter: 12; avg. reward=-122.05821773723605\n",
      "Iter: 13; avg. reward=-138.6183830932686\n",
      "Iter: 14; avg. reward=-108.9546088362364\n",
      "Iter: 15; avg. reward=-140.31326713164546\n",
      "-115.61829361948274\n",
      "Iter: 16; avg. reward=-117.7515263244716\n",
      "Iter: 17; avg. reward=-128.40167224745815\n",
      "Iter: 18; avg. reward=-101.18263108011341\n",
      "Iter: 19; avg. reward=-91.23610083361514\n",
      "Iter: 20; avg. reward=-77.95565266126897\n",
      "-115.72290558069476\n",
      "Iter: 21; avg. reward=-108.0189625936939\n",
      "Iter: 22; avg. reward=-118.53417427931308\n",
      "Iter: 23; avg. reward=-137.23252161303424\n",
      "Iter: 24; avg. reward=-109.26811392956799\n",
      "Iter: 25; avg. reward=-112.25615141871022\n",
      "-46.59078260707348\n",
      "Iter: 26; avg. reward=-95.28708974698152\n",
      "Iter: 27; avg. reward=-96.82387034114369\n",
      "Iter: 28; avg. reward=-77.36269423342142\n",
      "Iter: 29; avg. reward=-79.33275405247544\n",
      "Iter: 30; avg. reward=-69.16131345280871\n",
      "-95.47703038999471\n",
      "Iter: 31; avg. reward=-77.2609518929354\n",
      "Iter: 32; avg. reward=-74.04322694473132\n",
      "Iter: 33; avg. reward=-78.74353445321498\n",
      "Iter: 34; avg. reward=-76.37879696339813\n",
      "Iter: 35; avg. reward=-74.83418894159914\n",
      "-38.14652478196228\n",
      "Iter: 36; avg. reward=-69.92081145477727\n",
      "Iter: 37; avg. reward=-67.29552233278831\n",
      "Iter: 38; avg. reward=-66.74033900598985\n",
      "Iter: 39; avg. reward=-63.08856634953768\n",
      "Iter: 40; avg. reward=-56.05272887540487\n",
      "-47.25716198888172\n",
      "Iter: 41; avg. reward=-51.155434448200815\n",
      "Iter: 42; avg. reward=-49.99765390919161\n",
      "Iter: 43; avg. reward=-48.909557356221555\n",
      "Iter: 44; avg. reward=-56.71255051574668\n",
      "Iter: 45; avg. reward=-58.935703176591794\n",
      "-34.05626995742943\n",
      "Iter: 46; avg. reward=-63.81438459156033\n",
      "Iter: 47; avg. reward=-59.25723140275522\n",
      "Iter: 48; avg. reward=-61.28842211177169\n",
      "Iter: 49; avg. reward=-55.122662423303645\n",
      "Iter: 50; avg. reward=-51.34001493787822\n",
      "-41.658917261490245\n",
      "Iter: 51; avg. reward=-46.32010018881331\n",
      "Iter: 52; avg. reward=-46.27346006129298\n",
      "Iter: 53; avg. reward=-47.150034163049305\n",
      "Iter: 54; avg. reward=-48.799820203684874\n",
      "Iter: 55; avg. reward=-51.53486849893754\n",
      "-46.217983131325646\n",
      "Iter: 56; avg. reward=-51.35810110082943\n",
      "Iter: 57; avg. reward=-52.53873571466953\n",
      "Iter: 58; avg. reward=-54.26572736169543\n",
      "Iter: 59; avg. reward=-54.00439136613738\n",
      "Iter: 60; avg. reward=-53.35029388704785\n",
      "-36.69708094209467\n",
      "Iter: 61; avg. reward=-55.10907883057286\n",
      "Iter: 62; avg. reward=-53.32392973827856\n",
      "Iter: 63; avg. reward=-48.08247263686595\n",
      "Iter: 64; avg. reward=-45.732263035668595\n",
      "Iter: 65; avg. reward=-44.79386355249469\n",
      "-34.50391903823986\n",
      "Iter: 66; avg. reward=-47.2234172141161\n",
      "Iter: 67; avg. reward=-45.75670716324859\n",
      "Iter: 68; avg. reward=-51.888530107621094\n",
      "Iter: 69; avg. reward=-50.738898111163735\n",
      "Iter: 70; avg. reward=-49.66653234554372\n",
      "-40.18074348323102\n",
      "Iter: 71; avg. reward=-45.235496197564544\n",
      "Iter: 72; avg. reward=-49.9909347296596\n",
      "Iter: 73; avg. reward=-51.3113383898598\n",
      "Iter: 74; avg. reward=-53.025989320604175\n",
      "Iter: 75; avg. reward=-49.05659909683117\n",
      "-43.255324791483396\n",
      "Iter: 76; avg. reward=-49.6073930166512\n",
      "Iter: 77; avg. reward=-46.9126026815436\n",
      "Iter: 78; avg. reward=-48.460115365427875\n",
      "Iter: 79; avg. reward=-44.772551285040215\n",
      "Iter: 80; avg. reward=-43.929814517922814\n",
      "-39.87534815944891\n",
      "Iter: 81; avg. reward=-48.375029556528304\n",
      "Iter: 82; avg. reward=-54.80821504901837\n",
      "Iter: 83; avg. reward=-56.136513619261066\n",
      "Iter: 84; avg. reward=-49.62654829602116\n",
      "Iter: 85; avg. reward=-45.89181433772846\n",
      "-36.11195149195242\n",
      "Iter: 86; avg. reward=-55.53776561833141\n",
      "Iter: 87; avg. reward=-58.14864345772486\n",
      "Iter: 88; avg. reward=-61.71446517191501\n",
      "Iter: 89; avg. reward=-51.211318539954\n",
      "Iter: 90; avg. reward=-51.20085705654728\n",
      "-26.839072980994853\n",
      "Iter: 91; avg. reward=-49.44414570241419\n",
      "Iter: 92; avg. reward=-51.638612326483695\n",
      "Iter: 93; avg. reward=-49.06921125532712\n",
      "Iter: 94; avg. reward=-45.28735451056831\n",
      "Iter: 95; avg. reward=-46.167537166281164\n",
      "-42.8779495916128\n",
      "Iter: 96; avg. reward=-46.4736099797743\n",
      "Iter: 97; avg. reward=-50.58022760324527\n",
      "Iter: 98; avg. reward=-48.58467151175757\n",
      "Iter: 99; avg. reward=-50.52227595124079\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' export policy checkpoint\\ndef export_policy_checkpoint(\\n            self,\\n            export_dir: str,\\n            filename_prefix: str = \"model\",\\n            policy_id: PolicyID = DEFAULT_POLICY_ID,\\n    )   \\n'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    results = trainer.train()\n",
    "    #if i%100==0:\n",
    "    #trainer.export_policy_model(\"D:/14_DecomposedQ_DRL/single_agent_environment/struc_SA_jupyter/savedModel\")\n",
    "    print(f\"Iter: {i}; avg. reward={results['episode_reward_mean']}\")\n",
    "    #print(f\"Iter: {i}; evaluation={results['evaluation']['episode_reward_mean']}\")\n",
    "    \n",
    "    if i%5==0:\n",
    "        evaluat = trainer.evaluate()\n",
    "        print(evaluat['evaluation']['episode_reward_mean'])\n",
    "#         print(f\"Iter: {i}; evaluation={results['evaluation']['episode_reward_mean']}\")\n",
    "        \n",
    "# ''' export policy checkpoint\n",
    "# def export_policy_checkpoint(\n",
    "#             self,\n",
    "#             export_dir: str,\n",
    "#             filename_prefix: str = \"model\",\n",
    "#             policy_id: PolicyID = DEFAULT_POLICY_ID,\n",
    "#     )   \n",
    "# '''\n",
    "# PATH_model = \"D:/14_DecomposedQ_DRL/single_agent_environment/struc_SA_jupyter/savedModel\"\n",
    "\n",
    "# trainer.export_policy_checkpoint(PATH_model, filename_prefix='modelx')\n",
    "\n",
    "# trainer.save_checkpoint(PATH_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del results[\"config\"]\n",
    "pprint.pprint(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relevant methods => Check policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = trainer.get_policy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = trainer.get_policy().model\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the policy's action and observation spaces.\n",
    "print(f\"Our Policy's observation space is: {policy.observation_space}\")\n",
    "print(f\"Our Policy's action space is: {policy.action_space}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Produce a random obervation (B=1; batch of size 1).\n",
    "obs = np.array([policy.observation_space.sample()])\n",
    "# Alternatively for PyTorch:\n",
    "#import torch\n",
    "#obs = torch.from_numpy(obs)\n",
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits, _ = model({\"obs\": obs})\n",
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits_np = policy.get_session().run(logits)\n",
    "logits_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.utils.numpy import softmax\n",
    "action_probs = np.squeeze(softmax(logits_np))\n",
    "action_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Action scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(action_scores, logits, dist) = model.get_q_value_distributions(model_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_scores.graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition of action transformation (change of base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_base_action(action_, base, comp):\n",
    "    action_multi = np.zeros((comp,), dtype=int)\n",
    "    if action_ == 0:\n",
    "            return action_multi\n",
    "    digits = []\n",
    "    index_comp = int(comp) - 1 \n",
    "    while action_:\n",
    "        digits = (int(action_ % base))\n",
    "        #print(digits)\n",
    "        action_multi[index_comp] = digits\n",
    "        action_ //= base\n",
    "        index_comp -= 1\n",
    "        #print(index_comp, action_multi)\n",
    "    return action_multi\n",
    "\n",
    "# Testing\n",
    "# action_test = convert_base_action(5, 3, 2)\n",
    "# action_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "You must feed a value for placeholder tensor 'default_policy/obs' with dtype float and shape [?,241]\n\t [[node default_policy/obs (defined at C:\\Users\\user\\Anaconda3\\envs\\gym\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py:1751) ]]\n\nOriginal stack trace for 'default_policy/obs':\n  File \"C:\\Users\\user\\Anaconda3\\envs\\gym\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\Users\\user\\Anaconda3\\envs\\gym\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\user\\Anaconda3\\envs\\gym\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"C:\\Users\\user\\Anaconda3\\envs\\gym\\lib\\site-packages\\traitlets\\config\\application.py\", line 664, in launch_instance\n    app.start()\n  File \"C:\\Users\\user\\Anaconda3\\envs\\gym\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 612, in start\n    self.io_loop.start()\n  File \"C:\\Users\\user\\Anaconda3\\envs\\gym\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 148, in start\n    self.asyncio_loop.run_forever()\n  File \"C:\\Users\\user\\Anaconda3\\envs\\gym\\lib\\asyncio\\base_events.py\", line 438, in run_forever\n    self._run_once()\n  File \"C:\\Users\\user\\Anaconda3\\envs\\gym\\lib\\asyncio\\base_events.py\", line 1451, in _run_once\n    handle._run()\n  File \"C:\\Users\\user\\Anaconda3\\envs\\gym\\lib\\asyncio\\events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"C:\\Users\\user\\Anaconda3\\envs\\gym\\lib\\site-packages\\tornado\\ioloop.py\", line 690, in <lambda>\n    lambda f: self._run_callback(functools.partial(callback, future))\n  File \"C:\\Users\\user\\Anaconda3\\envs\\gym\\lib\\site-packages\\tornado\\ioloop.py\", line 743, in _run_callback\n    ret = callback()\n  File \"C:\\Users\\user\\Anaconda3\\envs\\gym\\lib\\site-packages\\tornado\\gen.py\", line 787, in inner\n    self.run()\n  File \"C:\\Users\\user\\Anaconda3\\envs\\gym\\lib\\site-packages\\tornado\\gen.py\", line 748, in run\n    yielded = self.gen.send(value)\n  File \"C:\\Users\\user\\Anaconda3\\envs\\gym\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 365, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"C:\\Users\\user\\Anaconda3\\envs\\gym\\lib\\site-packages\\tornado\\gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"C:\\Users\\user\\Anaconda3\\envs\\gym\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 268, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"C:\\Users\\user\\Anaconda3\\envs\\gym\\lib\\site-packages\\tornado\\gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"C:\\Users\\user\\Anaconda3\\envs\\gym\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 545, in execute_request\n    user_expressions, allow_stdin,\n  File \"C:\\Users\\user\\Anaconda3\\envs\\gym\\lib\\site-packages\\tornado\\gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"C:\\Users\\user\\Anaconda3\\envs\\gym\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 306, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\Users\\user\\Anaconda3\\envs\\gym\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\Users\\user\\Anaconda3\\envs\\gym\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2848, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"C:\\Users\\user\\Anaconda3\\envs\\gym\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2874, in _run_cell\n    return runner(coro)\n  File \"C:\\Users\\user\\Anaconda3\\envs\\gym\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 68, in _pseudo_sync_runner\n    coro.send(None)\n  File \"C:\\Users\\user\\Anaconda3\\envs\\gym\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3051, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\Users\\user\\Anaconda3\\envs\\gym\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3242, in run_ast_nodes\n    if (await self.run_code(code, result,  async_=asy)):\n  File \"C:\\Users\\user\\Anaconda3\\envs\\gym\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3319, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-106-7cdb4d8bac2b>\", line 44, in <module>\n    trainer = DQNTrainer(config=config)\n  File \"C:\\Users\\user\\Anaconda3\\envs\\gym\\lib\\site-packages\\ray\\rllib\\agents\\trainer.py\", line 747, in __init__\n    sync_function_tpl)\n  File \"C:\\Users\\user\\Anaconda3\\envs\\gym\\lib\\site-packages\\ray\\tune\\trainable.py\", line 124, in __init__\n    self.setup(copy.deepcopy(self.config))\n  File \"C:\\Users\\user\\Anaconda3\\envs\\gym\\lib\\site-packages\\ray\\rllib\\agents\\trainer.py\", line 827, in setup\n    num_workers=self.config[\"num_workers\"])\n  File \"C:\\Users\\user\\Anaconda3\\envs\\gym\\lib\\site-packages\\ray\\rllib\\agents\\trainer.py\", line 2002, in _make_workers\n    logdir=self.logdir,\n  File \"C:\\Users\\user\\Anaconda3\\envs\\gym\\lib\\site-packages\\ray\\rllib\\evaluation\\worker_set.py\", line 132, in __init__\n    spaces=spaces,\n  File \"C:\\Users\\user\\Anaconda3\\envs\\gym\\lib\\site-packages\\ray\\rllib\\evaluation\\worker_set.py\", line 540, in _make_worker\n    spaces=spaces,\n  File \"C:\\Users\\user\\Anaconda3\\envs\\gym\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 590, in __init__\n    seed=seed)\n  File \"C:\\Users\\user\\Anaconda3\\envs\\gym\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 1578, in _build_policy_map\n    conf, merged_conf)\n  File \"C:\\Users\\user\\Anaconda3\\envs\\gym\\lib\\site-packages\\ray\\rllib\\policy\\policy_map.py\", line 134, in create_policy\n    observation_space, action_space, merged_config)\n  File \"C:\\Users\\user\\Anaconda3\\envs\\gym\\lib\\site-packages\\ray\\rllib\\policy\\tf_policy_template.py\", line 252, in __init__\n    get_batch_divisibility_req=get_batch_divisibility_req,\n  File \"C:\\Users\\user\\Anaconda3\\envs\\gym\\lib\\site-packages\\ray\\rllib\\policy\\dynamic_tf_policy.py\", line 247, in __init__\n    **prev_action_ph))\n  File \"C:\\Users\\user\\Anaconda3\\envs\\gym\\lib\\site-packages\\ray\\rllib\\policy\\dynamic_tf_policy.py\", line 608, in _get_input_dict_and_dummy_batch\n    flatten=flatten,\n  File \"C:\\Users\\user\\Anaconda3\\envs\\gym\\lib\\site-packages\\ray\\rllib\\utils\\tf_utils.py\", line 216, in get_placeholder\n    name=name,\n  File \"C:\\Users\\user\\Anaconda3\\envs\\gym\\lib\\site-packages\\tensorflow_core\\python\\ops\\array_ops.py\", line 2630, in placeholder\n    return gen_array_ops.placeholder(dtype=dtype, shape=shape, name=name)\n  File \"C:\\Users\\user\\Anaconda3\\envs\\gym\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_array_ops.py\", line 6670, in placeholder\n    \"Placeholder\", dtype=dtype, shape=shape, name=name)\n  File \"C:\\Users\\user\\Anaconda3\\envs\\gym\\lib\\site-packages\\tensorflow_core\\python\\framework\\op_def_library.py\", line 793, in _apply_op_helper\n    op_def=op_def)\n  File \"C:\\Users\\user\\Anaconda3\\envs\\gym\\lib\\site-packages\\tensorflow_core\\python\\util\\deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"C:\\Users\\user\\Anaconda3\\envs\\gym\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\", line 3360, in create_op\n    attrs, op_def, compute_device)\n  File \"C:\\Users\\user\\Anaconda3\\envs\\gym\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\", line 3429, in _create_op_internal\n    op_def=op_def)\n  File \"C:\\Users\\user\\Anaconda3\\envs\\gym\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\", line 1751, in __init__\n    self._traceback = tf_stack.extract_stack()\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\envs\\gym\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1364\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1365\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1366\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\gym\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1349\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[1;32m-> 1350\u001b[1;33m                                       target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1351\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\gym\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1442\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1443\u001b[1;33m                                             run_metadata)\n\u001b[0m\u001b[0;32m   1444\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: You must feed a value for placeholder tensor 'default_policy/obs' with dtype float and shape [?,241]\n\t [[{{node default_policy/obs}}]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-136-6964c04827e1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpolicy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mq_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\gym\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    954\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    955\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 956\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    957\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    958\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\gym\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1178\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1179\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1180\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1181\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1182\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\gym\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1357\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1358\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1359\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1360\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1361\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\gym\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1382\u001b[0m                     \u001b[1;34m'\\nsession_config.graph_options.rewrite_options.'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1383\u001b[0m                     'disable_meta_optimizer = True')\n\u001b[1;32m-> 1384\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1385\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1386\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: You must feed a value for placeholder tensor 'default_policy/obs' with dtype float and shape [?,241]\n\t [[node default_policy/obs (defined at C:\\Users\\user\\Anaconda3\\envs\\gym\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py:1751) ]]\n\nOriginal stack trace for 'default_policy/obs':\n  File \"C:\\Users\\user\\Anaconda3\\envs\\gym\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\Users\\user\\Anaconda3\\envs\\gym\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\user\\Anaconda3\\envs\\gym\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"C:\\Users\\user\\Anaconda3\\envs\\gym\\lib\\site-packages\\traitlets\\config\\application.py\", line 664, in launch_instance\n    app.start()\n  File \"C:\\Users\\user\\Anaconda3\\envs\\gym\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 612, in start\n    self.io_loop.start()\n  File \"C:\\Users\\user\\Anaconda3\\envs\\gym\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 148, in start\n    self.asyncio_loop.run_forever()\n  File \"C:\\Users\\user\\Anaconda3\\envs\\gym\\lib\\asyncio\\base_events.py\", line 438, in run_forever\n    self._run_once()\n  File \"C:\\Users\\user\\Anaconda3\\envs\\gym\\lib\\asyncio\\base_events.py\", line 1451, in _run_once\n    handle._run()\n  File \"C:\\Users\\user\\Anaconda3\\envs\\gym\\lib\\asyncio\\events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"C:\\Users\\user\\Anaconda3\\envs\\gym\\lib\\site-packages\\tornado\\ioloop.py\", line 690, in <lambda>\n    lambda f: self._run_callback(functools.partial(callback, future))\n  File \"C:\\Users\\user\\Anaconda3\\envs\\gym\\lib\\site-packages\\tornado\\ioloop.py\", line 743, in _run_callback\n    ret = callback()\n  File \"C:\\Users\\user\\Anaconda3\\envs\\gym\\lib\\site-packages\\tornado\\gen.py\", line 787, in inner\n    self.run()\n  File \"C:\\Users\\user\\Anaconda3\\envs\\gym\\lib\\site-packages\\tornado\\gen.py\", line 748, in run\n    yielded = self.gen.send(value)\n  File \"C:\\Users\\user\\Anaconda3\\envs\\gym\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 365, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"C:\\Users\\user\\Anaconda3\\envs\\gym\\lib\\site-packages\\tornado\\gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"C:\\Users\\user\\Anaconda3\\envs\\gym\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 268, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"C:\\Users\\user\\Anaconda3\\envs\\gym\\lib\\site-packages\\tornado\\gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"C:\\Users\\user\\Anaconda3\\envs\\gym\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 545, in execute_request\n    user_expressions, allow_stdin,\n  File \"C:\\Users\\user\\Anaconda3\\envs\\gym\\lib\\site-packages\\tornado\\gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"C:\\Users\\user\\Anaconda3\\envs\\gym\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 306, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\Users\\user\\Anaconda3\\envs\\gym\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\Users\\user\\Anaconda3\\envs\\gym\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2848, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"C:\\Users\\user\\Anaconda3\\envs\\gym\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2874, in _run_cell\n    return runner(coro)\n  File \"C:\\Users\\user\\Anaconda3\\envs\\gym\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 68, in _pseudo_sync_runner\n    coro.send(None)\n  File \"C:\\Users\\user\\Anaconda3\\envs\\gym\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3051, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\Users\\user\\Anaconda3\\envs\\gym\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3242, in run_ast_nodes\n    if (await self.run_code(code, result,  async_=asy)):\n  File \"C:\\Users\\user\\Anaconda3\\envs\\gym\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3319, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-106-7cdb4d8bac2b>\", line 44, in <module>\n    trainer = DQNTrainer(config=config)\n  File \"C:\\Users\\user\\Anaconda3\\envs\\gym\\lib\\site-packages\\ray\\rllib\\agents\\trainer.py\", line 747, in __init__\n    sync_function_tpl)\n  File \"C:\\Users\\user\\Anaconda3\\envs\\gym\\lib\\site-packages\\ray\\tune\\trainable.py\", line 124, in __init__\n    self.setup(copy.deepcopy(self.config))\n  File \"C:\\Users\\user\\Anaconda3\\envs\\gym\\lib\\site-packages\\ray\\rllib\\agents\\trainer.py\", line 827, in setup\n    num_workers=self.config[\"num_workers\"])\n  File \"C:\\Users\\user\\Anaconda3\\envs\\gym\\lib\\site-packages\\ray\\rllib\\agents\\trainer.py\", line 2002, in _make_workers\n    logdir=self.logdir,\n  File \"C:\\Users\\user\\Anaconda3\\envs\\gym\\lib\\site-packages\\ray\\rllib\\evaluation\\worker_set.py\", line 132, in __init__\n    spaces=spaces,\n  File \"C:\\Users\\user\\Anaconda3\\envs\\gym\\lib\\site-packages\\ray\\rllib\\evaluation\\worker_set.py\", line 540, in _make_worker\n    spaces=spaces,\n  File \"C:\\Users\\user\\Anaconda3\\envs\\gym\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 590, in __init__\n    seed=seed)\n  File \"C:\\Users\\user\\Anaconda3\\envs\\gym\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 1578, in _build_policy_map\n    conf, merged_conf)\n  File \"C:\\Users\\user\\Anaconda3\\envs\\gym\\lib\\site-packages\\ray\\rllib\\policy\\policy_map.py\", line 134, in create_policy\n    observation_space, action_space, merged_config)\n  File \"C:\\Users\\user\\Anaconda3\\envs\\gym\\lib\\site-packages\\ray\\rllib\\policy\\tf_policy_template.py\", line 252, in __init__\n    get_batch_divisibility_req=get_batch_divisibility_req,\n  File \"C:\\Users\\user\\Anaconda3\\envs\\gym\\lib\\site-packages\\ray\\rllib\\policy\\dynamic_tf_policy.py\", line 247, in __init__\n    **prev_action_ph))\n  File \"C:\\Users\\user\\Anaconda3\\envs\\gym\\lib\\site-packages\\ray\\rllib\\policy\\dynamic_tf_policy.py\", line 608, in _get_input_dict_and_dummy_batch\n    flatten=flatten,\n  File \"C:\\Users\\user\\Anaconda3\\envs\\gym\\lib\\site-packages\\ray\\rllib\\utils\\tf_utils.py\", line 216, in get_placeholder\n    name=name,\n  File \"C:\\Users\\user\\Anaconda3\\envs\\gym\\lib\\site-packages\\tensorflow_core\\python\\ops\\array_ops.py\", line 2630, in placeholder\n    return gen_array_ops.placeholder(dtype=dtype, shape=shape, name=name)\n  File \"C:\\Users\\user\\Anaconda3\\envs\\gym\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_array_ops.py\", line 6670, in placeholder\n    \"Placeholder\", dtype=dtype, shape=shape, name=name)\n  File \"C:\\Users\\user\\Anaconda3\\envs\\gym\\lib\\site-packages\\tensorflow_core\\python\\framework\\op_def_library.py\", line 793, in _apply_op_helper\n    op_def=op_def)\n  File \"C:\\Users\\user\\Anaconda3\\envs\\gym\\lib\\site-packages\\tensorflow_core\\python\\util\\deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"C:\\Users\\user\\Anaconda3\\envs\\gym\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\", line 3360, in create_op\n    attrs, op_def, compute_device)\n  File \"C:\\Users\\user\\Anaconda3\\envs\\gym\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\", line 3429, in _create_op_internal\n    op_def=op_def)\n  File \"C:\\Users\\user\\Anaconda3\\envs\\gym\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\", line 1751, in __init__\n    self._traceback = tf_stack.extract_stack()\n"
     ]
    }
   ],
   "source": [
    "policy.get_session().run(q_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If Ray does not start..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To be investigated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://docs.ray.io/en/latest/rllib/rllib-training.html#accessing-policy-state\n",
    "\n",
    "help(trainer.get_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.get_policy().export_model(PATH_model+'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "rr = trainer.get_policy().q_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(trainer.get_policy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('my_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ee = model.value_function()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(action_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_scores.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(model.base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.base_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_out = model.__call__({\"obs\": np.array([struc_heur.reset()])})\n",
    "model_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session():\n",
    "    model_out[0].eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dir(model_out[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(model_out[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the trained policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation = trainer.evaluate()\n",
    "evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Storing and restoring checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_model = \"D:/14_DecomposedQ_DRL/single_agent_environment/struc_SA_jupyter/savedModel\"\n",
    "trainer.save(PATH_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.restore(PATH_model+'/checkpoint_000000/checkpoint-0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get action from the trained policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.compute_single_action(struc_heur.reset())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load checkpoint (it required setting up the same configuration as during training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.load_checkpoint(\"D:/14_DecomposedQ_DRL/single_agent_environment/struc_SA_jupyter/savedModel/checkpoint-52\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running from the console"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rllib train --run DQN --env CartPole-v0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additional test code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1):\n",
    "    results = trainer.train()\n",
    "    #if i%100==0:\n",
    "    print(f\"Iter: {i}; avg. reward={results['episode_reward_mean']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.algorithms.ppo import PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(agents.qmix.QMixTrainer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python gym",
   "language": "python",
   "name": "gym"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
